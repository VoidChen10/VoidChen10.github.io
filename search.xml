<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记3：使用朴素贝叶斯识别手写图片]]></title>
    <url>%2F2019%2F02%2F24%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B03%EF%BC%9A%E4%BD%BF%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[前言&ensp;&ensp;&ensp;&ensp;上一篇讲完了MNIST手写数字数据集的读取使用，今天呢就来讲讲使用分类算法——朴素贝叶斯算法(Naive Bayesian classification)来对这个数据集进行训练，看看使用这个算法训练出来的模型，对手写数字的识别率有多高。本人也是初学者，所以对于算法的讲述只是基于自身粗浅的理解，如有不同意见欢迎指正。话不多说，我们来进入正题。 朴素贝叶斯算法原理&ensp;&ensp;&ensp;&ensp;不得不说，这个名字起得太贴切了，在我了解完这个算法之后，才知道他真的很“朴素”。。。 关于分类问题&ensp;&ensp;&ensp;&ensp;先来感性认识一下这个算法，朴素贝叶斯算法是一个分类算法，对于分类，其实我们并不陌生，在生活中我们也常常会下意思地对事物进行“分类”： 例如走在街上，看到一个路人，我们会下意思地去判断是男是女；别人请你吃水果，看一眼就分辨出是苹果还是橘子；在奶茶店买一杯茶，看茶汤颜色会判断是红茶还是绿茶或者是乌龙茶。这里例子都说明了一件事，我们生活中无时无刻都在对事物进行着分类。&ensp;&ensp;&ensp;&ensp;但是分类的前提，是我们得对这个事物有基本的认知，比如，舍友给了我一个牛油果，但是我没吃过牛油果，所以我不能准确地把它分类到“牛油果”这个类别去，虽然我不能明确他的分类，但是我能确定它不是“衣服”，因为它的特征跟衣服完全不一致，根据已知的事物中，它的特征跟“水果”最像，所以它是水果的概率最高，所以我们会把他分类到“水果”去。&ensp;&ensp;&ensp;&ensp;看到是不是觉得有点“朴素”。。。对的，朴素贝叶斯算法，听起来好像很厉害，实际上就是这么一个“朴素”的思想，在已知的条件下，选出概率最高的一个类别作为该项的分类，“朴素”到我这种数学渣渣都露出了会心的微笑2333333 朴素贝叶斯分类的数学原理与流程 朴素贝叶斯分类的正式定义如下： 设为一个待分类项，而每个a为x的一个特征属性。 有类别集合。 计算。 如果，则。 那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做： 找到一个已知分类的待分类项集合，这个集合叫做训练样本集。 统计得到在各类别下各个特征属性的条件概率估计,即：。 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导： 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：对于朴素贝叶斯在机器学习上解决分类问题，主要是分为三个阶段：&ensp;&ensp;&ensp;&ensp;第一阶段——准备工作阶段，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。&ensp;&ensp;&ensp;&ensp;第二阶段——分类器训练阶段，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。&ensp;&ensp;&ensp;&ensp;第三阶段——应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。 以上引用自： 《算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)》 朴素贝叶斯的应用场景和局限优点： 算法简单容易理解，对分类结果的可解释性较强 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练。 缺点： 需要明确有哪些分类，如果给定没有出现过的类和特征，则该类别的条件概率估计将出现0，该问题被称为“零条件概率问题”。 特征之间独立的假设非常强。 在现实生活中几乎很难找到这样的数据集。在属性个数比较多或者属性之间相关性较大时，朴素贝叶斯分类模型的分类效率低。而在属性相关性较小时，朴素贝叶斯分类模型的性能最为良好。 应用场景： 欺诈检测中使用较多 垃圾邮件过滤 文本分类 人脸识别 朴素贝叶斯在Spark MLlib中的使用&ensp;&ensp;&ensp;&ensp;Spark MLlib中提供了朴素贝叶斯算法工具，我们调用的时候只需要把数据处理成spark的标准输入格式即可轻松得到训练模型。下面我将使用MNIST手写数字数据集来做训练，并用其提供的测试集计算准确率。（ps:后续会加入对自己手写数字图片进行识别，请等下一次更新(:з」∠)） 1、首先把标签数据和图片数据处理成 LabeledPoint 标签向量数据格式：1234val data = trainLabe.zip(trainImages).map( d =&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble))))val trainRdd = sc.makeRDD(data) 2、调用MLlib的api，输入数据进行训练，得到分类模型：1val model = NaiveBayes.train(trainRdd) 3、利用训练好的模型，对测试数据进行检验，看看识别准确率有多少：1234567891011val testData = testImages.map(d =&gt; Vectors.dense(d.map(p =&gt; (p &amp; 0xFF).toDouble )))val testRDD = sc.makeRDD(testData)val res = model.predict(testRDD).map(l =&gt; l.toInt).collect()val tr = res.zip(testLabe)val sum = tr.map( f =&gt;&#123; if(f._1 == f._2.toInt) 1 else 0&#125;).sumprintln(&quot;准确率为：&quot;+ sum.toDouble /tr.length) 如图，利用它提供的测试集呢，我这里得到的识别率约为0.8365，看样子识别效率并不怎么样啊，因为据我所知这份数据最高的识别率能高达99%甚至100%的，下一期我将会尝试使用随机森林算法来对这个数据集进行训练，稍微期待一下吧~此处只贴了部分代码，完整代码可前往github查看： 完整代码点我 参考链接 《算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)》 《机器学习|朴素贝叶斯（Naive Bayes）算法总结》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记2:MNIST手写数字的读取使用]]></title>
    <url>%2F2019%2F02%2F21%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B02%EF%BC%9AMNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%9A%84%E8%AF%BB%E5%8F%96%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言&ensp;&ensp;&ensp;&ensp;近来广州阴雨连绵，实在是让人提不起精神来，文档什么的也看不进去，脑袋昏昏沉沉的，所以拖拖拉拉到今天才出更新=。=&ensp;&ensp;&ensp;&ensp;重申一遍，我是沿着spark官网中MLlib的api往下去学习的，提到的相关算法都会去学习下。文档上有的东西呢，我就不赘述了，只是把自己学习过程中一些重要的，别的博客没有提到的，记录一下，以后自己复习也方便。在这篇东西之前，我已经自己看完了spark MLlib的数据类型和基本统计，没了解的可以先去看一下文档，后续会有用到。&ensp;&ensp;&ensp;&ensp;好了，言归正传，今天要讲的是被称为机器学习界的“Hello world”，几乎每个入门系列都会用到的一个知名数据集——MNIST手写数字图像数据集，该系列的后续文章中也经常会用到，所以今天独立出来讲一下好了。 MNIST手写数字数据集首先给出官网：THE MNIST DATABASE of handwritten digitsMNIST是一个经典的手写数字数据集，来自美国国家标准与技术研究所，由不同人手写的0至9的数字构成，由60000个训练样本集和10000个测试样本集构成，每个样本的尺寸为28x28，以二进制格式存储，如下图所示：&ensp;&ensp;&ensp;&ensp;这里有个比较坑的细节，就是数据采集都是外国人，他们的手写数字风格跟我们亚洲人的略有不同，这也导致识别我们自己手写的数字时候准确率下降了不少=。=下载下来之后是4个gz压缩包： train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes) 解压之后得到的是一个二进制文件，文件的格式官网有给出，这里我顺便贴一下： TEST SET LABEL FILE (train-labels-idx1-ubyte): [offset] [type] [value] [description] 0000 32 bit integer 0x00000801(2049) magic number (MSB first) 0004 32 bit integer 60000 number of items 0008 unsigned byte ?? label 0009 unsigned byte ?? label …….. unsigned byte ?? label The labels values are 0 to 9.TEST SET IMAGE FILE (train-images-idx3-ubyte): [offset] [type] [value] [description] 0000 32 bit integer 0x00000803(2051) magic number 0004 32 bit integer 60000 number of images 0008 32 bit integer 28 number of rows 0012 32 bit integer 28 number of columns 0016 unsigned byte ?? pixel 0017 unsigned byte ?? pixel …….. unsigned byte ?? pixel Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black). 训练集和测试集的结构都是一样的，这里我就只贴出训练集的结构，标签是0到9的数字，图像则是0到255的像素值。 其实一开始看到二进制文件我也是有点懵，因为以前工作也没接触过。不过看多两眼就明白了。以images数据集来说吧，这个train-images-idx3-ubyte文件： 第0000字节到第0003字节，是一个32位int类型数字，值为2051的魔数。 第0004字节到第0007字节也是一个32位int类型数字，转换过来值为60000，表示一共有60000个训练样本。 第0008字节到第00011字节和第0012字节到第0015字节同理，代表着有28行和28列。 &ensp;&ensp;&ensp;&ensp;这里的行和列，其实就是一张图片的像素矩阵，也就是每个图片都有28×28=784个像素。所以说，从第16个字节开始，就是图片的每一个像素点的值了。&ensp;&ensp;&ensp;&ensp;说到这里可能会有些朋友担心，会不会每一张图片都是这么排下来呢？为此我去算了下这个二进制文件的长度，刚好就是16+28*28*60000,所以文件描述信息呢就之在文件开头出现了，后面的就是每张图片的像素连在一起了，图片的规格统一被处理成28*28的图片了。因此，读取图片信息有了如下代码：123456789101112131415161718192021222324252627282930313233343536373839/** * 读取图像文件 * @param imagesPath * @return */def loadImages(imagesPath: String): Array[Array[Byte]] =&#123; val file = new File(imagesPath) val in = new FileInputStream(file) var trainingDS = new Array[Byte](file.length.toInt) using(new FileInputStream(file)) &#123; source =&gt; &#123; in.read(trainingDS) &#125; &#125; //32 bit integer 0x00000803(2051) magic number val magicNum = ByteBuffer.wrap(trainingDS.take(4)).getInt println(s&quot;magicNum=$magicNum&quot;) //32 bit integer 60000 number of items val numOfItems = ByteBuffer.wrap(trainingDS.slice(4, 8)).getInt println(s&quot;numOfItems=$numOfItems&quot;) //32 bit integer 28 number of rows val numOfRows = ByteBuffer.wrap(trainingDS.slice(8, 12)).getInt println(s&quot;numOfRows=$numOfRows&quot;) //32 bit integer 28 number of columns val numOfCols = ByteBuffer.wrap(trainingDS.slice(12, 16)).getInt println(s&quot;numOfCols=$numOfCols&quot;) trainingDS = trainingDS.drop(16) val itemsBuffer = new ArrayBuffer[Array[Byte]] for(i &lt;- 0 until numOfItems)&#123; itemsBuffer += trainingDS.slice( i * numOfCols * numOfRows , (i+1) * numOfCols * numOfRows) &#125; itemsBuffer.toArray&#125; &ensp;&ensp;&ensp;&ensp;代码里面提取了文件描述信息，并返回有每一张图片像素数据组成的数组。完整代码我放在github上了： 完整代码&ensp;&ensp;&ensp;&ensp;读取标签数据也是同理，在下一篇文章中，我将会详细讲述如何用标签和图片数据去做训练，敬请期待=。=&ensp;&ensp;&ensp;&ensp;其实数据集到这里已经是完成读取了，后续只要转换成spark MLlib的数据格式就能用了，但是！我在把像素值由二进制byte类型转换成double类型的时候，居然出现了负数。这是怎么回事呢，看官网说的，值应该是在0到255之间才对啊。再仔细看了一遍文件架构，发现标明的像素类型为unsigned byte，即无符号byte类型，在我学习java的时候，隐约记得有学过说java类型都是有符号的，第一位即是正负号。于是去搜索了一下，找到了这篇文章：《Java中对于unsigned byte类型的转换处理》 把二进制像素值转换为Double类型数字&ensp;&ensp;&ensp;&ensp;由于Java中没有unsigned byte类型，所以一个字节只能表示（-128，127），而想要表示（0，255），也很简单，只要跟0xFF取&amp;操作即可。原理其实就是正数的反码和补码都是其本身，负数的反码是对原码除了符号位之外作取反运算，补码则为反码+1。&ensp;&ensp;&ensp;&ensp;更多细节可以参考：《byte为什么要与上0xff？》、《Java中对于unsigned byte类型的转换处理》下面贴出我处理逻辑的部分代码：123val data = trainLabe.zip(trainImages).map( d =&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble)))) 下期预告&ensp;&ensp;&ensp;&ensp;下一期我将会记录我用spark MLlib中提供的朴树贝叶斯算法，对该数据集进行训练，并用测试集测试。关于该数据集呢，其实还有一点要补充的，比如把自己的手写图片转换成MNIST标准格式，还有替代MNIST手写数字集的图像数据集Fashion MNIST，这些后续都会补充进该文章。持续更新ing… 参考链接 《THE MNIST DATABASE of handwritten digits》 《（超详细）读取mnist数据集并保存成图片》 《Java中对于unsigned byte类型的转换处理》 《byte为什么要与上0xff？》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>MachineLeaning</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小技巧！从网易云音乐缓存中获取版权失效的音乐]]></title>
    <url>%2F2019%2F02%2F14%2F%E5%B0%8F%E6%8A%80%E5%B7%A7%EF%BC%81%E4%BB%8E%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E7%BC%93%E5%AD%98%E4%B8%AD%E8%8E%B7%E5%8F%96%E7%89%88%E6%9D%83%E5%A4%B1%E6%95%88%E7%9A%84%E9%9F%B3%E4%B9%90%2F</url>
    <content type="text"><![CDATA[起因&ensp;&ensp;&ensp;&ensp;前两天听了赵方婧小姐姐翻唱的《知否》，惊为天人，就点了颗心收藏了。结果今天在点开一看。居然没有版权了！！！&ensp;&ensp;&ensp;&ensp;这也不是一次两次了，充了两个会员跟没充一样，说下架就下架，喜欢的歌听不到，这怎么能忍！ 把爱曲抢救回来！&ensp;&ensp;&ensp;&ensp;想到这两天听过，电脑一定有缓存的，所以去设置里面看了下缓存目录：&ensp;&ensp;&ensp;&ensp;嗯。。。果然改了名字，看文件大小就知道这个.uc后缀的就是缓存的MP3文件了，文件名应该就是它id那这个id是什么呢？&ensp;&ensp;&ensp;&ensp;客户端看不到，我们把这首歌复制到浏览器去看看： 还是挺显眼的，参数名就叫id了哈哈哈哈拿到这个id，去文件夹搜索一下，就能找到我们想要的歌的缓存啦 &ensp;&ensp;&ensp;&ensp;然后按照原本的想法，后缀改成MP3之后，应该直接就能听了，然而。。。右键网易云音乐打开，提示解码失败！！！看来网易云也防了一手啊，大概是做了什么加密处理。 对缓存文件反向解密&ensp;&ensp;&ensp;&ensp;考虑到文件加密后大小跟原来的基本没有变化，大概是进行了与或处理。上google搜索了一下，很多人都遇到了，有大佬已经试出来了，就是逐字节跟0xa3与或一下就好了。至于大佬是怎么试出来的，我个人猜测应该是拿到缓存文件，再拿到下载文件，各取第一个字节来试一下，与或虽然不能逆运算，不过应该能拿来猜一下。下面贴出转换代码123456789101112131415161718192021222324252627282930import java.io.*;/** * @author Chenyl * @date 2019/2/14 10:41 */public class music &#123; public static void main(String[] args)&#123; try&#123; File inFile = new File(&quot;C:\\Users\\Void\\Desktop\\知否-赵方婧.mp3&quot;); File outFile = new File(&quot;C:\\Users\\Void\\Desktop\\知否-赵方婧(解).mp3&quot;); DataInputStream dis = new DataInputStream( new FileInputStream(inFile)); DataOutputStream dos = new DataOutputStream( new FileOutputStream(outFile)); byte[] by = new byte[1000]; int len; while((len=dis.read(by))!=-1)&#123; for(int i=0;i&lt;len;i++)&#123; by[i]^=0xa3; &#125; dos.write(by,0,len); &#125; dis.close(); dos.close(); &#125;catch(IOException ioe)&#123; System.err.println(ioe); &#125; &#125;&#125; &ensp;&ensp;&ensp;&ensp;解出来的文件直接就可以听啦！不过不知道是不是心理作用，总觉得音质受损了= = 一些感想此方法仅限于自己学习使用，请勿无版权传播音频。&ensp;&ensp;&ensp;&ensp;其实我也是很不愿意这样做的，各大运营商为了利益把版权弄得四分五裂，听歌都要各个软件来回切换，开四五个会员就算了，还经常突然下架歌曲，甚至有偷偷删除歌单的行为出现。希望国家早日立法，统一版权，给我们一个稳定付费的环境吧。&ensp;&ensp;&ensp;&ensp;真的，支持歌手，不差那点钱，现在是想给钱都听不到，我也是醉了。 参考链接 《网易云音乐.uc格式的缓存文件转.mp3》]]></content>
      <categories>
        <category>好玩的东西</category>
      </categories>
      <tags>
        <tag>网易云音乐</tag>
        <tag>小技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记1：K-means聚类分析]]></title>
    <url>%2F2019%2F01%2F30%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B01%EF%BC%9AK-means%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言&ensp;&ensp;&ensp;&ensp;作为该系列的第一篇文章，也是我正式步入机器学习的第一篇学习日记，想说几句话，机器学习跟普通编程不一样，门槛对于大多数人来说都是算比较高的。曾经我也在惊叹和好奇中望而却步，现在在大佬的推动下也是鼓起勇气正式开始学习，完成这个系列也是我2019的目标。由于公司用的是Spark，所以从Spark MLlib开始学起，不积硅步无以至千里，在这个系列我也会从简单一点地基础开始，一点点切实地去学习，希望看到这篇文章地小伙伴也能下定决心，一起攻克机器学习。 K-Means 聚类算法原理K-Means算法流程&ensp;&ensp;&ensp;&ensp;关于原理呢，我直接引用《使用Spark MLlib做K-mean聚类分析》里面的资料好了： 聚类分析是一个无监督学习 (Unsupervised Learning) 过程, 一般是用来对数据对象按照其特征属性进行分组，经常被应用在客户分群，欺诈检测，图像分析等领域。K-means 应该是最有名并且最经常使用的聚类算法了，其原理比较容易理解，并且聚类效果良好，有着广泛的使用。 &ensp;&ensp;&ensp;&ensp;和诸多机器学习算法一样，K-means 算法也是一个迭代式的算法，其主要步骤如下: 第一步，选择 K 个点作为初始聚类中心。 第二步，计算其余所有点到聚类中心的距离，并把每个点划分到离它最近的聚类中心所在的聚类中去。在这里，衡量距离一般有多个函数可以选择，最常用的是欧几里得距离 (Euclidean Distance), 也叫欧式距离,公式如下：其中 C 代表中心点，X 代表任意一个非中心点。 第三步，重新计算每个聚类中所有点的平均值，并将其作为新的聚类中心点。 最后，重复 (二)，(三) 步的过程，直至聚类中心不再发生改变，或者算法达到预定的迭代次数，又或聚类中心的改变小于预先设定的阀值。 &ensp;&ensp;&ensp;&ensp;光这么看有点不是很直观，其实就是不断换聚类中心以求达到“最合理”的分类的过程，下面我贴一张数据可视化的图，更直观地描述这个过程：&ensp;&ensp;&ensp;&ensp;如图，k-means算法在每一次迭代中，选择了更靠谱的红蓝中心点，试图让分类更合理，跟均匀。 欧式距离&ensp;&ensp;&ensp;&ensp;欧几里得公式的含义还是比较简单的，但是一开始看还是有点懵，在这里我稍微提一下自己的理解，如果不对的话欢迎留言指正=。=首先我们先回顾一下那个公式，那个n代表的是什么呢？ &ensp;&ensp;&ensp;&ensp;我们先来看看欧式距离在二维和三维中的展开：&ensp;&ensp;&ensp;&ensp;是不是觉得很熟悉咧，就是求坐标之间的距离嘛，以此类推，在4维空间5维空间或者更高维度的空间中怎么算两点坐标距离呢？那就是原版的欧式距离公式啦！n在我看来，就是代表所求的维度数嘛，对应到实际应用中，这个“维度”对应的就是我们想要聚类的对象的参数（特征）。当n个参数可以确定一个唯一（或近似）的对象，我们通过欧式距离公式计算这个对象坐标的距离，把最后n维空间坐标相近的对象聚到同一个类别里，就完成了K-Means聚类算法要做的工作了。 聚类测试数据&ensp;&ensp;&ensp;&ensp;在本文中，我们所用到目标数据集是来自UCI Machine Learning Repository的Wholesale customer Data Set。UCI是一个关于机器学习测试数据的下载中心站点，里面包含了适用于做聚类，分群，回归等各种机器学习问题的数据集。&ensp;&ensp;&ensp;&ensp;Wholesale customer Data Set是引用某批发经销商的客户在各种类别产品上的年消费数。为了方便处理，本文把原始的CSV格式转化成了两个文本文件，取前面20行作为测试集，其余的数据作为训练集。如图： 代码样例&ensp;&ensp;&ensp;&ensp;看注释就好了，基本就是处理好数据，然后调用spark-lib包里面的kmeans方法去做训练就ok了，源码分析大概会在做完这个系列之后再考虑要不要做。。。&ensp;&ensp;&ensp;&ensp;值得注意的是：val clusters:KMeansModel = KMeans.train(parsedTrainingData, numClusters, numIterations,runTimes)这行代码，我来介绍一下这几个参数： parsedTrainingData： 处理好的训练数据集 numClusters： 聚类的个数。这个值的选取有点学问，K的选择是K-means算法的关键，Spark MLlib在KMeansModel类里提供了computeCost方法，我的demo中也有该方法调用例子。该方法通过计算所有数据点到其最近的中心点的平方和来评估聚类的效果。一般来说，同样的迭代次数和算法跑的次数，这个值越小代表聚类的效果越好。但是在实际情况下，我们还要考虑到聚类结果的可解释性，不能一味的选择使computeCost结果值最小的那个K numIterations： K-means算法的迭代次数 runTimes： K-means算法run的次数 完整代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * @author Chenyl * @date 2019/1/23 15:57 */import org.apache.spark.&#123;SparkContext, SparkConf&#125;import org.apache.spark.mllib.clustering.&#123;KMeans, KMeansModel&#125;import org.apache.spark.mllib.linalg.Vectorsobject KMeansClustering &#123; def main (args: Array[String]) &#123; val trainingPath = &quot;src/main/resources/Wholesale customers data_training.txt&quot; //训练数据集文件路径 val testPath = &quot;src/main/resources/Wholesale customers data_test.txt&quot; //测试数据集文件路径 val numClusters = 8 //聚类的个数 val numIterations = 30 //K-means 算法的迭代次数 val runTimes = 3 //K-means 算法 run 的次数 val conf = new SparkConf().setAppName(&quot;Spark MLlib Exercise:K-Means Clustering&quot;) conf //TODO: 生成打包前，需注释掉此行 .setMaster(&quot;local[*]&quot;) .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;ERROR&quot;) //设置日志级别，info会有很多运行日志出现，也可以打开看看 /** *Channel Region Fresh Milk Grocery Frozen Detergents_Paper Delicassen * 2 3 12669 9656 7561 214 2674 1338 * 2 3 7057 9810 9568 1762 3293 1776 * 2 3 6353 8808 7684 2405 3516 7844 */ val rawTrainingData = sc.textFile(trainingPath) //加载测试数据 //去表头处理 val parsedTrainingData = rawTrainingData.filter(!isColumnNameLine(_)).map(line =&gt; &#123; Vectors.dense(line.split(&quot;\t&quot;).map(_.trim).filter(!&quot;&quot;.equals(_)).map(_.toDouble)) &#125;).cache()// findK(parsedTrainingData) // Cluster the data into two classes using KMeans var clusterIndex:Int = 0 //使用K-Means训练 val clusters:KMeansModel = KMeans.train(parsedTrainingData, numClusters, numIterations,runTimes) println(&quot;Cluster Number:&quot; + clusters.clusterCenters.length) println(&quot;Cluster Centers Information Overview:&quot;) clusters.clusterCenters.foreach(x =&gt; &#123; println(&quot;Center Point of Cluster &quot; + clusterIndex + &quot;:&quot;) println(x) clusterIndex += 1 &#125;) //begin to check which cluster each test data belongs to based on the clustering result val rawTestData = sc.textFile(testPath) //加载测试数据 val parsedTestData = rawTestData.map(line =&gt;&#123; Vectors.dense(line.split(&quot;\t&quot;).map(_.trim).filter(!&quot;&quot;.equals(_)).map(_.toDouble)) &#125;) parsedTestData.collect().foreach(testDataLine =&gt; &#123; val predictedClusterIndex: Int = clusters.predict(testDataLine) //使用训练好的模型进行分类 println(&quot;The data &quot; + testDataLine.toString + &quot; belongs to cluster &quot; +predictedClusterIndex) &#125;) println(&quot;Spark MLlib K-means clustering test finished.&quot;) &#125; private def isColumnNameLine(line:String):Boolean = &#123; if (line != null &amp;&amp; line.contains(&quot;Channel&quot;)) true else false &#125; /** * 查看最佳的K值 * @param parsedTrainingData */ private def findK(parsedTrainingData:org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]): Unit =&#123; val ks:Array[Int] = Array(3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20) ks.foreach(cluster =&gt; &#123; val model:KMeansModel = KMeans.train(parsedTrainingData, cluster,30,1) val ssd = model.computeCost(parsedTrainingData) println(&quot;sum of squared distances of points to their nearest center when k=&quot; + cluster + &quot; -&gt; &quot;+ ssd) &#125;) &#125;&#125; K-Means应用场景适用场景&ensp;&ensp;&ensp;&ensp;K-means常用的场景是在不清楚用户有几类时，尝试性的将用户进行分类，并根据每类用户的不同特征，决定下步动作。一个典型的应用场景是CRM管理中的数据库营销。举例，对于一个超市/电商网站/综合零售商，可以根据用户的购买行为，将其分为“年轻白领”、“一家三口”、“家有一老”、”初得子女“等等类型，然后通过邮件、短信、推送通知等，向其发起不同的优惠活动。 &ensp;&ensp;&ensp;&ensp;明尼苏达州一家塔吉特门店被客户投诉，一位中年男子指控塔吉特将婴儿产品优惠券寄给他的女儿 —— 一个高中生。但没多久他却来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。塔吉特百货就是靠着分析用户所有的购物数据，然后通过相关关系分析得出事情的真实状况。 &ensp;&ensp;&ensp;&ensp;在这个案例中，那个高中生少女明显是被聚到了孕妇那一类，因为她的行为模式与孕妇是很相近的。&ensp;&ensp;&ensp;&ensp;总的来说，聚类算法大多用在用户画像、客户分群之类的，也可用于图像分析（色块相近聚集，后续会有一篇文章介绍kmeans在图像分析上的应用）另外附上一篇《K-Means算法的10个有趣用例》,有兴趣的可以详细看看 不适用场景每一种算法都不是放之四海而皆准的，下面是 K-Means 算法不适用的两个场景：这种情况如果由人来聚类，分分钟搞定，但是 K-Means 算法是这样做的：下面这种情况更微妙一些，各个子集的密集程度相差很大：看看 K-Means 算法是怎样做的：好吧，的确是差成渣了，但这真的不是算法的问题，而是我们人类对算法的选择问题。 结束语 k-means算法无论是原理还是代码MLlib的使用都是较为简单，但是在某些领域其效果却是出奇的好，作为入门机器学习的第一步，K-means也向我们掀开了机器学习世界神秘面纱的一角，给我们这些初学者极大的信心。k-means只是机器学习中的一个算法工具，更多算法，将会在后续的日子中一一学习掌握，我也会即时把学习的一些心得写成系列文章，与君共勉。 参考链接 《Spark机器学习2：K-Means聚类算法》 《K-Means算法的10个有趣用例》 《Wholesale customer Data Set》 《使用Spark MLlib做K-mean聚类分析》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>MachineLeaning</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kerberos探索遇到的问题及心得]]></title>
    <url>%2F2019%2F01%2F27%2FKerberos%E6%8E%A2%E7%B4%A2%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[Kerberos在集群开启时做了什么kerberos在浪潮集群中启动时会自动安装clien端，此时它会针对每个服务以及机器创建对应得Principal，这些principal也将创建成Linux内的用户。 注意，这些用户将只存在于创建这些用户的机器上 关于Hive执行HQL时count(*)会出错 Hive跟HDFS一样，在集群开启Kerberos后不需要特别的设置即可使用，然而在试图执行 1select count( * ) from test 时，会出现以下报错: 原因便是上面提到的，创建的用户只在创建的机器上有，而Hive这个count其实是启动了一个MapReduce在集群上跑，集群上其他的机器没有nassoft_m这个账号，所以报错说找不到用户名。 解决办法： &ensp;&ensp;&ensp;&ensp;在每台机都加上nassoft_m这个账号即可 关于Hive遇到的权限问题 hive很多操作都是对HDFS的，而hdfs默认文件权限为drwxr-xr-x ，即文件创建者拥有全权限，同组及其他用户只拥有读取权限，然而我们登陆的principal很多情况下都不是文件创建者，所以在kerberos启动的情况下使用hive，又时会出现权限不足的报错，就是因为hive可能对hdfs进行了写入操作。 解决办法： &ensp;&ensp;&ensp;&ensp;把nassoft_m账号添加到hdfs用户组里，然后使用chmod 777(或者774)命令修改权限 查看zookeeper节点认证失败 解决办法：在zkCli.sh 中加入 1JVMFLAGS=&quot;-Djava.security.auth.login.config=/usr/hdp/2.6.4.0-91/zookeeper/conf/zookeeper_jaas.conf&quot; 关于Hbase出现的权限不足报错认证后在hbase中使用命令行出现如下错误： 解决办法：给相应的用户分配权限1grant &apos;nassoft_r&apos;,&apos;RWXCA&apos; 关于kafka在Kerberos中的使用命令行稍有变化 生产者端命令为：bin/kafka-console-producer.sh –broker-list hd2.bigdata:6667 –topic test_wifi2 –producer.config conf/producer.properties –security-protocol SASL_PLAINTEXT 消费者端命令为：bin/kafka-console-consumer.sh –bootstrap-server hd2.bigdata:6667 –from-beginning –topic test_wifi2 conf/consumer.properties –security-protocol SASL_PLAINTEXT 另外就是java代码中需要添加以下配置：12345678System.setProperty(&quot;java.security.krb5.conf&quot;, &quot;E:\\krb5.conf&quot;);System.setProperty(&quot;java.security.auth.login.config&quot;, &quot;E:\\kafka_client_jaas.conf&quot;);Properties props = new Properties();props.put(&quot;security.protocol&quot;, &quot;SASL_PLAINTEXT&quot;);props.put(&quot;sasl.mechanism&quot;, &quot;GSSAPI&quot;);props.put(&quot;sasl.kerberos.service.name&quot;, &quot;kafka&quot;);...... 需要从服务器下载以下3个文件： krb5.conf kafka_client_jaas.conf kafka.service.keytab 其中kafka_client_jaas.conf稍作修改：1234567891011121314151617181920Client&#123; com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useTicketCache=true principal=&quot;kafka/hd2.bigdata@HADOOP.COM&quot; useKeyTab=true serviceName=&quot;zookeeper&quot; keyTab=&quot;E:\\kafka.service.keytab&quot; client=true;&#125;;KafkaClient &#123; com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useTicketCache=true principal=&quot;kafka/hd2.bigdata@HADOOP.COM&quot; useKeyTab=true serviceName=&quot;kafka&quot; keyTab=&quot;E:\\kafka.service.keytab&quot; client=true;&#125;; solr的相关问题12Kinit nassoft_mcurl --negotiate -u : &quot;http://hd1.bigdata:8983/solr/&quot; 解决方法：1234server/scripts/cloud-scripts/zkcli.sh -zkhost hd2.bigdata:2181,master2.bigdata:2181,master1.bigdata:2181 -cmd put /security.json &apos;&#123;&quot;authentication&quot;:&#123;&quot;class&quot;: &quot;org.apache.solr.security.KerberosPlugin&quot;&#125;&#125;&apos;刷新solr keytab修改 keytab 权限重启solr集群 curl --negotiate -u : &quot;http://hd1.bigdata:8983/solr/&quot;成功]]></content>
      <categories>
        <category>集群</category>
      </categories>
      <tags>
        <tag>Keeberos</tag>
        <tag>集群安全</tag>
      </tags>
  </entry>
</search>
