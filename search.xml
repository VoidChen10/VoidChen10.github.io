<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一次es内存使用异常引发的思考]]></title>
    <url>%2F%E4%B8%80%E6%AC%A1es%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E5%BC%82%E5%B8%B8%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[1、起因某天java组的同事跟我说，es查询报错了，报了个Data too large异常，网上找了一下，好像这个内存不足是es默认设置引起的，具体的再分析看看。 详细的报错信息如下： 查询的json： 1234567891011121314151617181920212223242526272829303132333435363738394041get &#x2F;es_ic_mobile_20200401_20200430&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;capTimeFormat&quot;: &#123; &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;gte&quot;: &quot;2020-03-06&quot;, &quot;lte&quot;: &quot;2020-05-07&quot; &#125; &#125; &#125;, &quot;sort&quot;: [&#123; &quot;capTimeFormat&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;, &quot;_id&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;], &quot;size&quot;: 20&#125; 。 2、内存不足是怎么引起的？官方文档关于这个问题有很详细的解释: 限制内存使用 设想我们正在对日志进行索引，每天使用一个新的索引。通常我们只对过去一两天的数据感兴趣，尽管我们会保留老的索引，但我们很少需要查询它们。不过如果采用默认设置，旧索引的 fielddata 永远不会从缓存中回收！ fieldata 会保持增长直到 fielddata 发生断熔（请参阅 断路器），这样我们就无法载入更多的 fielddata。 显然，我们在装好华为集群的时候就没改过es的设置，用的设置也就是默认设置了，所以当缓存容量达到了熔断器的阈值的时候，每一次需要加载数据到内存的操作，都会因为触发熔断而直接返回报错：[FIELDDATA] Data too large, data for [proccessDate] would be larger than limit of [10307921510/9.5gb]] 网上找了个图更好理解： 而如果使用默认参数，就是相当于断路器那条红线跟驱逐界限那条蓝线重叠，这就意味着，永远不会触发回收，因为内存累积满了之后下一次查询会直接触发熔断，然后内存里面的数据也不会有变化。 值得注意的是，es的fielddata只会针对string类型数据做缓存，long、int、date等数据皆不会缓存起来。 3、解决办法官网也很贴心的给了出来： 为了防止发生这样的事情，可以通过在 config/elasticsearch.yml 文件中增加配置为 fielddata 设置一个上限： `indices.fielddata.cache.size: 20%` ( ps:*可以设置堆大小的百分比，也可以是某个值，例如： 5gb 。*)有了这个设置，最久未使用（LRU）的 fielddata 会被回收为新数据腾出空间。 在华为集群中，我看了下，只有indices.breaker.fielddata.limit这个参数 然后官网也说是新增配置，考虑到熔断器设置在40%，所以缓存大小设置到20%或者30%就好了，目标就是让他不至于一次性查询就触发熔断。当然最好还是要用大内存的。 4、为什么这个配置默认不配呢明明是挺重要的一个参数，这么不配迟早内存会用完的啊，仔细看了文档之后才明白，这个居然是刻意这么做的！ 这个默认设置是刻意选择的：fielddata 不是临时缓存。它是驻留内存里的数据结构，必须可以快速执行访问，而且构建它的代价十分高昂。如果每个请求都重载数据，性能会十分糟糕。 这个设置是一个安全卫士，而非内存不足的解决方案。 如果没有足够空间可以将 fielddata 保留在内存中，Elasticsearch 就会时刻从磁盘重载数据，并回收其他数据以获得更多空间。内存的回收机制会导致重度磁盘I/O，并且在内存中生成很多垃圾，这些垃圾必须在晚些时候被回收掉。 说到底，还是需要资源把它堆起来，才能发挥最大的性能，之前查资料有看到过，要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。 当然，这个问题只在es 7.0版本之前存在，7.0之后会引入一种数据类型叫doc_values，用以解决这种es聚合带来的内存开销。 其核心的思想其实也很简单，原本的es存储结构为倒排索引，这个结构搜索快，但是要做聚合就只能全拿到内存去，7.0之后的版本就是给了个doc_values类型给你选择，标记为这种类型的数据会额外多存一份正排的索引，专门用于聚合查询。 关于倒排索引之前也有过一段研究，有时间再好好记录下来吧。]]></content>
      <categories>
        <category>es</category>
      </categories>
      <tags>
        <tag>es</tag>
        <tag>内存使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArchLinux安装实况]]></title>
    <url>%2FArchLinux%E5%AE%89%E8%A3%85%E5%AE%9E%E5%86%B5%2F</url>
    <content type="text"><![CDATA[最终效果 本来以前一直是用ubuntu作为日常使用系统的，没想到买了联想yoga 14s之后，这个4800U新到ubuntu内核都没驱动，然后archlinux也要换成zen内核才有，于是只能用archlinux试试了。 结果一试发现好爽。。回不去了。。kde的华丽效果，居然还没有一丝卡顿，开机内存也仅仅只是一个G，就这么用着试试先吧= = 前期安装官网下载：https://www.archlinux.org/ 安装指南：https://wiki.archlinux.org/index.php/Installation_guide （中文） 参考博客：https://www.viseator.com/2017/05/17/arch_install/ 1、Arch开始arch Linux没有图形界面，U盘启动之后选第一个，进去shell界面 2、联网arch linux的安装必须联网，安装包都得网上下载所以必须要先联网 无线网络 1wifi-menu 有线网络并且路由器支持DHCP 1dhcpcd 检查 1ping www.baidu.com 3、更新系统时间1timedatectl set-ntp true 4、分区与格式化4.1. 标准UEFI分区参考: 4.2. fdisk分区操作4.2.1.重置磁盘安装要是空硬盘就最好，如果不是空硬盘就先重置硬盘吧 1fdisk -l #列出磁盘情况 1234fdisk /dev/nvme0n1 #进入fdisk操作界面，参数为你的磁盘名字d #操作界面下d为删除，提示你输入数字表示要删除哪个分区，重复操作至全部删除完w #保存操作并退出，之后就是一个空得磁盘了 4.2.2. 重新分区参考标准分区来，有条件可以独立给/home 一个分区，这样重装就可以不用动他 123456fdisk /dev/nvme0n1 #进入fdisk操作界面，参数为你的磁盘名n #创建分区，后续会让你输入创建第几个分区，分区从那开始，这些都默认回车就好 #到最后选择哪里结束的时候看着大小来分配，比如引导分区要512M就输入： +512Mt #选择新创建的分区序号来更改分区的类型，输入l可以查看所有支持的类型和代码，输入代码：1更改分类型为EFIw #重复以上建完所有分区之后保存并退出 4.2.3. 格式化分区12345mkfs.fat -F32 /dev/nvme0n1p1 #格式化EFI分区mkfs.ext4 /dev/nvme0n1p2 #格式化根分区mkswap /dev/nvme0n1p3 #格式化交换分区swapon /dev/nvme0n1p3 4.2.4. 挂载分区1234mount /dev/nvme0n1p2 /mnt #将根分区挂载到/mntmkdir /mnt/boot #创建/boot文件夹并将引导分区挂载到上面mount /dev/nvme0n1p1 /mnt/boot 5、安装系统5.1. 更换安装源镜像源是我们下载的软件包的来源，我们需要根据自己的地区选择不同的源来加快下载的速度。 执行以下命令，用Vim来编辑/etc/pacman.d/mirrorlist这个文件 123456789vim /etc/pacman.d/mirrorlist##小技巧/n China #快速查找中国的服务器，n往下，N往上dd #剪切当前光标行gg #回到文件第一行P #粘贴到当前行wq #保存退出 我们的目标是把中国源放最上面去，找包会按顺序找起 5.2. 安装系统和基本包下面就要安装最基本的ArchLinux包到磁盘上了。这是一个联网下载并安装的过程。 执行以下命令： 1pacstrap /mnt base base-devel linux linux-firmware dhcpcd 如果不想用默认的arch内核，可以用激进的zen内核，那么这里就换一个内核包 1pacstrap /mnt base base-devel linux-zen linux-firmware dhcpcd 当然内核也能以后再更换。 根据下载速度的不同在这里需要等待一段时间，当命令提示符重新出现的时候就可以进行下一步操作了。 5.3. 配置Fstab生成自动挂载分区的fstab文件，执行以下命令： 1genfstab -L /mnt &gt;&gt; /mnt/etc/fstab 由于这步比较重要，所以我们需要输出生成的文件来检查是否正确，执行以下命令： 1cat &#x2F;mnt&#x2F;etc&#x2F;fstab 5.4. Chroot到新装系统Chroot意为Change root，相当于把操纵权交给我们新安装（或已经存在）的Linux系统，执行了这步以后，我们的操作都相当于在磁盘上新装的系统中进行。 执行如下命令： 1arch-chroot &#x2F;mnt 这里顺便说一下，如果以后我们的系统出现了问题，只要插入U盘并启动， 将我们的系统根分区挂载到了/mnt下（如果有efi分区也要挂载到/mnt/boot下），再通过这条命令就可以进入我们的系统进行修复操作。 5.5. 设置时区依次执行如下命令设置我们的时区为上海并生成相关文件： 123ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimehwclock --systohcdate #看下时间对不对 5.6. 安装基本包安装包的命令格式为pacman -S 包名，pacman会自动检查这个包所需要的其他包（即为依赖）并一起装上。下面我们就通过pacman来安装一些包，这些包在之后会用上，在这里先提前装好。 1pacman -S vim dialog wpa_supplicant ntfs-3g networkmanager netctl 一路确认之后包就被成功装上了。 5.7. 设置Locale设置我们使用的语言选项，执行如下命令来编辑/etc/locale.gen文件： 123vim &#x2F;etc&#x2F;locale.genlocale-gen 在文件中找到zh_CN.UTF-8 UTF-8 zh_HK.UTF-8 UTF-8 zh_TW.UTF-8 UTF-8 en_US.UTF-8 UTF-8这四行，去掉行首的#号，保存退出并执行locale-gen。 打开（不存在时会创建）/etc/locale.conf文件： 1vim &#x2F;etc&#x2F;locale.conf 在文件的第一行加入以下内容： 1LANG&#x3D;en_US.UTF-8 保存并退出。 5.8. 设置主机名打开（不存在时会创建）/etc/hostname文件： 1vim &#x2F;etc&#x2F;hostname 在文件的第一行输入你自己设定的一个myhostname 保存并退出。 编辑/etc/hosts文件： 1vim &#x2F;etc&#x2F;hosts 在文件末添加如下内容（将myhostname替换成你自己设定的主机名） 123127.0.0.1 localhost::1 localhost127.0.1.1 myhostname.localdomain myhostname 保存并退出。 5.9. 设置Root密码Root是Linux中具有最高权限帐户，有些敏感的操作必须通过Root用户进行，比如使用pacman，我们之前进行所有的操作也都是以Root用户进行的，也正是因为Root的权限过高，如果使用不当会造成安全问题，所以我们之后会新建一个普通用户来进行日常的操作。在这里我们需要为Root帐户设置一个密码： 1passwd 按提示设置并确认就可以了。 5.10. 安装CPU升级补丁如果为intel cpu 1pacman -S intel-ucode 如果为amd cpu 1pacman -S amd-ucode 5.11. 安装Bootloader经常听说很多人因为引导问题导致系统安装失败，多数是因为教程没有统一或是过时的教程引起的，这里只要按照步骤来其实是不难的。这里我们安装最流行的Grub2。（如果曾经装过Linux，记得删掉原来的Grub，否则不可能成功启动） 首先安装os-prober和ntfs-3g这两个包，它可以配合Grub检测已经存在的系统，自动设置启动选项。 1pacman -S os-prober ntfs-3g 如果为EFI/GPT引导方式： 安装grub与efibootmgr两个包： 1pacman -S grub efibootmgr 生成配置文件： 1grub-mkconfig -o &#x2F;boot&#x2F;grub&#x2F;grub.cfg 部署grub： 1grub-install --target&#x3D;x86_64-efi --efi-directory&#x3D;&#x2F;boot --bootloader-id&#x3D;grub 出现Installation finished. No error reported.字样表示安装成功。 5.12. 取消挂载并重启至此archlinux其实已经安装好了，接下来，你需要进行重启来启动已经安装好的系统，执行如下命令： 1exit 如果挂载了/mnt/boot，先umount /mnt/boot，再umount /mnt，否则直接umount /mnt： 123umount &#x2F;mnt&#x2F;bootumount &#x2F;mntreboot 注意这个时候你可能会卡在有两行提示的地方无法正常关机，长按电源键强制关机即可，没有影响。 关机后拔出U盘，启动顺序会自动以硬盘启动，如果一切顺利，你将看到一个shell登陆界面。 输入root，再输入之前设置的密码，显示出命令提示符，恭喜你，你已经成功安装ArchLinux！ 后期定制及美化安装指南：https://wiki.archlinux.org/index.php/General_recommendations （中文） 参考博客：https://www.viseator.com/2017/05/19/arch_setup/ 1、更换内核arch linux的安装必须联网，安装包都得网上下载所以必须要先联网 无线网络 1wifi-menu 有线网络并且路由器支持DHCP 1dhcpcd 检查 1ping www.baidu.com 参考wiki：https://wiki.archlinux.org/index.php/Kernel 参考博客： https://poemdear.com/2019/03/27/arch-linux-%E6%9B%B4%E6%8D%A2%E5%88%B0%E7%A8%B3%E5%AE%9A%E7%89%88lts%E5%86%85%E6%A0%B8/ 这里我会换成zen内核，所以操作如下： 1234pacman -S linux-zen ##安装内核pacman -Rsdd linux ##卸载旧的内核grub-mkconfig -o /boot/grub/grub.cfg ##生成zen版本的grub.cfgreboot 2、创建普通用户在这之前所有操作都是以root用户的身份进行的，由于root的权限过高，日常使用root用户是不安全的。Linux为我们提供了强大的用户与组的权限管理，提高了整个系统的安全性。这里我们就来新建一个用户。 执行以下命令来创建一个名为username的用户（请自行替换username为你的用户名）： 1useradd -m -G wheel username （请自行替换username为你的用户名） 在这里稍微解释一下各参数的含义： -m：在创建时同时在/home目录下创建一个与用户名同名的文件夹，这个目录就是你的家目录啦！家目录有一个别名是~，你可以在任何地方使用~来代替家目录路径。这个神奇的目录将会用于存放你所有的个人资料、配置文件等所有跟系统本身无关的资料。 -G wheel：-G代表把用户加入一个组，对用户与组的概念感兴趣的同学可以自行查找有关资料学习。后面跟着的wheel就是加入的组名，至于为什么要加入这个组，后面会提到。 当然记得为新用户设置一个密码，执行如下命令： 1passwd username （请自行替换username为你的用户名） 根据提示输入两次密码就可以了，注意，这是你的用户密码，推荐与之前设置的root用户的密码不同。 3、日常配置3.1. 配置sudo和vimsudo本身也是一个软件包，所以我们需要通过pacman来安装（没想到连这个都没有。。。）： 1pacman -S sudo 接下来我们需要用专门的visudo命令来编辑sudo的配置文件（前面这句是为了创建vim到vi的软链接避免找不到vi编辑器）： 12ln -s /usr/bin/vim /usr/bin/vivisudo 实际上就是vim的操作，使用它是为了对编辑后的文件进行检查防止格式的错误。 找到 1# %wheel ALL=(ALL)ALL 这行，去掉之前的#注释符，保存并退出就可以了。 这里的%wheel就是代表wheel组，意味着wheel组中的所有用户都可以使用sudo命令。 当然为了安全使用sudo命令还是需要输入当前用户的密码的。 配置好sudo以后，我们进行一次重启，重启以后输入你刚创建的用户名与密码来登录。注意登录后要重新进行联网操作。 3.2. 启用 NTP 同步时间服务1sudo systemctl enable systemd-timesyncd.service 3.3. 安装 CPU 调频守护程序CPU 调频守护程序可以帮助我们更好的控制 CPU 的频率和功耗 安装：sudo pacman -S cpupower 启用：sudo systemctl enable cpupower 3.4. 安装蓝牙支持安装：sudo pacman -S bluedevil bluez bluez-utils pulseaudio-bluetooth 启用：sudo systemctl enable --now bluetooth 4、安装图形界面4.1. 显卡驱动的安装 参照这个表格，安装相应的包，比如你是amd的集成显卡（绝大多数人的情况），执行： 1sudo pacman -S xf86-video-amdgpu 4.2. 安装XorgXorg是Linux下的一个著名的开源图形服务，我们的桌面环境需要Xorg的支持。 执行如下命令安装Xorg及相关组件： 1sudo pacman -S xorg 回车和y确认即可 4.3. 安装KDE(Plasma)直接安装软件包组（包含了很多软件包）即可： 12345678910111213141516sudo pacman -S plasma kde-applications5 ARK: 一个允许你操作各种格式的压缩文件的程序13 dolphin： KDE Plasma 桌面的默认，易用文件管理器。14 dolphin-plugins： KDE Plasma配置工具21 gwenview: 一个快速容用的图像浏览器。36 kate: 一个简单但功能强大的文本编辑器。52 kdeconnect: 是一个允许所有设备与手机的连接的工具55 kdenetwork-filesharing: 文件分享68 kget: 下载管理器，兼简易的 BT 下载客户端99 knote: 把你的笔记粘到桌面上或是Kontact中。103 Kompare: 清晰的显示 2 个版本文本文件的差异。106 konsole: 一个多标签的终端模拟器,允许使用多种shell和用户配置。122 KSystemLog: KDE 工作空间的系统日志查看器.138 okular: KDE 的通用文档查看器。148 spectacle: 截图软件 4.4. 安装桌面管理器sddm安装好了桌面环境包以后，我们需要安装一个图形化的桌面管理器来帮助我们登录并且选择我们使用的桌面环境，这里我推荐使用sddm。执行： 1sudo pacman -S sddm 4.5. 设置开机启动sddm服务这里就要介绍一下Arch下用于管理系统服务的命令systemctl了，服务的作用就是字面意思，为我们提供特定的服务，比如sddm就为我们提供了启动xorg与管理桌面环境的服务。 命令的使用并不复杂： 1234sudo systemctl start 服务名 （启动一项服务）sudo systemctl stop 服务名 （停止一项服务）sudo systemctl enable 服务名 （开机启动一项服务）sudo systemctl disable 服务名 （取消开机启动一项服务） 所以这里我们就执行下面命令来设置开机启动sddm： 1sudo systemctl enable sddm 5、重新配置网络到现在我们已经安装好了桌面环境，但是还有一件事情需要我们提前设置一下。由于我们之前使用的一直都是netctl这个自带的网络服务，而桌面环境使用的是NetworkManager这个网络服务，所以我们需要禁用netctl并启用NetworkManager： 12sudo systemctl disable netctlsudo systemctl enable NetworkManager （注意大小写） 同时你可能需要安装工具栏工具来显示网络设置图标（某些桌面环境已经装了，但是为了保险可以再装一下）： 1sudo pacman -S network-manager-applet 这样开机以后我们就可以在图形界面下配置我们的网络啦。重启继续美化 6、安装日用软件6.1. 安装chrome浏览器Chromium 是开源版的 Google Chrome，它们的区别： https://en.wikipedia.org/wiki/Chromium_(web_browser)#Differences_from_Google_Chrome 1.程序图标：两者图标只在色彩上不同，Chromium是天蓝色，而Chrome是Google公司的代表色（红、黄、蓝、绿）；2.自动更新：Chromium不开放自动更新功能，所以用户需要手动下载更新，而Chrome则可自动脸上Google的服务器更新，但新版的推出很慢；3.安装模式：Chromium可以免安装，下载zip压缩包后解压即可使用，而Chrome则只有安装板；4.功能差异：新功能会率先在Chromium上推出，Chrome则会相对落后很多。 安装命令： 1sudo pacman -S chromium 6.2. 安装中文字体及输入法不装的话中文会变成小方格，省事我们就装文泉驿好了，有更好看的再美化 1sudo pacman -S noto-fonts noto-fonts-cjk noto-fonts-extra ttf-liberation ttf-dejavu 安装中文输入法，这个remi输入法比搜狗好用多了= = 12pacman -S fcitx fcitx-rime fcitx-im kcm-fcitxvim ~/.xprofile 空文件写入这些本地化配置： 12345678910111213export LANG=zh_CN.UTF-8export LC_CTYPE=en_US.UTF-8export LC_NUMERIC=zh_CN.UTF-8export LC_TIME=zh_CN.UTF-8export LC_COLLATE=zh_CN.UTF-8export LC_MONETARY=zh_CN.UTF-8export LC_MESSAGES=zh_CN.UTF-8export LC_ALL=zh_CN.UTF-8export GTK_IM_MODULE=fcitxexport QT_IM_MODULE=fcitxexport XMODIFIERS="@im=fcitx" qw保存之后reboot重启即可 注意：这里这么配置会把整个环境完全变成中文环境，如果你像我一样不希望shell提示也变成中文的话，就不要配这个，在系统的设置中心里面加中文就可以了 6.3. Typora为了可以先用这笔记本开始记录安装实况，所以先装了这个md编辑器。 还没有装AUR工具，所以先在AUR仓库搜索这个软件包。 地址：https://aur.archlinux.org/packages/typora/ 点击右边的Download snapshot下载压缩包到本地，然后解压安装： 123tar -xvf typora.tar.gzcd typoramakepkg -si 6.4. 安装AUR工具这里我选择yay,主要是看起来比较短= = 先安装下git：sudo pacman -S git 还是先在AUR仓库搜索这个软件包。 地址：https://aur.archlinux.org/packages/yay/ 点击右边的Download snapshot下载压缩包到本地，然后解压安装： 123tar -xvf yay.tar.gz cd yaymakepkg -si 好的这种官方的方式不行，傻逼go语言编译还要再远程拉包，然后我们被墙了。。。 好在在github看到有人提了issues，我们加个国内的镜像库，参考链接：https://www.archlinuxcn.org/archlinux-cn-repo-and-mirror/ 12345678910sudo vim /etc/pacman.conf #文末加入[archlinuxcn]Server = https://repo.archlinuxcn.org/$arch#保存推出后刷新并导入PGP Keyssudo pacman -Syy &amp;&amp; sudo pacman -S archlinuxcn-keyringsudo pacman -S yay 6.5. 修复discover这个鬼东西默认装好还用不了，利用sudo pacman -Qi discover查看一下依赖，把必选和可选的依赖都装上： 123sudo pacman -S knewstuff kitemmodels kdeclarative qt5-graphicaleffects appstream-qt archlinux-appstream-data hicolor-icon-theme kirigami2 discount kuserfeedbacksudo pacman -S packagekit-qt5 flatpak fwupd 6.6. 安装ZSH用惯了这个，没有好不习惯，先装回来： 123456789101112131415161718192021sudo pacman -S zsh git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zshcp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrccd ~/.oh-my-zsh/custom/plugins#高亮插件git clone git://github.com/zsh-users/zsh-syntax-highlighting.git#命令补齐git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestionsvim ~/.zshrc #在文本内添加： #设置主题，内置主题可前往github查看：https://github.com/robbyrussell/oh-my-zsh/wiki/ThemesZSH_THEME="robbyrussell"#添加高亮插件和自动补齐plugins=(git zsh-syntax-highlighting zsh-autosuggestions web-search)source /etc/profile #文末添加#wq保存后刷新一下source ~/.zshrc 6.7. 跳过grub修改/etc/default/grub文件 123sudo vim /etc/default/grub## GRUB_TIMEOUT=5 把这个改成0后wq保存退出sudo grub-mkconfig -o /boot/grub/grub.cfg 6.8. 安装解码器Kid3 — MP3, Ogg/Vorbis, FLAC, MPC, MP4/AAC, MP2, Speex, TrueAudio, WavPack, WMA, WAV and AIFF files tag editor. 1sudo pacman -S kid3 7、美化环境7.1. 安装latte-dock所谓的dock就是一个停靠栏面板，安装好启动一下就可以设置了: 1sudo pacman -S latte-dock 7.2. 挑选主题和图标安装网页工具： 1sudo pacman -S ocs-url 去官网挑选主题： https://store.kde.org/p/1354062https://store.kde.org/p/1294729 7.3. 安装QQ和微信直接装如果依赖报错，如下： 12345678910111213141516171819➜ ~ sudo pacman -S deepin.com.qq.im [sudo] lin 的密码：正在解析依赖关系...警告：无法解决 "lib32-gettext"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-libxcursor"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-fontconfig"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-mesa"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-lcms2"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-libjpeg6"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-libpulse"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-alsa-plugins"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-libxml2"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-libxrandr"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-libxi"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-glu"，"deepin-wine" 的一个依赖关系警告：无法解决 "lib32-libldap"，"deepin-wine" 的一个依赖关系警告：无法解决 "deepin-wine"，"deepin.com.qq.im" 的一个依赖关系:: 因为无法解决依赖关系，以下软件包无法进行更新： deepin.com.qq.im 那么你跟我一样是有个32位包没装，先装一下再执行: 使用文本编辑器打开/etc/pacman.conf，找到 12#[multilib]#Include &#x3D; &#x2F;etc&#x2F;pacman.d&#x2F;mirrorlist 将之修改为 12[multilib]Include &#x3D; &#x2F;etc&#x2F;pacman.d&#x2F;mirrorlist 而后sudo pacman -Syyu 接下来就能正常安装了： 12345sudo pacman -S deepin.com.qq.imsudo pacman -S gnome-settings-daemonsudo cp /etc/xdg/autostart/org.gnome.SettingsDaemon.XSettings.desktop ~/.config/autostart/sudo chmod 777 ~/.config/autostart/org.gnome.SettingsDaemon.XSettings.desktop 系统设置——开机和关机——自动启动——双击GNOME开头的那个程序——勾选开启——点击右下角高级——只在Plasma中自动启动——完成 重启之后再打开QQ，能用。 微信同理： 12345yay -S deepin.com.wechat2##调整wineenv WINEPREFIX="$HOME/.deepinwine/Deepin-QQ" deepin-wine winecfgenv WINEPREFIX="$HOME/.deepinwine/Deepin-WeChat" deepin-wine winecfg 安装完一切都很好，就是QQ图片显示不出来，查了一下发现是qq默认使用了ipv6，禁用一下就可以了： 123456sudo vim /etc/default/grub #把其中的 GRUB_CMDLINE_LINUX_DEFAULT="quiet spalsh"#修改为 GRUB_CMDLINE_LINUX_DEFAULT="ipv6.disable=1 quiet splash"sudo grub-mkconfig -o /boot/grub/grub.cfg 7.4. 调整色温安装redshift： 1sudo pacman -S redshift To instantly adjusts the color temperature of your screen use: 1$ redshift -P -O TEMPERATURE where TEMPERATURE is the desired color temperature(between 1000 and 25000). 7.5. 主题图标和皮肤这个见仁见智吧。。。自己去搜索下，这里给出我最后选的配置： 12345678910111213141516171819 -&#96; lin@voidChen .o+&#96; ------------ &#96;ooo&#x2F; OS: Arch Linux x86_64 &#96;+oooo: Host: 82A8 Yoga 14sARE 2020 &#96;+oooooo: Kernel: 5.6.15-zen2-1-zen -+oooooo+: Uptime: 10 mins &#96;&#x2F;:-:++oooo+: Packages: 1004 (pacman), 5 (flatpak) &#96;&#x2F;++++&#x2F;+++++++: Shell: zsh 5.8 &#96;&#x2F;++++++++++++++: Resolution: 1920x1080 &#96;&#x2F;+++ooooooooooooo&#x2F;&#96; DE: Plasma .&#x2F;ooosssso++osssssso+&#96; WM: KWin .oossssso-&#96;&#96;&#96;&#96;&#x2F;ossssss+&#96; WM Theme: 微风Blurred -osssssso. :ssssssso. Theme: Sweet [Plasma], Breeze [GTK2&#x2F;3] :osssssss&#x2F; osssso+++. Icons: Tela [Plasma], Tela [GTK2&#x2F;3] &#x2F;ossssssss&#x2F; +ssssooo&#x2F;- Terminal: konsole &#96;&#x2F;ossssso+&#x2F;:- -:&#x2F;+osssso+- Terminal Font: Source Code Pro Semibold 12 &#96;+sso+:-&#96; &#96;.-&#x2F;+oso: CPU: AMD Ryzen 7 4800U with Radeon Graphics &#96;++:. &#96;-&#x2F;+&#x2F; GPU: AMD ATI 03:00.0 Renoir .&#96; &#96;&#x2F; Memory: 1578MiB &#x2F; 15423MiB 8、配置开发环境8.1. 安装java、maven、scala前往各个官网下载tar.gz包，sudo tar -zxvf命令，解压到/opt目录下，然后修改/etc/profile文件，添加环境变量，wq保存后自行一下 source /etc/profile 命令刷新一下就ok啦。 java官网：https://www.oracle.com/technetwork/java/javase/downloads/index.html maven官网：http://maven.apache.org/download.cgi scala官网：https://www.scala-lang.org/download/ 123456export MAVEN_HOME=/opt/apache-maven-3.6.1export JAVA_HOME=/opt/jdk1.8.0_211export SCALA_HOME=/opt/scala-2.11.1export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;MAVEN_HOME&#125;/bin:$&#123;SCALA_HOME&#125;/bin:$&#123;PATH&#125; 另外还有maven的配置，设置一下本地仓库和添加国内源： 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;localRepository&gt;/home/repository&lt;/localRepository&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;ibiblio&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://mirrors.ibiblio.org/pub/mirrors/maven2/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;jboss-public-repository-group&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;JBoss Public Repository Group&lt;/name&gt; &lt;url&gt;http://repository.jboss.org/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Repository Switchboard&lt;/name&gt; &lt;url&gt;http://repo1.maven.org/maven2/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;repo2&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://repo2.maven.org/maven2/&lt;/url&gt; &lt;/mirror&gt; 8.2. 安装idea和pycharm这里不打算去源里安装了，1是下载慢，2是wiki指导也是去官方下载tar包 idea：https://www.jetbrains.com/idea/download/#section=linux pycharm：https://www.jetbrains.com/pycharm/download/#section=linux 12345678910111213cd Downloads tar -zxvf ideaIU-2020.1.1.tar.gzmv idea-IU-201.7223.91 ../Program/idea-IUtar -zxvf pycharm-professional-2020.1.1.tar.gzmv pycharm-2020.1.1 ../Program/pycharmcd ../Program/idea-IU/bin./idea.shcd Program/pycharm/bin./pycharm.sh]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>小技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双显卡1050TI笔记本的Ubuntu18安装实记]]></title>
    <url>%2F%E5%8F%8C%E6%98%BE%E5%8D%A11050TI%E7%AC%94%E8%AE%B0%E6%9C%AC%E7%9A%84Ubuntu18%E5%AE%89%E8%A3%85%E5%AE%9E%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1、前言其实在这次安装前，我已经用了一个多星期的ubuntu-18.04.2了，在经历了数次重装和各种安装的坑之后，总算是对ubuntu有了一些理解。然后早上不知道做了个什么操作又把驱动给弄坏了，正好因为之前的安装也是一路磕磕碰碰搞错了不少东西，索性就直接再重装一次好了。吸取了之前各种驱动和软件安装的经验，想着这次应该能完美装好，所以有了这篇安装实记。一来是给自己留个记录方便以后查看（说不定又会重装= =），二来是有感于C某N上的文章抄来抄去把很多错误的、过时的办法给抄进去，让我这种小白看完跟着做一直踩坑，所以干脆把自己这次安装记录下来，我想应该也能帮助到一些同学吧。 这里我的笔记本是hp spectre x360 15，4K触摸屏（有坑）+ Nvidia GTX1050ti maxQ（巨坑），安装的是当前时间最新的一个长期支持版本ubuntu-18.04.2，这次安装包括系统安装、开发环境搭建以及系统的美化，如无意外最终的效果应该如下图所示，篇幅可能会有点长，嫌烦的同学可以根据导航栏跳转查阅。 2、系统安装这一部分照着官网来就好了，建议是下载LTS长期支持版，然后下面有usb烧录的教程，照着来制作USB启动盘插进去安装就好了，值得注意的主要有以下几点： bios设置找自己电脑型号进入bios，改成U盘启动优先，另外禁用掉安全启动，就是把secure boot设置为disable nvidia显卡的电脑安装的时候就卡在logo界面进不去：原因是nvidia的显卡和ubuntu自带的开源驱动Nouveau有冲突，临时解决的办法就是在开机进入grub引导界面的时候，在光标移动到Install Ubuntu这个选项的时候，按e进入到启动参数编辑界面，禁用掉显卡驱动。具体的做法就是在倒数第二行quiet slash 这句后面加一个nomodeset，空格隔开就行，然后按F10保存并启动，之后就能以低分辨率进入到安装界面。安装过车界面可能会卡卡的，鼠标动一下刷新界面显示就行，没装好驱动前只能讲究下= = 关于linux分区方案：这边我是建议自己手动分区的，新手可以参考我这个简单分区，我是1T的固态硬盘，分区如下： efi分区：500M，用于安装引导 /boot：500M Swap交换分区：20G，大小最好为你实际机器内存大小的2倍 /：400G，根目录，相当于win10的C盘，后续安装的软件大多会丢里面（/opt） /home：580G，相当于D、E、F盘，反正剩下多少都给他就好了 3、Nvidia驱动完美安装（巨坑）如无意外，N卡笔记本的话安装好系统之后重启就会卡在登陆界面，输入完密码一按回车就卡住，或者登陆进去卡住，总之就是各种卡住。。。原因就是上面提到的驱动冲突问题，我们需要以禁用内置驱动的方式去登陆系统，再去安装正确的驱动。 在这里，如果之前尝试过C某D上面文章的处理办法，基本就会各种扑街= =，什么“ctrl+alt+F1进入tty模式” 啊，或者“把Nouveau驱动加入到内核禁用列表“，通通不管用。。。我甚至怀疑我们装的根本不是同一个版本（难道我的1050TI特别难装？），因为这些方法全都过时了。。参考还是可以的，比如tty模式其实ctrl+alt+F1默认就是桌面了，要进F2～F6，另外驱动没装好切过去一样是卡住的。总之，我们还是一个个问题来解决。 3.1 进入grub引导，以nomodeset模式登陆系统首先想要进入系统，这需要和我们安装ubuntu时一样，临时把nouveau驱动给禁用掉，就是以“nomodeset”模式进入（据说ubuntu19.01的grub引导已经有专门的安全图形模式启动了）。这里首先会面临一个问题，就是装完系统重启之后，我们看不到grub引导界面就直接进入系统了。。。我去查资料发现，原来ubuntu18当电脑内只有一个系统的时候，会默认跳过grub，这需要我们进入系统后修改/etc/default/grub文件。这里又是一个死循环，想要进系统需要进入grub并修改启动参数，而想要进入grub又需要先进入系统去修改grub配置文件。。。 这显然是无解的，于是我上网找资料，看看怎么可以进入grub，然后又是在C某D上面的文章中找到 “长按shift可以进入引导界面”，显然我试过也是不行的，最后在一个别人处理矿机的帖子里找到了正确的解决办法：“在开机出现了紫色背景的时候，迅速按下ESC键” ，然后就能顺利看到grub引导界面（不行多按几下，不过按多了会进去别的），然后如同我们安装ubuntu那会一样，选中启动项，按e进入界面编辑，在倒数第二行quiet slash 这句后面加一个nomodeset，然后按F10保存并启动。 PS： 之前的安装里面，我并没有看到那个”ESC进入grub“，你们知道我是怎么处理的吗，我强行分了10G硬盘出来，多装了一个ubuntu弄成双系统，就为了让他显示出grub。。。这也是为什么我能下定决心重装，因为洁癖让我很不爽这个瑕疵，虽然通过修改grub可以直接隐藏这个系统。说道这顺便贴一下这个关于grub的文章：《如何修改GRUB》，里面讲的非常细致，可以学习下。 3.2 安装最新的Nvidia 1050TI驱动这个安装驱动也是花了我好多时间，网上找到的资料都是什么“将nouveau添加到黑名单blacklist.conf中”，又要禁用nouveau驱动，又要卸载旧的nvidia，又要加载源什么的，搞得非常复杂，又没什么len用，最后驱动没装上反倒是重装了几遍系统。实际并不需要搞这么多花里胡哨的，先联网，然后就几行代码完事了： 1234sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update ubuntu-drivers devices sudo ubuntu-drivers autoinstall 解释下这几句，第一句就是添加新的源，如果不添加的话，默认ubuntu版本库里最新的也就390版本，我去官网查自己的型号最新的是430版本了。第二句就是更新到本地版本库，网络不好的同学可以先更换一下国内源。第三句就是检查可以安装的驱动有哪些，看不堪都行，就是为了看看自动推荐的是不是最新的而已，都能用的。第四句就是自动安装推荐的版本了。 这里我不建议下载包自己手动装，反正我就没手动装成功过= =，如无疑外的话，重启系统应该就装好了（不需要启动的时候加nomodeset了）。打开终端，输入nvidia-smi，回车能看到以下界面就是装好了： 输入nvidia-settings，回车可见nvidia的设置的图形界面，这里可以切换独显和集显： 4、系统设置和软件安装装好系统之后首先把日常的软件装上，先打造出日常办公环境。我也是一边装一边写这篇文章，实打实的安装配置实录啊。 4.1 更换国内源这个网上很多，为了方便我就贴我现在用到的。编辑apt源配置文件：sudo vi /etc/apt/sources.list这里我还没来得及装vim，vi的操作有点不熟练，a是追加模式，找个空行把下面的阿里源复制粘贴过去就好了，里面其他的默认源最好就#注释掉。改好之后wq报存退出，并更新源sudo apt-get update 12345678910deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main restricted universe multiverse deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-proposed main restricted universe multiverse deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-backports main restricted universe multiverse deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main restricted universe multiverse deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-proposed main restricted universe multiverse deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-backports main restricted universe multiverse 4.2 安装markdown编辑器Typora为了边装边写，所以第一时间就装这个了= =，上官网找到linux版安装，也是几句代码的事,，这里我就直接贴出来了： 123456wget -qO - https:&#x2F;&#x2F;typora.io&#x2F;linux&#x2F;public-key.asc | sudo apt-key add - sudo add-apt-repository &#39;deb https:&#x2F;&#x2F;typora.io&#x2F;linux .&#x2F;&#39; sudo apt-get update sudo apt-get install typora Typora官网：https://www.typora.io/#linux 4.3 安装chrome浏览器也是差不多的步骤，添加源，更新源，安装。唯一多了的一步就是添加了一个授权key。 1234sudo wget https:&#x2F;&#x2F;repo.fdzh.org&#x2F;chrome&#x2F;google-chrome.list -P &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F; sudo wget -q -O - https:&#x2F;&#x2F;dl.google.com&#x2F;linux&#x2F;linux\_signing\_key.pub | sudo apt-key add - sudo apt-get update sudo apt-get install google-chrome-stable 装好就能抛弃掉自带的火狐浏览器了，卸载火狐命令如下： 12dpkg --get-selections | grep firefox sudo apt-get purge firefox firefox-locale-en firefox-locale-zh-hans 第一句就是查看装了哪些火狐相关的，第二句是指定哪些要删除，效果如图： 4.4 安装搜狗拼音输入法搜狗输入法官网：https://pinyin.sogou.com/linux/ubuntu18如果你在安装的时候语言选了中文，那么系统里面是内置了拼音输入法，我正是用这个输入法敲到现在，实在是不好用，所以直接安装一个搜狗输入法。我们直接去官网下载该软件，下载deb包,然后命令安装： 1sudo dpkg -i sogoupinyin\_2.2.0.0108\_amd64.deb 然后就报错了= =。。。。我们还缺少一些相关依赖， sogou是基于fcitx的，而系统默认的键盘输入法系统是iBus，且ubuntu18并没有自带这个输入法，所以我们还得先安装fcitx。根据报错提示下面有一句自动修复的命令，我猜应该就是根据依赖关系自动安装一些需要的软件包吧，我们把这句命令执行一下： 1sudo apt --fix-broken install 如无意外在软件界面就能看到fcitx小企鹅了，然后在系统设置-&gt;区域语言-&gt;管理已安装语言 那里选择输入法为fcitx，应用到系统，再重新执行一下上面安装sougo的命令，安装好后重启下系统并设置一下输入法就能用啦。 4.5 安装屏幕截图软件FlameshotFlameshot这款截图软件还蛮好用的，基本满足日常需求，可以加马赛克，添加箭头和文本，安装也很简单，下面一句命令就装好了= = 1sudo apt-get install flameshot 为了方便使用，我们还可以添加到快捷键，这个就看个人习惯了，在系统设置-&gt;设备-&gt;键盘 里设置，拉到最下面有个+号，添加一个就好了。 4.6 安装QQ、微信目前试了挺多版本的QQ微信，还是deepin封装的win软件最好用，这里贴一下deepin官网：https://wiki.deepin.org/ ，感谢他们付出的努力。 先去他们提供的仓库下载deb包，如果有别的软件需要也可以上上面找找： 首先安装deepin封装好的框架：123git clone https:&#x2F;&#x2F;gitee.com&#x2F;wszqkzqk&#x2F;deepin-wine-for-ubuntu.git cd deepin-wine-for-ubuntu .&#x2F;install.sh QQ：http://packagess.deepin.com:8081/deepin/pool/non-free/d/deepin.com.qq.im/ 1sudo dpkg -i deepin.com.qq.im\_8.9.19983deepin23\_i386.deb 微信：http://packagess.deepin.com:8081/deepin/pool/non-free/d/deepin.com.wechat/ 1sudo dpkg -i deepin.com.wechat\_2.6.2.31deepin0\_i386.deb deepin官方还提供了迅雷，百度云等等各种软件，有需要的朋友可以自行找找看。我这里只用了QQ和微信，原因是有个问题一直没想到好的解决办法，就是我这个屏幕是4K屏，而deepin移植的包其实是运行在wine里，并不能随系统一起缩放，这就导致了我分辨率选4K的时候，QQ微信界面和字体都非常的小。。如果以后找到好的解决办法，我会更新到这里的QAQ 更新！！找到解决的办法啦！！deepin的QQ和微信都是自己封装在一个wine里面，所以常规的wine设置对他来说不起作用，不过最后还是被我找到了设置deepin-wine的办法！ 12env WINEPREFIX&#x3D;&quot;&#x2F;home&#x2F;chen&#x2F;.deepinwine&#x2F;Deepin-WeChat&quot; deepin-wine winecfg env WINEPREFIX&#x3D;&quot;&#x2F;home&#x2F;chen&#x2F;.deepinwine&#x2F;Deepin-QQ&quot; deepin-wine winecfg 输入这个就会弹出一个界面，上面有字体大小设置，自己去改就好啦，我把dpi改成200，如果发现改完启动秒退，可以尝试改小一点。 如果你是有别的deepin的软件，可以在~/.deepinwine/看到对应的文件夹名字，照着上面的改改就好了 4.7 安装为知笔记官网下载：http://www.wiz.cn/download.html 一行命令安装： 1sudo dpkg -i wiznote\_2.3.2.4\_amd64.deb 4.8 安装TeamViewer官网下载：https://www.teamviewer.cn/cn/download/linux/ 一行命令安装： 1sudo dpkg -i teamviewer\_14.3.4730\_amd64.deb 提示依赖关系没有配置，执行以下命令修复并再次安装： 12sudo apt-get -f -y install sudo dpkg -i teamviewer\_14.3.4730\_amd64.deb 4.9 安装网易云音乐官网下载：https://music.163.com/#/download一行命令安装： 1sudo dpkg -i netease-cloud-music\_1.2.1\_amd64\_ubuntu\_20190428.deb 如果觉得网易云音乐UI的DPI太小，可以改下.desktop的exec： 1Exec&#x3D;netease-cloud-music --force-device-scale-factor&#x3D;2 %U 4.10 安装百度网盘官网下载：https://pan.baidu.com/download一行命令安装： 1sudo dpkg -i baidunetdisk\_linux\_2.0.1.deb 4.11 安装wps及卸载自带的Liboffice本来我是想将就着用liboffice算了，但是用了一周发现他改完的文档格式老是有些莫名其妙的问题，所以干脆就卸载掉直接安装wps了，正好wps也有linux版。 先卸载LibOffice： 1sudo apt-get remove libreoffice-common 安装wps：前往官网下载deb包：https://www.wps.cn/product/wpslinux 1sudo dpkg -i wps-office\_11.1.0.8722\_amd64.deb 5、安装配置开发环境开发环境其实大部分都是在官网下载好，然后解压丢到/opt，配置环境变量就好了，因为每个人开发环境都不完全相同，我也不说这么细了，前往各个官网下载自己需要的版本吧。。。 5.1 Java、scala、maven安装配置前往各个官网下载tar.gz包，sudo tar -zxvf命令，解压到/opt目录下，然后修改/etc/profile文件，添加环境变量，wq保存后自行一下 source /etc/profile 命令刷新一下就ok啦。 java官网：https://www.oracle.com/technetwork/java/javase/downloads/index.html maven官网：http://maven.apache.org/download.cgi scala官网：https://www.scala-lang.org/download/ 123456export MAVEN_HOME&#x3D;&#x2F;opt&#x2F;apache-maven-3.6.1 export JAVA\_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0\_211 export SCALA_HOME&#x3D;&#x2F;opt&#x2F;scala-2.11.1 export JRE\_HOME&#x3D;$&#123;JAVA\_HOME&#125;&#x2F;jre export CLASSPATH&#x3D;.:$&#123;JAVA\_HOME&#125;&#x2F;lib:$&#123;JRE\_HOME&#125;&#x2F;lib export PATH&#x3D;$&#123;JAVA\_HOME&#125;&#x2F;bin:$&#123;MAVEN\_HOME&#125;&#x2F;bin:$&#123;SCALA_HOME&#125;&#x2F;bin:$&#123;PATH&#125; 另外还有maven的配置，设置一下本地仓库和添加国内源： 12345678910111213141516171819202122232425262728293031323334353637383940 &lt;localRepository&gt;&#x2F;home&#x2F;repository&lt;&#x2F;localRepository&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;&#x2F;id&gt; &lt;name&gt;aliyun maven&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&#x2F;&lt;&#x2F;url&gt; &lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt; &lt;&#x2F;mirror&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;&#x2F;id&gt; &lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt; &lt;name&gt;aliyun maven&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;repositories&#x2F;central&#x2F;&lt;&#x2F;url&gt; &lt;&#x2F;mirror&gt; &lt;mirror&gt; &lt;id&gt;ibiblio&lt;&#x2F;id&gt; &lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;mirrors.ibiblio.org&#x2F;pub&#x2F;mirrors&#x2F;maven2&#x2F;&lt;&#x2F;url&gt; &lt;&#x2F;mirror&gt; &lt;mirror&gt; &lt;id&gt;jboss-public-repository-group&lt;&#x2F;id&gt; &lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt; &lt;name&gt;JBoss Public Repository Group&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;repository.jboss.org&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&lt;&#x2F;url&gt; &lt;&#x2F;mirror&gt; &lt;mirror&gt; &lt;id&gt;central&lt;&#x2F;id&gt; &lt;name&gt;Maven Repository Switchboard&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;repo1.maven.org&#x2F;maven2&#x2F;&lt;&#x2F;url&gt; &lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt; &lt;&#x2F;mirror&gt; &lt;mirror&gt; &lt;id&gt;repo2&lt;&#x2F;id&gt; &lt;mirrorOf&gt;central&lt;&#x2F;mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;&#x2F;name&gt; &lt;url&gt;http:&#x2F;&#x2F;repo2.maven.org&#x2F;maven2&#x2F;&lt;&#x2F;url&gt; &lt;&#x2F;mirror&gt; 5.2 安装idea前往官网下载：https://www.jetbrains.com/idea/download/#section=linux 下载完解压到/opt ： 123sudo tar -zxvf ideaIU-2019.1.3.tar.gz -C &#x2F;opt cd &#x2F;opt&#x2F;ideaIU-2019.1.3&#x2F;bin .&#x2F;idea 接下来就能看到熟悉的idea的界面啦，一直下一步配置完就好了，至于破解什么的，百度最新的方法吧hhhhhhh，安装完之后顺手先配置一下maven，免得使用默认的.m2目录又浪费时间下载（自己的本地仓库移动硬盘有备份=。=） 5.3 安装破解Navicat官网下载：https://www.navicat.com/en/download/navicat-premium下载完解压到/opt ： 123sudo tar -zxvf navicat121\_premium\_cs_x64.tar.gz -C &#x2F;opt cd &#x2F;opt&#x2F;navicat121\_premium\_cs_x64&#x2F; .&#x2F;start_navicat ./start_navicat这句千万不要加sudo，不然等下图标就不够权限了（加了的话删除掉~/.navicat64/文件夹就行，原因是里面的配置文件是root权限的）。然后他会提示叫你安装： 配置好就可以启动了，就是界面非常的丑，而且不支持中文字体，会变成小方格，这些都没关系，等我们后续美化的时候，装个好看的字体，换过来就解决了= = 如果不想装字体的话，现在解决乱码的方式就是：先在linux系统中添加中文支持，然后手动编辑start_navicat里面的内容，将export LANG=&quot;en_US.UTF-8&quot;将这句话改为export LANG=&quot;zh_CN.UTF-8&quot;，然后在设置里更改字体为Noto Sans Mono CJK JP Regular，重启程序即可： 关于破解呢，网上有好多方法，嫌麻烦我选择删配置文件重置试用就好了，命令如下： 1rm &#x2F;home&#x2F;chen&#x2F;.navicat64&#x2F;user.reg 另外就是这个软件的linux版本非常偷懒，图标还需要自己去创建，还需要自己下载图标。。。 123456789101112131415161718192021cd &#x2F;opt&#x2F;navicat121\_premium\_cs_x64&#x2F; sudo wget http:&#x2F;&#x2F;www.navicat.com.cn&#x2F;images&#x2F;02.Product\_00\_AllProducts\_Premium\_large.png sudo mv 02.Product\_00\_AllProducts\_Premium\_large.png navicat.png #以上为下载图标&#x3D;。&#x3D;，名好长顺便给改一下，然后创建图标文件： sudo vim navicat.desktop #在打开的文本里输入以下内容，wq保存即可 \[Desktop Entry\] Encoding&#x3D;UTF-8 Name&#x3D;Navicat Comment&#x3D;Navicat Premium Exec&#x3D;&#x2F;opt&#x2F;navicat121\_premium\_cs\_x64&#x2F;start\_navicat Icon&#x3D;&#x2F;opt&#x2F;navicat121\_premium\_cs_x64&#x2F;navicat.png Terminal&#x3D;0 Type&#x3D;Application Categories&#x3D;Application #赋权后赋值到app目录 chmod +x navicat.desktop sudo cp navicat.desktop &#x2F;usr&#x2F;share&#x2F;applications&#x2F; 图标路径后面主要不要有空格，执行如果出不来记得个文件夹附权，或者是sudo导致配置文件（~/.navicat64）权限为root 把用户加入到root组：sudo adduser chen root 最新更新！解决高分屏下这个navicat 缩放（UI太小，DPI低）的问题！！！我研究了好久他的启动脚本start_navicat，发现他就是内置了一个wine，然后我们用外部的winecfg影响不到他，最后我写了个脚本。。。 1234567891011121314151617!&#x2F;bin&#x2F;sh cd \&#96;dirname &quot;$0&quot;\&#96; navicat_root&#x3D;\&#96;pwd\&#96; # Wine environment variables WINEDIR&#x3D;&quot;wine&quot; export LANG&#x3D;&quot;en_US.UTF-8&quot; export PATH&#x3D;&quot;$navicat\_root&#x2F;$WINEDIR&#x2F;bin&quot;:&quot;$navicat\_root&quot;:&quot;$navicat\_root&#x2F;$WINEDIR&#x2F;drive\_c&#x2F;windows&quot;:&quot;$PATH&quot; export LD\_LIBRARY\_PATH&#x3D;&quot;$navicat\_root&#x2F;$WINEDIR&#x2F;lib&quot;:&quot;$navicat\_root&#x2F;lib&quot;:&quot;$LD\_LIBRARY\_PATH&quot; export WINEDLLPATH&#x3D;&quot;$navicat_root&#x2F;$WINEDIR&#x2F;lib&#x2F;wine&quot; export WINELOADER&#x3D;&quot;$navicat_root&#x2F;$WINEDIR&#x2F;bin&#x2F;wine64&quot; export WINESERVER&#x3D;&quot;$navicat_root&#x2F;$WINEDIR&#x2F;bin&#x2F;wineserver&quot; export WINEPREFIX&#x3D;&quot;$HOME&#x2F;.navicat64&quot; export WINEDLLOVERRIDES&#x3D;&quot;ucrtbase,msvcp140,vcruntime140,msvcp110,msvcr110,msxml3&#x3D;n,b&quot; exec &quot;$&#123;WINELOADER:-wine&#125;&quot; &quot;winecfg&quot; 把它保存为test.sh，放在跟start_navicat同级目录下面，执行一下就会弹出一个界面来，在里面改DPI就可以了！！！ 5.4 安装RedisDesktopManager官方网站：https://redisdesktop.com/download一句命令行解决： 1sudo snap install redis-desktop-manager 或者直接通过软件管理中心下载，如果网络卡下载不了，可以先翻墙= = 5.5 安装svn客户端一行命令： 1sudo apt-get install subversion 在idea中file--&gt;settings...--&gt;subversion---&gt;Enable interactive mode 勾选上 5.6 安装git一行命令： 1234sudo apt install git git config --global user.name &quot;VoidChen10&quot; git config --global user.email &quot;425325925@qq.com&quot; 5.7 配置ssh的config文件实现快速登录服务器如果没装ssh的话自己装一下，我这好像是自己装了。linux的终端配置好了比xshell还方便，把服务器信息加到~/.ssh/config文件即可，没有该文件就创建一下： 1234567891011vim .ssh&#x2F;config #配置多个服务器 Host 100 #别名 Hostname 192.168.1.100 #服务器 Port 22 #端口 User root #用户 Host 172 Hostname 172.26.152.1 Port 2233 User chen 编辑好wq保存下，就可以愉快的使用别名登录了，效果如下： 搞完之后还有个问题，就是登录还需要密码，这里我们首先生成秘钥： 1ssh-keygen -t rsa 敲3下回车默认即可，然后把秘钥发送到服务端，由于我们刚配好了config，这里-i后面直接接别名即可： 1ssh-copy-id -i 100 如图，已经能轻松无密码登录了~~ 5.8 配置Hexo博客关于如何创建和使用hexo可以参考这篇文章：《用Hexo + github搭建自己的博客 — 再也不用羡慕别人了！》我以前弄好的直接拿过来用就好了，就是装一下npm和配置一下就好了(nodejs已经预装了)： 12sudo apt install npm npm install hexo-cli -g 把生成的公钥加到github上： 测试一下github配置是否成功： 1ssh -T git@github.com 然后hexo就装好啦，进入到博客安装目录： 1234hexo clean #清理 hexo g #打包 hexo s #本地发布 hexo d #上传 6、系统美化6.1 安装gnome优化工具1sudo apt-get install gnome-tweak-tool 装完会多一个叫优化的程序，进去可以初步定制我们的系统了 安装扩展以支持安装自己的主题： 1sudo apt-get install gnome-shell-extensions 安装完重启一下，把User themes选上： 6.2添加托盘插件TopIconswine应用（qq、微信）不装的话启动之后会多个窗口显示托盘图标，这显然是及其难受的。。。好在有个插件可以帮我们解决，github地址：https://github.com/phocean/TopIcons-plus 123git clone https:&#x2F;&#x2F;github.com&#x2F;phocean&#x2F;TopIcons-plus.git cd TopIcons-plus make install 这将编译glib模式并将所有必需的文件复制到您自己的用户帐户的GNOME Shell扩展目录（因此您不需要管理员权限才能运行make）。默认情况下，TopIcons Plus将存在于目录中~/.local/share/gnome-shell/extensions/TopIcons@phocean.net/。 如果要安装扩展以使其在系统范围内可用，则必须更改INSTALL_PATH变量，并以root身份运行。 1sudo make install INSTALL_PATH&#x3D;&#x2F;us &#x2F;share&#x2F;gnome-shell&#x2F;extensions 现在，重新加载GNOME Shell。您可以点击Alt+ F2，键入r，然后按Enter —或登录/注销。 最后，启动_gnome-tweak-tool_实用程序来管理扩展。在那里，您可以启用_TopIcons Plus_，然后调整其外观。 把图片偏移设置为靠右，然后就可以看到微信在托盘里啦 6.3 安装Cinnamon桌面环境 首先是添加源，输入密码后，根据提示按回车继续，更新完直接安装即可 12sudo add-apt-repository ppa:embrosyn&#x2F;cinnamon sudo apt update &amp;&amp; sudo apt install cinnamon 安装好重启下，等登录那个框别急着输入密码，点那个小齿轮，选择cinnamon使用装好的桌面环境。进去之后就是一个跟windows非常像的系统界面啦~~不过桌面还是空空如也，点击左下角的图片，找到你要用的软件，添加到桌面就好了。 同样在这个启动菜单的左上角，就是系统设置了，里面提供了丰富的系统设置，可以根据自己的需要进行修改。 另外就是截图的快捷键需要重新添加了，这个自己找找吧。搜索“键盘”就能找到了。 6.4 zsh终端的使用和美化安装zsh并让zsh作为默认的终端： 12sudo apt-get install -y zsh chsh -s &#x2F;bin&#x2F;zsh 装完重启下，默认的终端就是zsh了。接下来安装oh my zsh： 1wget --no-check-certificate https:&#x2F;&#x2F;github.com&#x2F;robbyrussell&#x2F;oh-my-zsh&#x2F;raw&#x2F;master&#x2F;tools&#x2F;install.sh -O - | sh 重启终端即可看到效果，接下来修改~/.zshrc文件，加入一些好用的插件和更换主题即可。 安装高亮插件和命令自动补齐： 12345678910111213141516cd ~&#x2F;.oh-my-zsh&#x2F;custom&#x2F;plugins #高亮插件 git clone git:&#x2F;&#x2F;github.com&#x2F;zsh-users&#x2F;zsh-syntax-highlighting.git #命令补齐 git clone https:&#x2F;&#x2F;github.com&#x2F;zsh-users&#x2F;zsh-autosuggestions $&#123;ZSH_CUSTOM:-~&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;plugins&#x2F;zsh-autosuggestions vim ~&#x2F;.zshrc #在文本内添加： #设置主题，内置主题可前往github查看：https:&#x2F;&#x2F;github.com&#x2F;robbyrussell&#x2F;oh-my-zsh&#x2F;wiki&#x2F;Themes ZSH_THEME&#x3D;&quot;robbyrussell&quot; #添加高亮插件和自动补齐 plugins&#x3D;(git zsh-syntax-highlighting zsh-autosuggestions web-search) #wq保存后刷新一下 source .zshrc 最后一步就是右键终端，配置首选项，把终端调整成自己喜欢的样子~~ 6.5 安装Vimix主题官网地址：https://www.opendesktop.org/p/1013698直接在该页面下载下来，注意他是一个xz包，然后解压安装： 1234xz -d vimix-color-light.tar.xz tar -xvf vimix-color-light.tar cd vimix-color-light sudo sh Install 安装完系统设置-&gt;主题，就能看到了，选一下应用即可]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>小技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记8：K-Means的扩展学习]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B08%EF%BC%9AK-Means%E7%9A%84%E6%89%A9%E5%B1%95%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前言关于聚类的经典算法K-Means 算法在第一期就已经讲过了，需要回顾的同学点这里，现在顺着spark的api继续看下去，又看到聚类这一块，就着重讲一下基于流数据的K-Menas算法使用，因为最近比较忙的，所以感觉又一些东西都还没很完整的看完，比如回归的算法，还有聚类的一些统计学方法，这些都会在今后慢慢补上，给自己一个小目标，今年6月底把spark的api全部都过一遍，然后开始又针对地去做人脸识别相关的学习，还有神经网络和深度学习，在此之前希望我能打下较好的基础。 有点扯远了，看回今天的主题，Streaming k-means——基于流的k-means算法，其实就是在原有的k-means算法里面引入流的概念，并放在分布式的集群上去做，下面我们就来简单了解一下。 流式k-means算法spark官网讲的挺清楚的，我就翻译下搬过来了，官网地址：点这里 当数据是以流的方式到达的时候，我们可能想动态地去估算(estimate)聚类的簇，通过新的到达的数据来更新它们。spark.mllib支持流式k-means聚类，并且可以通过参数控制估计衰减(decay)（ 或“忽略”(forgetfulness) ）。 这个算法使用一般地小批量更新规则来更新簇。 对每批新到的数据，我们首先将点分配给距离它们最近的簇，然后计算新的数据中心，最后更新每一个簇。使用的公式如下所示：$$\begin{equation} c_{t+1} = \frac{c_tn_t\alpha + x_tm_t}{n_t\alpha+m_t} \end{equation}$$$$\begin{equation} n_{t+1} = n_t + m_t \end{equation}$$在上面的公式中，$c_{t}$表示前一个簇中心，$n_{t}$表示至今位置分配给这个簇的点的数量， $x_{t}$表示从当前批数据中计算出来的簇中心，$m_{t}$表示当前添加的这批数据的点数量。衰减因子 $\alpha$ 可以被用来忽略过去的聚类中心：当 $\alpha$ 等于1时，所有的批数据赋予相同的权重，当 $\alpha$ 等于0时，数据中心点完全通过当前数据确定。 这里我想稍微讲一下这个衰减因子 $\alpha$ ，看他在公式中，当 $\alpha = 0$ 的时候，其实就相当于完全忽略掉前面历史累积下来的聚类中心的结果了，所以说他是“衰减因子”挺贴切的，这个系数可以在每一次迭代中消减历史数据带来的影响，这个系数越大，则代表着越看重最新的数据。要知道源源不断的流数据，其实还是挺看重实时数据的，比如说你冬天喜欢买羽绒，到了夏天不可能还这么喜欢买羽绒吧。。。 衰减因子$\alpha$ 也可以通过halfLife参数联合 时间单元（time unit）来确定，时间单元可以是一批数据也可以是一个数据点。假如数据从t时刻到来并定义了halfLife为h， 在t+h时刻，应用到t时刻的数据的折扣（discount）为0.5。 halfLife翻译过来就是半衰期，挺有意思的，这个参数可以让衰减因子随着时间来动态调整，越来越依赖新的数据，在过完一个时间单元（或者叫周期吧）重置。应该说这个衰减因子的变化在每个时间单元中独立生效。 算法步骤流式k-means算法的步骤如下所示： （1）分配新的数据点到离其最近的簇； （2）根据时间单元（time unit）计算折扣（discount）值，并更新簇权重； （3）应用更新规则； （4）应用更新规则后，有些簇可能消失了，那么切分最大的簇为两个簇。 应用场景流式K-means的应用场景还是挺广阔的，比如在实时推荐预测系统方面，比如广告推荐、商业预测之类的，由于推荐预测系统对数据时效性的敏感度较高，而且其数据处于连续实时且快速的变化，所以必须建立起流式的机器学习应用，从而对流式的数据进行实时的预测分析与处理，这对于商业分析与运营而言将十分关键。 另外还有一篇论文，将该算法应用在数据安全领域，用来实时验证分析大数据环境下的DDoS攻击检测，参考论文：《基于Spark Streaming的实时数据分析系统及其应用》 代码demo这里训练数据我们还是用回之前k-means用过的数据集，值得一提的是，这里的测试集需要改成(1.0), [1.7, 0.4, 0.9]这种标签向量的格式。我们就把上一个k-means聚类出来的结果作为这里的测试集试试。 每个训练点应格式化为[x1, x2, x3]，并且每个测试数据点应格式化为(y, [x1, x2, x3])，其中y是一些有用的标签或标识符（例如，真正的类别分配） 除此以外还要注意的是，spark streaming读取文件的时候，旧文件是不识别的，他只会读取新创建的文件，也就是说我们要单独写一个文件IO，把训练集和测试集以流形式给写进去。。。不过生产环境的时候，就可以上kafka来做信息读取了。 下面贴出训练的主函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package SparkMLlib.Clusteringimport org.apache.spark.SparkConfimport org.apache.spark.mllib.clustering.StreamingKMeansimport org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * @Author: voidChen * @Date: 2019/3/19 17:24 * @Version 1.0 */object StreamingKMeansExample &#123; def main(args: Array[String]) &#123; val trainingPath = "D:\\data\\train\\" //训练数据集文件路径// val testPath = userPath + "/src/main/resources/data/StreamingKmeans-test.txt" //测试数据集文件路径 val testPath = "D:\\data\\test\\" //测试数据集文件路径 val conf = new SparkConf().setAppName("StreamingKMeansExample") .setMaster("local[*]") //TODO: 生成打包前，需注释掉此行 val ssc = new StreamingContext(conf, Seconds(10)) //数据量太少这里设置了1秒给它。。 val trainingData = ssc.textFileStream(trainingPath) .filter(!isColumnNameLine(_)) .map(line =&gt; &#123;//处理数据成标准向量 println(line) Vectors.dense(line.split("\t").map(_.trim).filter(!"".equals(_)).map(_.toDouble)) &#125;) val testData = ssc.textFileStream(testPath).map(LabeledPoint.parse) val model = new StreamingKMeans() .setK(8) //聚类中心 .setDecayFactor(1.0) //损失因子 .setRandomCenters(8, 0.0) //初始化随机中心，只需要维数。 第一个参数是维数，第二个参数是簇权重 model.trainOn(trainingData) model.predictOnValues(testData.map(lp =&gt; (lp.label, lp.features))).print() ssc.start() ssc.awaitTermination() &#125; /** * 只是用来去掉表头 * @param line * @return */ def isColumnNameLine(line:String):Boolean = &#123; if (line != null &amp;&amp; line.contains("Channel")) true else false &#125;&#125; 再给出一个文件读写的简易demo： 12345678910111213141516171819202122def FileIODemo(): Unit =&#123; val f= new File("D:\\data\\test.txt")// val f= new File("D:\\data\\Wholesale customers data_training.txt") val in = new InputStreamReader(new FileInputStream(f)) val out = new OutputStreamWriter(new FileOutputStream("D:\\data\\test\\test4.txt"))// val out = new OutputStreamWriter(new FileOutputStream("D:\\data\\train\\train4.txt")) val r = new BufferedReader(in) val w = new BufferedWriter(out) var lineTxt = Option(r.readLine) while ( !lineTxt.isEmpty)&#123; println(lineTxt.getOrElse("")) w.write(lineTxt.getOrElse("")) w.flush() lineTxt = Option(r.readLine) if(!lineTxt.isEmpty) w.newLine() &#125; in.close w.close() &#125; 完整代码请前往github查看，文件路径请灵性修改。。。另外数据集分别就是Wholesale customers data_training.txt 和 StreamingKmeans-test.txt，我会放在github上：点我前往github]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>MachineLeaning</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记7：初探梯度提升决策树]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B07%EF%BC%9A%E5%88%9D%E6%8E%A2%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[前言嗯。。怎么说呢，本来见识到随机森林的强大之后，觉得分类算法里面，另一个集成算法——梯度提升决策树(GBDT)看不看都那样了，看各类书和spark官网对他的介绍也是一笔带过，所以抱着轻视的态度，顺手训练了下模型，结果识别率居然高达99.763593%！！！用的还是官方给的默认参数。。。而且训练速度贼快。我随机森林要跑15分钟，这个几分钟就好了。所以决定单独写一篇日记，来深入地去学习一下这个算法。 比较可惜地是，该算法目前还不支持多分类，不过找了下资料貌似挺多大神在一点点改进这个算法了，比如陈天奇的xgboost。总而言之，不能以一份数据来衡量机器学习算法的好坏，应该根据数据特点，选择最适合这个数据的算法，这也是我这个系列学习希望达成的目标之一。 什么是梯度提升决策树梯度提升决策树简称 GBDT ，是 Gradient Boosting Decision Tree 的缩写，也有文章称之为 MART（Multiple Additive Regression Tree），是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。注意这里的决策树算法用的并不是随机森林提到的分类树，而是回归树，而且梯度提升树目前不支持多分类问题。 梯度提升树依次迭代训练一系列的决策树。在一次迭代中，算法使用现有的集成来对每个训练实例的类别进行预测，然后将预测结果与真实的标签值进行比较。通过重新标记，来赋予预测结果不好的实例更高的权重。所以，在下次迭代中，决策树会对先前的错误进行修正。 对实例标签进行重新标记的机制由损失函数来指定。每次迭代过程中，梯度迭代树在训练数据上进一步减少损失函数的值。spark.ml为分类问题提供一种损失函数（Log Loss），为回归问题提供两种损失函数（平方误差与绝对误差）。 算法原理GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。 在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是 $f_{t-1}(x)​$，损失函数是 $L(y, f_{t-1}(x))​$， 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)​$，让本轮的损失函数$L(y, f_{t}(x) =L(y, f_{t-1}(x)+ h_t(x))​$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 构造过程这里只给出网上找到的算法步骤，里面很多推导以及公式我还没弄懂，实在有些惭愧，后续闲下来会针对这个再进一步学习。 算法如下步骤，截图来自《The Elements of Statistical Learning》： 算法步骤解释： 1、初始化，估计使损失函数极小化的常数值，它是只有一个根节点的树，即ganma是一个常数值。 2、（a）计算损失函数的负梯度在当前模型的值，将它作为残差的估计（b）估计回归树叶节点区域，以拟合残差的近似值（c）利用线性搜索估计叶节点区域的值，使损失函数极小化（d）更新回归树 3、得到输出的最终模型 f(x) GBDT的优缺点及应用场景优点： 相对少的调参时间情况下可以得到较高的准确率。 可灵活处理各种类型数据，包括连续值和离散值，使用范围广。 可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。 缺点： 弱学习器之间存在依赖关系，难以并行训练数据。 应用场景：GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。 而在实际应用中呢，多是作为一种预测分析算法，用在各行各业的领域，如： 故障预测分析 欺诈预测分析 搜索引擎用户体验提升 视频流媒体服务体验提升 个人征信 GBDT重要参数介绍 lose：损失函数。 ​ spark mllib一共提供3种，注意每一种都只适合一类问题，要么是回归，要么是分类。分类只可选择Log Loss，回归问题可选择平方误差(Squared Error)和绝对值误差(Absolute Error)。分别又称为L2损失和L1损失。绝对值误差（L1损失）在处理带有离群值的数据时比L2损失更加具有鲁棒性。分别如下表格： Loss Task Formula Description Log Loss Classification 2∑Ni=1log(1+exp(−2yiF(xi))) Twice binomial negative log likelihood. Squared Error Regression ∑Ni=1(yi−F(xi))2 也称为L2损失。回归任务的默认损失。 Absolute Error Regression ∑Ni=1|yi−F(xi)| 也称为L1损失。对于异常值比Squared Error更强大。 numIterations：迭代次数。 ​ GBDT迭代次数，每一次迭代将产生一棵树，因此这个numIterations也是算法中所包含的树的数目。增加numIterations会提高训练集数据预测准确率（注意是训练集数据上的准确率哦）。但是相应的会增加训练的时间。 learningRate：学习率。 ​ 官方建议是不需要调整此参数。如果算法行为看起来不稳定，则降低此值可以提高稳定性。小的学习率（步长）肯定会增加训练的时间。 algo：使用的算法。 ​ 初始化的时候直接给出这个树的算法类型,像这样：BoostingStrategy.defaultParams(&quot;Classification&quot;) 。一共就两种,分类(Classification) 或者是 回归(Regression）。 treeStrategy.numClasses：分类数量。 ​ 由于分类只能做二分类，所以这个参数填2就可以了，多了会报错。 treeStrategy.maxDepth：树深度。 ​ 和随机森林的一样，树越深，训练时间越长。 这里面主要用到的就是这几个参数，较为有用的就是迭代次数了，为了防止过拟合，这个迭代次数也得调整下，随着迭代次数的增加，一开始在验证集上预测误差会减小，迭代次数增大到一定程度后误差反而会增加，那么通过准确度vs.迭代次数曲线可以选择最合适的numIterations。 在MNIST手写数字集上使用GBDT做分类由于这一系列暂时用的都是同一份数据集，我就不重复讲怎么处理数据了，有需要的同学可以看下前面的数据集介绍，对比上一篇随机森林的代码呢，其实差别只是换了一个训练函数，以及因为GBDT只能做二分类所以把数据过滤了下，只取了0和1的数据来做判断，下面贴出主函数，完整代码请点击这里：前往GitHub 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package SparkMLlib.Classificationimport SparkMLlib.Base.MNIST_Utilimport org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.mllib.tree.&#123;GradientBoostedTrees&#125;import org.apache.spark.mllib.tree.configuration.BoostingStrategyimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object GBDT_Example &#123; def main(args: Array[String]): Unit = &#123; // 获取当前运行路径 val userPath = System.getProperty("user.dir") val trainLabelFilePath = userPath + "/src/main/resources/data/train-labels.idx1-ubyte" val trainImageFilePath = userPath + "/src/main/resources/data/train-images.idx3-ubyte" val testLabelFilePath = userPath + "/src/main/resources/data/t10k-labels.idx1-ubyte" val testImageFilePath = userPath + "/src/main/resources/data/t10k-images.idx3-ubyte"// val testLabelFilePath = "C:\\Users\\42532\\Desktop\\test-labels.idx1-ubyte"// val testImageFilePath = "C:\\Users\\42532\\Desktop\\test-images.idx3-ubyte" val conf = new SparkConf().setMaster("local[*]").setAppName("NaiveBayesExample") val sc = new SparkContext(conf) val trainLabel = MNIST_Util.loadLabel(trainLabelFilePath) val trainImages = MNIST_Util.loadImages(trainImageFilePath) val testLabel = MNIST_Util.loadLabel(testLabelFilePath) val testImages = MNIST_Util.loadImages(testImageFilePath) // Train a GradientBoostedTrees model. // The defaultParams for Classification use LogLoss by default. val boostingStrategy = BoostingStrategy.defaultParams("Classification") boostingStrategy.numIterations = 3 // Note: Use more iterations in practice. boostingStrategy.treeStrategy.numClasses = 2 boostingStrategy.treeStrategy.maxDepth = 5 // Empty categoricalFeaturesInfo indicates all features are continuous. boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]() //处理成mlLib能用的基本类型 LabeledPoint if(trainLabel.length == trainImages.length) &#123; //标签数量和图像数量能对上则合并数组 Array[(labe,images)] val data = trainLabel.zip(trainImages) .filter(d =&gt; &#123;d._1.toInt == 0 || d._1.toInt == 1&#125;) //梯度树不能处理多分类问题，这里处理成判断0和1 .map( d =&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble))) ) val trainRdd = sc.makeRDD(data) println("开始计算") val model = GradientBoostedTrees.train(trainRdd, boostingStrategy) model.save(sc,userPath + "/src/main/resources/model/GBDT") println("检验结果") val testData = testLabel.zip(testImages) .filter(d =&gt; &#123;d._1.toInt == 0 || d._1.toInt == 1&#125;) //梯度树不能处理多分类问题，这里处理成判断0和1 .map(d =&gt;( d._1.toInt,Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble )) )) val testRDD = sc.makeRDD(testData.map(_._2)) val res = model.predict(testRDD).map(l =&gt; l.toInt).collect() //res.foreach(println(_)) val tr = res.zip(testData.map(_._1)) val sum = tr.map( f =&gt;&#123; if(f._1 == f._2.toInt) 1 else 0 &#125;).sum println("准确率为："+ sum.toDouble /tr.length.toDouble) &#125; &#125;&#125; 我们运行一下看看，用他默认的参数居然就有99.76% 的识别率！因为只是0和1，所以训练样本也少了五分之一，光看准确率可能并不能说明什么，不过也可以看出，对于二分类问题它是非常厉害的，而且训练时间也比随机森林快很多（可能随机森林调的参数树比较多，而且样本数据比较少）。 相关术语泛化能力（generalization ability）：​ 是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据对背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。 鲁棒性（Robustness）：​ 从英文翻译而来-健壮是体质强壮健康的特性。当它被转换到系统中时，它指的是容忍可能影响系统功能体的扰动的能力。在同一行中，鲁棒性可以定义为“系统抵抗变化而不适应其初始稳定配置的能力”。 后记其实梯度提升决策树最适合的用处并不是在分类上，而是处理一些回归问题，在今后的学习中我肯定也会遇到类似的问题，到时候在拿相关的数据集来进一步学习和尝试吧，今天这篇只能算是初探梯度提升决策树，一些推导和理论我也还不是太清楚明白，这些也都是有待以后补充了。 参考链接 《Spark2.0机器学习系列之5：GBDT（梯度提升决策树）、GBDT与随机森林差异、参数调试及Scikit代码分析》 《[梯度提升树(GBDT)原理小结]》 《GBDT（梯度提升决策树）》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记6：使用随机森林算法识别手写数字]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B06%EF%BC%9A%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[前言上一篇文章中我们讲到了一种常用的分类算法——决策树，今天我们要用到的随机森林算法，正是基于决策树的变种算法。随机森林算法(Random Forest) 和 *梯度提升决策树(*GradientBoosted Trees) 都是一种 集成学习（Ensemble Learning) 算法，核心的思想就是 “三个臭皮匠顶得过一个诸葛亮” 哈哈哈，就是假如一棵决策树他的分类预测可能是错的话，那么多颗树组成的森林，各自预测后通过投票得到一致的分类预测，那应该就不会错了吧，毕竟一棵错不可能棵棵都是错的嘛，这背后其实体现了一种群体的智慧。 讲到的知识点稍多，所以篇幅有点长，将就下。。。 集成学习（Ensemble Learning)在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。 集成学习在各个规模的数据集上都有很好的策略。 数据集大：划分成多个小数据集，学习多个模型进行组合 数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合 而集成学习算法里面，用得最多的就是 套袋法(Bagging) 和 提升法(Boosting) ,当决策树和套袋法结合到一起的时候，就是我们今天要讲的随机森林(Random Forest),而当决策树和提升法结合到一起就是梯度提升决策树(GradientBoosted Trees)了。下面简单地说下Bagging 和 Boosting两种算法的算法过程和区别。 套袋法(Bagging)Bagging的算法过程如下： 从原始样本集中使用 Bootstrapping 方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复） 对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等） 对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同） 这里的抽样算法 —— 自助法(Bootstrapping)，在统计学中，是一种从给定训练集中有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。这里给出一张抽样后生成决策树的图，应该比较容易地去理解了，值得注意的是，样本放回是在每一次抽取样本的时候，所以看下图会发现同一个样本被多次抽了回去。 提升法(Boosting)Boosting的算法过程如下： 对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。 进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大） Bagging，Boosting的主要区别 样本选择上：Bagging采用的是Bootstrap随机有放回抽样；而Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重。 样本权重：Bagging使用的是均匀取样，每个样本权重相等；Boosting根据错误率调整样本权重，错误率越大的样本权重越大。 预测函数：Bagging所有的预测函数的权重相等；Boosting中误差越小的预测函数其权重越大。 并行计算：Bagging各个预测函数可以并行生成；Boosting各个预测函数必须按顺序迭代生成。 随机森林算法(Random Forest)随机森林算法的基本思想随机森林是决策树的集合，可以说随机森林是用于分类和回归的最成功的机器学习模型之一。它们组合了许多相互独立没有关联的决策树，以降低过度拟合的风险。随机森林的出现也正是为了解单一决策树可能出现的很大误差和过拟合(over-fitting)的问题。 随机森林的“随机“选取数据的随机选取关于数据的随机选取，用的就是上面提到的抽样算法——自助法，首先，从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。 待选特征的随机选取与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征（通过上一篇文章提到的信息熵来选取 ）。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。以下图为例来说明随机选取待选特征的方法。 随机森林算法的优缺点随机森林的优点： 具有极高的准确率 随机性的引入，使得随机森林不容易过拟合 随机性的引入，使得随机森林有很好的抗噪声能力 能处理很高维度的数据，并且不用做特征选择 既能处理离散型数据，也能处理连续型数据，数据集无需规范化 训练速度快，可以得到变量重要性排序 容易实现并行化 随机森林的缺点： 当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大 随机森林模型还有许多不好解释的地方，算是个黑盒模型，由于几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。 随机森林算法可以解决回归问题，但是由于不能输出一个连续型值和作出超越训练集数据范围的预测，导致在对某些噪声的数据进行建模时出现过度拟合 基于随机森林的手写数字识别Spark-Mllib 中带有了随机森林的算法，在org.apache.spark.mllib.tree.RandomForest这个类里面，我们根据它提供的格式输入数据以及参数即可得到训练模型。因为前几期已经见过如何处理MNIST的手写数字数据，所以这里就不重复说了，代码其实基本和上次的朴素贝叶斯 用到的是一样的，只不过换了一个训练的算法以及多了随机森林算法的可调参数，所以这里我就着重讲解参数，需要看完整的代码的同学可前往github查看：完整代码 MLlib中的参数及部分代码这里先贴出部分代码，然后介绍参数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package SparkMLlib.Classificationimport SparkMLlib.Base.MNIST_Utilimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.mllib.tree.RandomForestobject RondomForestExample &#123; def main(args: Array[String]): Unit = &#123; // 获取当前运行路径 val userPath = System.getProperty("user.dir") val trainLabelFilePath = userPath + "/src/main/resources/data/train-labels.idx1-ubyte" val trainImageFilePath = userPath + "/src/main/resources/data/train-images.idx3-ubyte" val testLabelFilePath = userPath + "/src/main/resources/data/t10k-labels.idx1-ubyte" val testImageFilePath = userPath + "/src/main/resources/data/t10k-images.idx3-ubyte" val conf = new SparkConf().setMaster("local[*]").setAppName("NaiveBayesExample") val sc = new SparkContext(conf) val trainLabel = MNIST_Util.loadLabel(trainLabelFilePath) val trainImages = MNIST_Util.loadImages(trainImageFilePath) val testLabel = MNIST_Util.loadLabel(testLabelFilePath) val testImages = MNIST_Util.loadImages(testImageFilePath) // Train a RandomForest model. // Empty categoricalFeaturesInfo indicates all features are continuous. val numClasses = 10 val categoricalFeaturesInfo = Map[Int, Int]() val numTrees = 3 // Use more in practice. val featureSubsetStrategy = "auto" // Let the algorithm choose. val impurity = "gini" val maxDepth = 5 val maxBins = 32 //处理成mlLib能用的基本类型 LabeledPoint if(trainLabel.length == trainImages.length) &#123; //标签数量和图像数量能对上则合并数组 Array[(labe,images)] val data = trainLabel.zip(trainImages).map( d =&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble))) ) val trainRdd = sc.makeRDD(data) println("开始计算") val model = RandomForest.trainClassifier(trainRdd, numClasses, categoricalFeaturesInfo,numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins) println("检验结果") val testData = testImages.map(d =&gt; Vectors.dense(d.map(p =&gt; (p &amp; 0xFF).toDouble ))) val testRDD = sc.makeRDD(testData) val res = model.predict(testRDD).map(l =&gt; l.toInt).collect() //res.foreach(println(_)) val tr = res.zip(testLabel) val sum = tr.map( f =&gt;&#123; if(f._1 == f._2.toInt) 1 else 0 &#125;).sum println("准确率为："+ sum.toDouble /tr.length) &#125; &#125;&#125; 用法还是蛮简单的，数据格式处理成LabeledPoint 即可,主要的参数就是这么几个，现在来一一讲解： numClasses：分类的数目 比如手写数字0 - 9则分类的数目为10。这里定死了也意味着随机森林算法不能增量添加需要训练的分类，也不适合分类数目未知的数据（这种情况用聚类算法=。=） categoricalFeaturesInfo： 指定离散特征 原文：”Map storing arity of categorical features. An entry (n to k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, …, k-1}.“。是一个map，用来表明特征和类别的类型。 numTrees： 森林中的树木数量。 增加树的数量将减少预测的方差，从而提高模型的测试时间准确性。 训练时间在树木数量上大致线性增加。 featureSubsetStrategy：要用作每个树节点处拆分的候选特征的数量。 该数字被指定为特征总数的分数或函数。减少这个数字会加快培训速度，但如果太低，有时会影响性能。一般来说填 auto 让算法自己选择就可以了。 impurity：用于信息增益计算的标准。详细可查看我上一篇博客。 “ gini ”：基尼不纯度 “ entropy ”：信息熵 maxDepth：森林中每棵树的最大深度。 更深的一棵树意味模型预测更有力，需要的训练时间也越长，而且更容易过度拟合。 但是值得注意的是，随机森林算法和单一决策树算法对这个参数的要求是不一样的。随机森林由于是多个的决策树预测结果的投票或平均而降低而预测结果的方差，因此相对于单一决策树而言，不容易出现过拟合的情况。所以随机森林可以选择比决策树模型中更大的maxDepth。甚至有的文献说，随机森林的每棵决策树都最大可能地进行生长而不进行剪枝。但是不管怎样，还是建议对maxDepth参数进行一定的实验，看看是否可以提高预测的效果。 maxBins：决策规则集，可以理解成是决策树的孩子节点的数量。(suggested value: 100) 对MNIST数据集进行训练并查看准确率我们先按照Spark MLlib里面demo 的参数来跑一下看看： 准确率才68.48%。。。。。。甚至比朴素贝叶斯的准确率还低吧(╯‵□′)╯︵┻━┻不过别急，也就3棵树，深度也就5而已，让我们调大点再来试试： 改了森林中树的数量及深度，把信息增益计算的标准换成了上一篇文章着重讲的信息熵，准确率立马飙升到了95.36% 啊！对比朴素贝叶斯训练出来的模型，准确率只有85.65%，看来随机森林不愧是最强的分类算法啊。 时间允许的话，应该多尝试不同的参数看看识别率的，理论上来讲应该会在某个概率区间收敛，但是我笔记本真的不行啊。。跑一次要十几分钟说，温度爆表cpu也占满了，最后在网上找资料看别人的设置，针对这个MNIST数据集的话，树的数量大约是29，深度大约是30，就能得到一个不错的识别率了。下面贴出我跑的结果,识别率为96.47%，有兴趣的小伙伴可以尝试更多的参数组合，并在评论区留言给大家参考下。（PS：有随机因素的影响下，同样参数跑出来的模型也有所偏差，想要得到最完美的模型，只能多试几次随机种子） 更新！！！多试了几次之后（跑一次要20分钟QAQ），得到一个最高的识别率为96.59% 一些有待解决的问题训练随机森林还是挺吃资源的，至少我的笔记本跑起来并不是很快。。而且第一次跑的时候还内存溢出了，只是13棵树而已，在运行参数里改大了内存分配才解决，添加运行参数-Xmn16m -Xms64m -Xmx8000m 就ok了，不过我的电脑是16G内存的，所以才分配8G左右过去，没有多尝试，不过8G至少能跑到29棵树，深度为30。 机器资源好解决，有机会放上spark集群的话应该会好很多，这方面的暂时还没条件去尝试，毕竟随机森林的这种设计，真的很适合放到分布式集群上去啊。 还有一个需要尝试的就是如何利用显卡的GPU去做运算，有机会查查资料出一期window上怎么利用GPU跑机器学习算法，TensorFlow我倒是知道怎么做，Spark应该也可以才对。 另外还有一些疑问就是，随机森林的两个随机中，样本数据的随机抽样，是不是抽出来的每一个样本都和原始数据样本是一样的，这个可以调整吗？ 特征值的随机呢?每次随机抽出n个备选特诊，然后从备选特征中用几个?是否放回或重复? 或许以后的学习中会有新的知识解决我这些疑惑（大概是统计学里抽样的科学）。 参考链接 《知乎——集成学习（Ensemble Learning)》 《随机森林算法学习(RandomForest)》 《随机森林算法（有监督学习）》 《spark官网——随机森林及梯度提升决策树》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记5：决策树与信息熵]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B05%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E4%BF%A1%E6%81%AF%E7%86%B5%2F</url>
    <content type="text"><![CDATA[前言前面提到了朴素贝叶斯，一种通过对比概率来进行分类的分类算法，今天我们来讲讲另外一种基于信息熵的分类算法——决策树。有点意思的是，朴素贝叶斯的基础理论是概率论，帮助我们作出分类判断的是哪个分类（或选项）的概率最高，而决策树恰恰相反，是基于信息熵的。概率是某件事情（宏观态）某个可能情况（微观态）的确定性，而熵是某件事情（宏观态）到底是哪个情况（微观态）的不确定性。两者正好相反，决策树通过不断消减分类（或选项）的不确定性，从而给出正确的分类。 什么是决策树其实按照字面的理解，就是一棵帮助你做决策的树，二叉树又或者是非二叉树，树的节点就是if … else … 的判断，期望每一次的判断都是最好的决定，一直到最后获得预期最好的结果。这种策略也被称为贪心算法。 当然，这只是我的个人看法，处于一个初学者的见解而已，下面给出决策树的定义： 决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。 生硬的定义也不太好理解，我们举个具体的例子。（来自《Spark MLlib机器学习实践》——黄晓华） 小明喜欢出去玩，大多数情况他会选择天气好的条件出去，但是偶尔也会选择天气差的时候出去，而天气的标准又有如下4个属性： 温度 起风 下雨 湿度 为了简便起见，这里的每个属性均二值化为0和1，例如温度高用1表示，温度低用0表示，有如下具体记录表格： 温度 起风 下雨 湿度 出去玩 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 根据这个表格，我们来尝试构造一颗决策树，例如将是否起风作为根节点，能得出以下半成品树： 根据表格的数据，在得知是否会起风的情况下，再根据剩余的属性来判断小明是否会出去玩，比如说起风了之后，得到左子树的数据表格，明显可以看到，高温的时候就会出去玩，而低温这不会出去玩。像这样，确定了一个属性之后不断往下构造决策树，最终得到完整决策树如下： 就这样，我们一步一步就构建了一个决策树出来了，从图上看起来，就是不断地if … else …作出决定嘛，好像也没什么嘛。事实上真的是这样的吗？为什么要吧 起风 放在根节点呢？为什么不能放 温度 呢？ 我们如何去判断把哪个属性作为根节点往下分裂呢？ 举例的这棵决策树的构建过程，其实就是广为应用的ID3算法的算法流程。基本的ID3算法是通过自顶向下构造决策树进行学习的。首先考虑的问题是哪一个属性将在树的根节点测试。为解决这一问题，使用统计测试来确定每一个实例属性单独分类训练样本的能力。将分类能力最好的属性作为树的跟节点，之后根节点属性的每一个可能值会产生一个分支，然后把训练样例排列到适当的分支下，重复整个过程，用每个分支结点关联的训练样本来选择最佳属性。这是对合格决策树的贪婪搜索，也就是说算法从不回溯重新考虑以前的选择。 至于怎么去判断哪个属性的分类能力最好，这就要提到了决策树的基础理论——信息熵 信息熵与决策树的构造信息熵可以说是决策树算法里面的理论基础了，通过计算各个属性的信息熵，求出信息增益才能最好地去确定接下来用哪个属性去做分裂，有效的减小了树的深度，最快地降低不确定性，得出预测分类。可以说理解了信息熵，基本上就能理解决策树了，接下来我们来看看信息熵的定义以及如何计算信息熵。 信息熵的含义定义： 当一件事情有多种可能情况时，这件事情对某人而言具体是哪种情况的不确定性叫做熵，而能够消除该人对这件事情不确定性的事物叫做信息 为了区分开热力学上的熵，所以才会有信息熵这个概念，准确来说应该叫做信息的熵，是一个衡量一个事件的状态的单位，跟千克，米类似的，他是一个实实在在的客观物理量，而不是虚无缥缈的抽象概念。获取信息等于消除熵，这是什么意思呢，就拿上面的例子来说，得知外面的天气是“起风了”，那对于判断小明会不会出去玩就更有“把握”了，这里获知的“起风了”，就是获取了“信息“，更有”把握“则表明进一步确定小明会不会出去玩，就是消除了对”小明会不会出去玩“这件事的”不确定性“，也就是消除了熵。 是不是稍微有一点能理解了呢。一个事件或者属性中，其信息熵越大，所包含的不确定信息越大，对数据分析的计算就越有价值，因为得到了其中包含的信息，就消除了事件的不确定性，因此决策树总是优先选择拥有最高信息熵的属性作为待测属性，并以此分裂子树。 信息熵的计算公式假设事件一共有n种属性，其各自对应的概率为：$P_1,P_2,…,P_n$ ，且各属性之间出现时彼此互相独立无相关性。此时可以将信息熵定义为单个属性的对数的平均值。即：$$E(P) = E(-log\ P_i) \ = -\sum_{i=1}^{n} P_i logP_i$$更详细的推导过程可以看下我下面推荐的第二个视频，包括那个负号是哪来的。。。这里我们结合上面表格的数据举一个例子，来套套公式，看下信息熵的计算流程。 以 出去玩 为例，首先根据公式计算出去玩的概率，从表格可知，一共6个样本数据，0出现了2次，1出现了4次，分别求两者的概率：$P_1 = \frac{4}{2 + 4} = \frac{4}{6}$$P_2 = \frac{2}{2 + 4} = \frac{2}{6}$$E(o) = -\sum_{i=1}^{n} P_i logP_i = -(\frac{4}{6} log_2 \frac{4}{6}) -(\frac{2}{6} log_2 \frac{2}{6}) ≈ 0.918$即出去玩（out）的信息熵为0.918。同样用公式套进去，可得：温度的信息熵为：$E(t) ≈ 0.809 $起风的信息熵为：$E(w) ≈ 0.459 $下雨的信息熵为：$E(r) ≈ 0.602 $湿度的信息熵为：$E(h) ≈ 0.874 $ 利用信息熵构造决策树通过上面的公式，我们求出了各个属性的信息熵，那决策树是怎么去利用这些熵的呢？这里我就简单介绍其中的一种经典决策树构造算法——ID3算法。 ID3算法是一种贪心算法，以信息熵的下降速度作为选取测试属性的标准，即在每个节点选取还尚未被用来划分的，具有最高信息增益的属性作为划分标准，然后递归这个过程，直到生成的决策树能完美分类训练样例。可以说，ID3算法的核心其实就是信息增益的计算。 所谓的信息增益(Information Gain)，其实指的就是一个事件中，事件发生前后的信息量之间的差值，即：熵A - 属性熵B，也就是说，一开始是Ａ，属性划分之后变成了Ｂ，则属性引起的信息量变化是A - B，即信息增益（它描述的是变化Delta）。用公式来表示即是：$$Gain(P_1,P_2) = E(P_1) - E(P_2)$$好的属性就是信息增益越大越好，即变化完后熵越小越好（熵代表混乱程度，最大程度地减小了混乱）。因此我们在树分叉的时候，应优先使用信息增益最大的属性，这样降低了复杂度，也简化了后边的逻辑。例如我们在上面的那个例子里面，我们把是否出去玩的信息熵作为最后的数值，而每个不同的划分属性与其相减则可获得各个属性的信息增益：$Gain(o,t) = 0.918 - 0.809 = 0.109​$$Gain(o,w) = 0.918 - 0.459 = 0.459​$$Gain(o,r) = 0.918 - 0.602 = 0.316​$$Gain(o,h) = 0.918 - 0.874 = 0.044​$ 从上面就可以看出，信息增益最大的是”起风“这个属性，所以它首先被选中作为决策树的跟节点，下往下划分子树，以此类推得出最终完整的决策树。 就这样通过信息熵计算出每个属性的信息增益，以此来决定决策树划分属性，不断循环这个过程，最终得到一棵”最优“的决策树。 关于信息论需要进一步了解的东西那些不能够消除某人对某件事情不确定性的事物被称为数据或噪音。 噪音是干扰某人获得信息的事物。 而数据是噪音与信息的混合，需要用知识将其分离。 这里给出两个视频，up主讲得非常清楚，对于像我这种从未接触过信息论的人来说，简直是醍醐灌顶，所以在这里也分享给大家。什么是信息，什么是信息熵？： **如何计算信息熵** 相关术语与名词节点杂质（node impurity）节点杂质是节点处标签的均匀度的量度。一开始不太理解，国内基本都是直接翻译过来并没有很好的解释什么叫节点杂质。然后看到一个国外前辈给出的一个例子，直接搬来： In simple terms, let’s say you are trying to predict whether you will go out or not based on weather parameters. If it is raining you will definitely not go. So all observations at this point are ‘No’ i.e. pure node While if is is not raining you will check weather temperature is below 20 C then “yes” else “no”. This node is impure node. You can measure this impurity based on metric of your choice. Thus deciding how you split the variables (concept from decision trees https://www.youtube.com/watch?v=Zze7SKuz9QQ) You can choose your impurity measure based on requirement as Peter has suggested. 就是说这个节点不纯度，其实是衡量这个节点能够直接给出结论，能给出多少程度的结论，这个多少程度就是节点不纯度，它应该是一个可以衡量的单位，例如基尼不纯度，信息熵，方差之类的。 再直观点去看，看我上面给出的那颗树，温度高低这个节点能直接决定出不出去玩，那么这个节点就是“纯洁的节点”，可以说节点不纯度为0%，因为它给出的“信息”足以帮我们确定一个事实。而是否下雨这个节点不能完全帮我们下定论，如果不下雨的话还要看湿度这个节点，那么可以说下雨这个节点是“有杂质的节点”，节点不纯度为50%（当然具体计算肯定不是这样，下面有公式）。 以上皆属于个人理解，如有偏差欢迎留言指正。 基尼不纯度（Gini Impurity）基尼不纯度：将来自集合的某种结果随机应用于某一数据项的预期误差率。大概意思是 一个随机事件变成它的对立事件的概率。$$I_G(f)=\sum_{i=1}^mf_i(1-f_i)=\sum_{i=1}^mf_i-\sum_{i=1}^mf_i^2=1-\sum_{i=1}^mf_i^2​$$ 显然基尼不纯度越小，纯度越高，集合的有序程度越高，分类的效果越好； 基尼不纯度为 0 时，表示集合类别一致； 基尼不纯度最高（纯度最低）时,$f_1=f_2=\ldots =f_m=\frac1m$,$I_G(f)=1-(\frac1m)^2\times m=1-\frac1m$ 例，如果集合中的每个数据项都属于同一分类，此时误差率为 0。如果有四种可能的结果均匀地分布在集合中，此时的误差率为 1−0.25=0.75； 过拟合（Over-fitting）在统计学中，过拟合现象是指在拟合一个统计模型时，使用过多参数。 对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。一个出名的谈及过拟合的案例，就是某个科学家说，只要给我4个点，我就能画出一头大象，要是再加1个点，鼻子还能甩。。。。。。 在决策树算法中，可能会过于针对训练数据，其熵值与真实情况相比可能会有所降低。剪枝的过程就是对具有相同父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否会小于某个指定的阈值。如果确实如此，则这些叶节点会被合并成一个单一的节点，合并后的新节点包含了所有可能的结果值。这种做法有助于过度避免过度拟合的情况，使得决策树做出的预测结果，不至于比从数据集中得到的实际结论还要特殊 后记信息论不止作用在决策树中，还有很多分类算法都能用到这个思想，在机器学习领域中举足轻重。在这次的学习过程中，不断的查询资料和学习，让我对数学有了重新的认识，感觉还是学到了不少东西的。 另外信息熵并不是决策树唯一的划分标准，Spark MLlib 中提供的API还可以选”基尼不纯度“来作为选择属性的依据，只不过思想都是大同小异的。构造算法也不是只有ID3一种，只是扩展讲下去怕是越讲越多。。。 该篇算法介绍暂时不配代码来做了，代码留到下一期随机森林里面做，因为随机森林也就是多颗决策树组合起来的而已嘛。]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记4：将自己手写的图片处理成MNIST格式]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B04%EF%BC%9A%E5%B0%86%E8%87%AA%E5%B7%B1%E6%89%8B%E5%86%99%E7%9A%84%E5%9B%BE%E7%89%87%E5%A4%84%E7%90%86%E6%88%90MNIST%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言今天要讲的呢，是自己制作手写图片，并处理成MNIST的标准格式，输入到我们训练好的模型来做识别，看看效果怎样，一直用别人提供的东西，调调api什么的总感觉参与感少了点哈哈。 本来这一部分我是打算给加到MNIST数据集介绍和朴素贝叶斯那两期后面作为扩展阅读的，因为觉得应该是蛮简单的一件事，没想到实际做下来又花了我2天的时间。遇到了不少难点和细节处理，所以最后决定独立一篇文章算了。 今天这篇呢，会涉及到图像处理的一些相关概念，比如RGB转灰度，还有图像在计算机里面的存储方式；也会涉及到一些计算机二进制的处理，蛮多知识盲点的，网上资料也不齐全，所以接下来会整理一下，结合自己的理解把这个学习过程记录下来。特别注意的是，网上相关代码基本都是python的，都是调用封装好的工具类，我这会用scala来全部手动实现一遍，希望下次跟我一样的小菜鸟遇到这些问题的时候不会像我那样束手无策了哈哈哈哈哈哈哈哈 制作手写图片这里为了省事，我就直接用ps新建一张28 x 28像素的图片来写数字了，不过一开始做出来的图片我拿去识别效果并不好，所以拿了训练数据输出灰度矩阵，再对比自己手写的数字输出的灰度矩阵，发现识别不准的原因主要出现在以下两个地方： 手写数字要尽可能居中，且不要占满整个图片，边缘要留出部分空白 画笔像素过大，边缘像素有所偏差。 对于上述问题，经过我多次尝试后，画笔参数调成柔边，2像素效果较好，创建过程如图： 画笔的选择： 图片的灰度处理手写出来的图片看着是黑白的，其实是RGB真彩图片，要变成我们MNIST的标准格式，还要先把24位深的图片处理成8位深的灰度图。 几种灰度化方法 分量法：使用RGB三个分量中的一个作为灰度图的灰度值。 最值法：使用RGB三个分量中最大值或最小值作为灰度图的灰度值。 均值法：使用RGB三个分量的平均值作为灰度图的灰度值。 加权法：由于人眼颜色敏感度不同，按下一定的权值对RGB三分量进行加权平均能得到较合理的灰度图像。一般情况按照：Y = 0.30R + 0.59G + 0.11B。 从网上看到的资料来说哈，据说是加权法出来的图片效果最好。实际上java有自己的灰度化办法： 123456789101112131415161718public void grayImage() throws IOException&#123; File file = new File(System.getProperty("user.dir")+"/test.jpg"); BufferedImage image = ImageIO.read(file); int width = image.getWidth(); int height = image.getHeight(); BufferedImage grayImage = new BufferedImage(width, height, BufferedImage.TYPE_BYTE_GRAY); for(int i= 0 ; i &lt; width ; i++)&#123; for(int j = 0 ; j &lt; height; j++)&#123; int rgb = image.getRGB(i, j); grayImage.setRGB(i, j, rgb); &#125; &#125; File newFile = new File(System.getProperty("user.dir")+"/method1.jpg"); ImageIO.write(grayImage, "jpg", newFile); &#125; 看代码就是创建一个灰度图对象，然后把原图的RGB像素点强塞进去，输出成灰度图。出来的效果相当简陋，对于人来说看起来颜色比加权法出来的要糟糕不少，下面给出对比图片： 原图: Java自带的处理出来的灰度图： 自己实现的加权法出来的灰度图： 很明显的，加权法出来的图片肉眼看起来更清晰，因此最后我选择用scala来实现图片的灰度化 scala中实现图片灰度化说是scala实现吧，其实还是调了java的包哈哈哈，完整代码我放到了github上，顺手做了个批量读取文件夹里图片拼装成MNIST数据格式，所以分了3个class，于下列代码有点不一样，所以想要看如何实现还是去看完整代码吧：完整代码这里一步步来，首先是读取图片文件： 12val file = new File(path)val img = mageIO.read(file) 把图片灰度化： 1234567891011121314151617181920212223/* 把图片转成灰度值图片 * 原理其实还蛮简单的，就是按比例来取RGB三色，例如按比例3:6:1，就是R*0.3 + G*0.59 + B*0.11得出灰度值 */def getGrayImg(img:BufferedImage): BufferedImage = &#123; //宽度（行） val width = img.getWidth //高度（列） val height = img.getHeight //创建一个灰度图对象 val grayImg = new BufferedImage(width,height,img.getType) //按比例计算灰度值 for(i &lt;- 0 until width ; j &lt;- 0 until height)&#123; val color = img.getRGB(i,j) val r = (color &gt;&gt; 16) &amp; 0xff val g = (color &gt;&gt; 8) &amp; 0xff val b = color &amp; 0xff val gray =255 - (0.3 * r + 0.59 * g + 0.11 * b).toInt val newPixel = colorToRGB(256, gray, gray, gray) grayImg.setRGB(i,j,newPixel) &#125; grayImg&#125; 到这一步就已经得到了一个灰度化图片了，接下来我们只需要按照MNIST格式 把灰度图拼装成MNIST格式按照MNIST官网的格式，把一些数据信息转成byte数组拼接起来，再把灰度图的像素值以8位每个点（1byte）拼接起来就可以了，这里我直接给出代码，为了方便我还写了个批量读取文件夹内的图片，把这些图片全部转成byte数组再一一进行拼接。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182package SparkMLlib.Baseimport java.io.&#123;File, FileInputStream, FileOutputStream&#125;import java.nio.ByteBufferimport SparkMLlib.Base.FileUtil.usingimport SparkMLlib.Base.ImageUtil.&#123;getGrayArray, getGrayImg&#125;import javax.imageio.ImageIOimport scala.collection.mutable.ArrayBuffer/** * @author voidChen * @date 2019/2/27 14:47 */object MNIST_Util &#123; /** * 生成灰度图和MNIST格式文件 * @param args */ def main(args: Array[String]): Unit = &#123; val path = "C:\\Users\\42532\\Desktop\\own\\" val files = FileUtil.getFileList(path,".jpg","灰") //处理成（标签，灰度数组） val datas:Array[(Int, Array[Byte])] = files.map(f =&gt; (f.getName.substring(0,1).toInt,getGrayArray(ImageIO.read(f)))) //生成MNIST文件 imgToMNIST(datas) //输出灰度图像 files.foreach(f =&gt; &#123; val grayImg = getGrayImg(ImageIO.read(f)) val newFile = new File(path+"灰"+f.getName); ImageIO.write(grayImg, "jpg", newFile); &#125;) println("图像输出完成") &#125; /** * 读取标签 * @param labelPath * @return */ def loadLabel(labelPath: String): Array[Byte] =&#123; val file = new File(labelPath) val in = new FileInputStream(file) var labelDS = new Array[Byte](file.length.toInt) using(new FileInputStream(file)) &#123; source =&gt; &#123; in.read(labelDS) &#125; &#125; //32 bit integer 0x00000801(2049) magic number (MSB first--high endian) val magicLabelNum = ByteBuffer.wrap(labelDS.take(4)).getInt println(s"magicLabelNum=$magicLabelNum") //32 bit integer 60000 number of items val numOfLabelItems = ByteBuffer.wrap(labelDS.slice(4, 8)).getInt println(s"numOfLabelItems=$numOfLabelItems") //删掉前面的文件描述 labelDS = labelDS.drop(8) //打印测试数据 for ((e, index) &lt;- labelDS.take(3).zipWithIndex) &#123; println(s"image$index is $e") &#125; labelDS &#125; /** * 读取图像文件 * @param imagesPath * @return */ def loadImages(imagesPath: String): Array[Array[Byte]] =&#123; val file = new File(imagesPath) val in = new FileInputStream(file) var trainingDS = new Array[Byte](file.length.toInt) using(new FileInputStream(file)) &#123; source =&gt; &#123; in.read(trainingDS) &#125; &#125; //32 bit integer 0x00000803(2051) magic number val magicNum = ByteBuffer.wrap(trainingDS.take(4)).getInt println(s"magicNum=$magicNum") //32 bit integer 60000 number of items val numOfItems = ByteBuffer.wrap(trainingDS.slice(4, 8)).getInt println(s"numOfItems=$numOfItems") //32 bit integer 28 number of rows val numOfRows = ByteBuffer.wrap(trainingDS.slice(8, 12)).getInt println(s"numOfRows=$numOfRows") //32 bit integer 28 number of columns val numOfCols = ByteBuffer.wrap(trainingDS.slice(12, 16)).getInt println(s"numOfCols=$numOfCols") trainingDS = trainingDS.drop(16) val itemsBuffer = new ArrayBuffer[Array[Byte]] for(i &lt;- 0 until numOfItems)&#123; itemsBuffer += trainingDS.slice( i * numOfCols * numOfRows , (i+1) * numOfCols * numOfRows) &#125; itemsBuffer.toArray &#125; /** * 转换成MNIST数据 * @param grayArray * @return */ def imgToMNIST(data: Array[(Int, Array[Byte])])&#123; //组装MNIST格式的图像文件头 var images = ArrayBuffer[Byte]() images ++= intToByteArray(2051) //magicNum images ++= intToByteArray(data.length) //number of items images ++= intToByteArray(28) //number of rows images ++= intToByteArray(28) //number of columns //组装MNIST格式的标签文件头 var labels = ArrayBuffer[Byte]() labels ++= intToByteArray(2049) //magicNum labels ++= intToByteArray(data.length) //number of items //组装数据 data.foreach(d =&gt;&#123; val label = (d._1 &amp; 0xFF).toByte val image = d._2 labels += label images ++= image &#125;) // var i = 0 // images.drop(16).toArray.foreach( // b =&gt;&#123; // print(b+" ") // i = i+1 // if(i == 28)&#123; // i = 0 // println() // &#125;&#125; // ) //输出二进制文件 val imagesFile = new File("C:\\Users\\42532\\Desktop\\test-images.idx3-ubyte"); val labelsFile = new File("C:\\Users\\42532\\Desktop\\test-labels.idx1-ubyte"); val imagesWriter = new FileOutputStream(imagesFile) imagesWriter.write(images.toArray) imagesWriter.close() val labelsWriter = new FileOutputStream(labelsFile) labelsWriter.write(labels.toArray) labelsWriter.close() println("文件写入完成") &#125; /** * int到byte[] * * @param i * @return */ def intToByteArray(i: Int): Array[Byte] = &#123; val result = new Array[Byte](4) //由高位到低位 result(0) = ((i &gt;&gt; 24) &amp; 0xFF).toByte result(1) = ((i &gt;&gt; 16) &amp; 0xFF).toByte result(2) = ((i &gt;&gt; 8) &amp; 0xFF).toByte result(3) = (i &amp; 0xFF).toByte result &#125;&#125; 生成的图片和文件如下：标签起名直接做成图片文件名，如果有多个“1”的图片数据呢，把文件名弄成1-1.jpg,1-2.jpg,······这样就好了 关于图像处理的一些知识扩展图像位深一个像素用多少位表示，这里的位，只是针对每一个像素点而言的。考虑到位深度平均分给R, G, B和Alpha，而只有RGB可以相互组合成颜色。所以4位颜色的图，它的位深度是4，只有2的4次幂种颜色，即16种颜色或16种灰度等级 ) 。8位颜色的图，位深度就是8，用2的8次幂表示，它含有256种颜色 ( 或256种灰度等级 )。24位颜色可称之为真彩色，位深度是24，它能组合成2的24次幂种颜色，即：16777216种颜色 ( 或称千万种颜色 )，超过了人眼能够分辨的颜色数量。当我们用24位来记录颜色时，实际上是以2^（8×3），即红、绿、蓝 ( RGB ) 三基色各以2的8次幂，256种颜色而存在的，三色组合就形成一千六百万种颜色。 图像在计算机的存储格式一般来说，我们自己做的图片都是24位的RGB真彩图，在上述代码中，img.getRGB(i,j) 这里get出来的只有一个int值，其实他已经把RGB三原色的值全放在里面了，由高位到低位排列下来，分别是R 8位 + G 8位 + B 8位，所以我们通过右移操作符 &gt;&gt; 每次移动8位，在&amp; 0xFF 一下，就能取出RGB分别的Int 颜色值来了。而最高的32位深的图片，只是在RGB的基础三多加了一个alpha透明度通道。对于32位的像素值，里面的排列顺序由高位到低位排列下来，分别是alpha 8位 + R 8位 + G 8位 + B 8位，同样可以按位把真实值取出来。 关于 &amp; 0xFF 操作回想一下大学学的计算机导论，有一条很重要但是不被当时我所重视的知识：计算机中整数以补码的形式存储。正数的补码与原码一致，这没什么问题，当时负数的补码却是原码的反码+1，这也导致了为什么我们没做&amp; 0xFF 操作的时候直接byte转int会由出现负数的原因之一（另一个原因是java里面int都是带了符号位，表示的数字范围在 -128 ~ 127，之间，而实际像素值是 0 ~ 255的正整数）。 为什么byte类型的数字要 &amp;0xff 再赋值给int类型呢，其本质原因就是想保持二进制补码的一致性。例如二进制的的129为10000001 ，8位byte就能存下这个数字，当时若是直接转int，出来的结果却是-127，其原因就在于，因为当系统检测到byte可能会转化成int或者说byte与int类型进行运算的时候，就会将byte的内存空间高位补1（也就是按符号位补位）扩充到32位,变成1111111111111111111111111 10000001，再参与运算，这样其二进制补码其实已经不一致了。而进行与操作 &amp;0xff 则是这样的： 1111111111111111111111111 10000001 &amp; 11111111 = 000000000000000000000000 10000001​显然，这就是为什么我们在byte转成int的时候，要做一下 &amp;0xff 操作了 参考链接 《Java实现图像灰度化》 《byte为什么要与上0xff？》 《计算机中整数为什么以【补码】的形式存储》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>MachineLeaning</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记3：使用朴素贝叶斯识别手写数字图片]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B03%EF%BC%9A%E4%BD%BF%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[前言上一篇讲完了MNIST手写数字数据集的读取使用，今天呢就来讲讲使用分类算法——朴素贝叶斯算法(Naive Bayesian classification)来对这个数据集进行训练，看看使用这个算法训练出来的模型，对手写数字的识别率有多高。本人也是初学者，所以对于算法的讲述只是基于自身粗浅的理解，如有不同意见欢迎指正。话不多说，我们来进入正题。 朴素贝叶斯算法原理不得不说，这个名字起得太贴切了，在我了解完这个算法之后，才知道他真的很“朴素”。。。 关于分类问题先来感性认识一下这个算法，朴素贝叶斯算法是一个分类算法，对于分类，其实我们并不陌生，在生活中我们也常常会下意思地对事物进行“分类”： 例如走在街上，看到一个路人，我们会下意思地去判断是男是女；别人请你吃水果，看一眼就分辨出是苹果还是橘子；在奶茶店买一杯茶，看茶汤颜色会判断是红茶还是绿茶或者是乌龙茶。这里例子都说明了一件事，我们生活中无时无刻都在对事物进行着分类。 但是分类的前提，是我们得对这个事物有基本的认知，比如，舍友给了我一个牛油果，但是我没吃过牛油果，所以我不能准确地把它分类到“牛油果”这个类别去，虽然我不能明确他的分类，但是我能确定它不是“衣服”，因为它的特征跟衣服完全不一致，根据已知的事物中，它的特征跟“水果”最像，所以它是水果的概率最高，所以我们会把他分类到“水果”去。 看到是不是觉得有点“朴素”。。。对的，朴素贝叶斯算法，听起来好像很厉害，实际上就是这么一个“朴素”的思想，在已知的条件下，选出概率最高的一个类别作为该项的分类，“朴素”到我这种数学渣渣都露出了会心的微笑2333333 朴素贝叶斯分类的数学原理与流程朴素贝叶斯分类的正式定义如下： 设$x={a_1,a_2,\ldots,a_m}$为一个待分类项，而每个$a$为$x$的一个特征属性。 有类别集合$C={y_1,y_2,\ldots,y_n}$。 计算$P(y_1|x),P(y_2|x),\ldots,P(y_n|x)$。 如果$P(y_k|x)=max{P(y_1|x),P(y_2|x),\ldots,P(y_n|x)}$，则$x \in y_k$。 那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做： 找到一个已知分类的待分类项集合，这个集合叫做训练样本集。 统计得到在各类别下各个特征属性的条件概率估计,即：$P(a_1|y_1),P(a_2|y_1),\ldots,P(a_m|y_1);$$P(a_1|y_2),P(a_2|y_2),\ldots,P(a_m|y_2);$ $\vdots$$P(a_1|y_n),P(a_2|y_n),\ldots,P(a_m|y_n);$ 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：$$P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)} $$ 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：$$P(x|y_i)P(y_i)=P(a_1|y_i)P(a_2|y_i)\ldots P(a_m|y_i)=P(y_i)\prod_{j=1}^{n} P(a_j|y_i)$$ 对于朴素贝叶斯在机器学习上解决分类问题，主要是分为三个阶段：第一阶段——准备工作阶段，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。 第二阶段——分类器训练阶段，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。 第三阶段——应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。 以上引用自： 《算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)》 朴素贝叶斯的应用场景和局限优点： 算法简单容易理解，对分类结果的可解释性较强 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练。 缺点： 需要明确有哪些分类，如果给定没有出现过的类和特征，则该类别的条件概率估计将出现0，该问题被称为“零条件概率问题”。 特征之间独立的假设非常强。 在现实生活中几乎很难找到这样的数据集。在属性个数比较多或者属性之间相关性较大时，朴素贝叶斯分类模型的分类效率低。而在属性相关性较小时，朴素贝叶斯分类模型的性能最为良好。 应用场景： 欺诈检测中使用较多 垃圾邮件过滤 文本分类 人脸识别 朴素贝叶斯在Spark MLlib中的使用Spark MLlib中提供了朴素贝叶斯算法工具，我们调用的时候只需要把数据处理成spark的标准输入格式即可轻松得到训练模型。下面我将使用MNIST手写数字数据集来做训练，并用其提供的测试集计算准确率。（ps:后续会加入对自己手写数字图片进行识别，请等下一次更新_(:з」∠)_） 1、首先把标签数据和图片数据处理成 LabeledPoint 标签向量数据格式： 1234val data &#x3D; trainLabe.zip(trainImages).map( d &#x3D;&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p &#x3D;&gt; (p &amp; 0xFF).toDouble))))val trainRdd &#x3D; sc.makeRDD(data) 2、调用MLlib的api，输入数据进行训练，得到分类模型： 1val model &#x3D; NaiveBayes.train(trainRdd) 3、利用训练好的模型，对测试数据进行检验，看看识别准确率有多少： 1234567891011val testData &#x3D; testImages.map(d &#x3D;&gt; Vectors.dense(d.map(p &#x3D;&gt; (p &amp; 0xFF).toDouble )))val testRDD &#x3D; sc.makeRDD(testData)val res &#x3D; model.predict(testRDD).map(l &#x3D;&gt; l.toInt).collect()val tr &#x3D; res.zip(testLabe)val sum &#x3D; tr.map( f &#x3D;&gt;&#123; if(f._1 &#x3D;&#x3D; f._2.toInt) 1 else 0&#125;).sumprintln(&quot;准确率为：&quot;+ sum.toDouble &#x2F;tr.length) 如图，利用它提供的测试集呢，我这里得到的识别率约为0.8365，看样子识别效率并不怎么样啊，因为据我所知这份数据最高的识别率能高达99%甚至100%的，下一期我将会尝试使用随机森林算法来对这个数据集进行训练，稍微期待一下吧~ 此处只贴了部分代码，完整代码可前往github查看： 完整代码点我 参考链接 《算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)》 《机器学习|朴素贝叶斯（Naive Bayes）算法总结》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记2：MNIST手写数字的读取使用]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B02%EF%BC%9AMNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%9A%84%E8%AF%BB%E5%8F%96%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言近来广州阴雨连绵，实在是让人提不起精神来，文档什么的也看不进去，脑袋昏昏沉沉的，所以拖拖拉拉到今天才出更新=。= 重申一遍，我是沿着spark官网中MLlib的api往下去学习的，提到的相关算法都会去学习下。文档上有的东西呢，我就不赘述了，只是把自己学习过程中一些重要的，别的博客没有提到的，记录一下，以后自己复习也方便。在这篇东西之前，我已经自己看完了spark MLlib的数据类型和基本统计，没了解的可以先去看一下文档，后续会有用到。 好了，言归正传，今天要讲的是被称为机器学习界的“Hello world”，几乎每个入门系列都会用到的一个知名数据集——MNIST手写数字图像数据集，该系列的后续文章中也经常会用到，所以今天独立出来讲一下好了。 MNIST手写数字数据集首先给出官网：THE MNIST DATABASE of handwritten digits MNIST是一个经典的手写数字数据集，来自美国国家标准与技术研究所，由不同人手写的0至9的数字构成，由60000个训练样本集和10000个测试样本集构成，每个样本的尺寸为28x28，以二进制格式存储，如下图所示：这里有个比较坑的细节，就是数据采集都是外国人，他们的手写数字风格跟我们亚洲人的略有不同，这也导致识别我们自己手写的数字时候准确率下降了不少=。= 下载下来之后是4个gz压缩包： train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes) 解压之后得到的是一个二进制文件，文件的格式官网有给出，这里我顺便贴一下： TEST SET LABEL FILE (train-labels-idx1-ubyte): [offset] [type] [value] [description] 0000 32 bit integer 0x00000801(2049) magic number (MSB first) 0004 32 bit integer 60000 number of items 0008 unsigned byte ?? label 0009 unsigned byte ?? label …….. unsigned byte ?? label The labels values are 0 to 9.TEST SET IMAGE FILE (train-images-idx3-ubyte): [offset] [type] [value] [description] 0000 32 bit integer 0x00000803(2051) magic number 0004 32 bit integer 60000 number of images 0008 32 bit integer 28 number of rows 0012 32 bit integer 28 number of columns 0016 unsigned byte ?? pixel 0017 unsigned byte ?? pixel …….. unsigned byte ?? pixel Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black). 训练集和测试集的结构都是一样的，这里我就只贴出训练集的结构，标签是0到9的数字，图像则是0到255的像素值。 其实一开始看到二进制文件我也是有点懵，因为以前工作也没接触过。不过看多两眼就明白了。以images数据集来说吧，这个train-images-idx3-ubyte文件： 第0000字节到第0003字节，是一个32位int类型数字，值为2051的魔数。 第0004字节到第0007字节也是一个32位int类型数字，转换过来值为60000，表示一共有60000个训练样本。 第0008字节到第00011字节和第0012字节到第0015字节同理，代表着有28行和28列。 这里的行和列，其实就是一张图片的像素矩阵，也就是每个图片都有28×28=784个像素。所以说，从第16个字节开始，就是图片的每一个像素点的值了。 说到这里可能会有些朋友担心，会不会每一张图片都是这么排下来呢？为此我去算了下这个二进制文件的长度，刚好就是16+28*28*60000,所以文件描述信息呢就之在文件开头出现了，后面的就是每张图片的像素连在一起了，图片的规格统一被处理成28*28的图片了。 因此，读取图片信息有了如下代码： 123456789101112131415161718192021222324252627282930313233343536373839&#x2F;** * 读取图像文件 * @param imagesPath * @return *&#x2F;def loadImages(imagesPath: String): Array[Array[Byte]] &#x3D;&#123; val file &#x3D; new File(imagesPath) val in &#x3D; new FileInputStream(file) var trainingDS &#x3D; new Array[Byte](file.length.toInt) using(new FileInputStream(file)) &#123; source &#x3D;&gt; &#123; in.read(trainingDS) &#125; &#125; &#x2F;&#x2F;32 bit integer 0x00000803(2051) magic number val magicNum &#x3D; ByteBuffer.wrap(trainingDS.take(4)).getInt println(s&quot;magicNum&#x3D;$magicNum&quot;) &#x2F;&#x2F;32 bit integer 60000 number of items val numOfItems &#x3D; ByteBuffer.wrap(trainingDS.slice(4, 8)).getInt println(s&quot;numOfItems&#x3D;$numOfItems&quot;) &#x2F;&#x2F;32 bit integer 28 number of rows val numOfRows &#x3D; ByteBuffer.wrap(trainingDS.slice(8, 12)).getInt println(s&quot;numOfRows&#x3D;$numOfRows&quot;) &#x2F;&#x2F;32 bit integer 28 number of columns val numOfCols &#x3D; ByteBuffer.wrap(trainingDS.slice(12, 16)).getInt println(s&quot;numOfCols&#x3D;$numOfCols&quot;) trainingDS &#x3D; trainingDS.drop(16) val itemsBuffer &#x3D; new ArrayBuffer[Array[Byte]] for(i &lt;- 0 until numOfItems)&#123; itemsBuffer +&#x3D; trainingDS.slice( i * numOfCols * numOfRows , (i+1) * numOfCols * numOfRows) &#125; itemsBuffer.toArray&#125; 代码里面提取了文件描述信息，并返回有每一张图片像素数据组成的数组。完整代码我放在github上了： 完整代码 读取标签数据也是同理，在下一篇文章中，我将会详细讲述如何用标签和图片数据去做训练，敬请期待=。= 其实数据集到这里已经是完成读取了，后续只要转换成spark MLlib的数据格式就能用了，但是！我在把像素值由二进制byte类型转换成double类型的时候，居然出现了负数。这是怎么回事呢，看官网说的，值应该是在0到255之间才对啊。再仔细看了一遍文件架构，发现标明的像素类型为unsigned byte，即无符号byte类型，在我学习java的时候，隐约记得有学过说java类型都是有符号的，第一位即是正负号。于是去搜索了一下，找到了这篇文章：《Java中对于unsigned byte类型的转换处理》 把二进制像素值转换为Double类型数字由于Java中没有unsigned byte类型，所以一个字节只能表示（-128，127），而想要表示（0，255），也很简单，只要跟0xFF取&amp;操作即可。 原理其实就是正数的反码和补码都是其本身，负数的反码是对原码除了符号位之外作取反运算，补码则为反码+1。 更多细节可以参考：《byte为什么要与上0xff？》、《Java中对于unsigned byte类型的转换处理》下面贴出我处理逻辑的部分代码： 123val data &#x3D; trainLabe.zip(trainImages).map( d &#x3D;&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p &#x3D;&gt; (p &amp; 0xFF).toDouble)))) 下期预告下一期我将会记录我用spark MLlib中提供的朴树贝叶斯算法，对该数据集进行训练，并用测试集测试。关于该数据集呢，其实还有一点要补充的，比如把自己的手写图片转换成MNIST标准格式，还有替代MNIST手写数字集的图像数据集Fashion MNIST，这些后续都会补充进该文章。持续更新ing… 参考链接 《THE MNIST DATABASE of handwritten digits》 《（超详细）读取mnist数据集并保存成图片》 《Java中对于unsigned byte类型的转换处理》 《byte为什么要与上0xff？》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>MachineLeaning</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小技巧！从网易云音乐缓存中获取版权失效的音乐]]></title>
    <url>%2F%E5%B0%8F%E6%8A%80%E5%B7%A7%EF%BC%81%E4%BB%8E%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E7%BC%93%E5%AD%98%E4%B8%AD%E8%8E%B7%E5%8F%96%E7%89%88%E6%9D%83%E5%A4%B1%E6%95%88%E7%9A%84%E9%9F%B3%E4%B9%90%2F</url>
    <content type="text"><![CDATA[起因前两天听了赵方婧小姐姐翻唱的《知否》，惊为天人，就点了颗心收藏了。结果今天在点开一看。居然没有版权了！！！这也不是一次两次了，充了两个会员跟没充一样，说下架就下架，喜欢的歌听不到，这怎么能忍！ 把爱曲抢救回来！想到这两天听过，电脑一定有缓存的，所以去设置里面看了下缓存目录：嗯。。。果然改了名字，看文件大小就知道这个.uc后缀的就是缓存的MP3文件了，文件名应该就是它id 那这个id是什么呢？客户端看不到，我们把这首歌复制到浏览器去看看： 还是挺显眼的，参数名就叫id了哈哈哈哈 拿到这个id，去文件夹搜索一下，就能找到我们想要的歌的缓存啦 然后按照原本的想法，后缀改成MP3之后，应该直接就能听了，然而。。。右键网易云音乐打开，提示解码失败！！！看来网易云也防了一手啊，大概是做了什么加密处理。 对缓存文件反向解密考虑到文件加密后大小跟原来的基本没有变化，大概是进行了与或处理。 上google搜索了一下，很多人都遇到了，有大佬已经试出来了，就是逐字节跟0xa3与或一下就好了。 至于大佬是怎么试出来的，我个人猜测应该是拿到缓存文件，再拿到下载文件，各取第一个字节来试一下，与或虽然不能逆运算，不过应该能拿来猜一下。下面贴出转换代码 123456789101112131415161718192021222324252627282930import java.io.*;&#x2F;** * @author Chenyl * @date 2019&#x2F;2&#x2F;14 10:41 *&#x2F;public class music &#123; public static void main(String[] args)&#123; try&#123; File inFile &#x3D; new File(&quot;C:\\Users\\Void\\Desktop\\知否-赵方婧.mp3&quot;); File outFile &#x3D; new File(&quot;C:\\Users\\Void\\Desktop\\知否-赵方婧(解).mp3&quot;); DataInputStream dis &#x3D; new DataInputStream( new FileInputStream(inFile)); DataOutputStream dos &#x3D; new DataOutputStream( new FileOutputStream(outFile)); byte[] by &#x3D; new byte[1000]; int len; while((len&#x3D;dis.read(by))!&#x3D;-1)&#123; for(int i&#x3D;0;i&lt;len;i++)&#123; by[i]^&#x3D;0xa3; &#125; dos.write(by,0,len); &#125; dis.close(); dos.close(); &#125;catch(IOException ioe)&#123; System.err.println(ioe); &#125; &#125;&#125; 解出来的文件直接就可以听啦！不过不知道是不是心理作用，总觉得音质受损了= = 一些感想此方法仅限于自己学习使用，请勿无版权传播音频。其实我也是很不愿意这样做的，各大运营商为了利益把版权弄得四分五裂，听歌都要各个软件来回切换，开四五个会员就算了，还经常突然下架歌曲，甚至有偷偷删除歌单的行为出现。 希望国家早日立法，统一版权，给我们一个稳定付费的环境吧。真的，支持歌手，不差那点钱，现在是想给钱都听不到，我也是醉了。 参考链接 《网易云音乐.uc格式的缓存文件转.mp3》]]></content>
      <categories>
        <category>好玩的东西</category>
      </categories>
      <tags>
        <tag>小技巧</tag>
        <tag>网易云音乐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记1：K-means聚类分析]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B01%EF%BC%9AK-means%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言作为该系列的第一篇文章，也是我正式步入机器学习的第一篇学习日记，想说几句话，机器学习跟普通编程不一样，门槛对于大多数人来说都是算比较高的。曾经我也在惊叹和好奇中望而却步，现在在大佬的推动下也是鼓起勇气正式开始学习，完成这个系列也是我2019的目标。 由于公司用的是Spark，所以从Spark MLlib开始学起，不积硅步无以至千里，在这个系列我也会从简单一点地基础开始，一点点切实地去学习，希望看到这篇文章地小伙伴也能下定决心，一起攻克机器学习。 K-Means 聚类算法原理K-Means算法流程关于原理呢，我直接引用《使用Spark MLlib做K-mean聚类分析》里面的资料好了： 聚类分析是一个无监督学习 (Unsupervised Learning) 过程, 一般是用来对数据对象按照其特征属性进行分组，经常被应用在客户分群，欺诈检测，图像分析等领域。K-means 应该是最有名并且最经常使用的聚类算法了，其原理比较容易理解，并且聚类效果良好，有着广泛的使用。和诸多机器学习算法一样，K-means 算法也是一个迭代式的算法，其主要步骤如下: 第一步，选择 K 个点作为初始聚类中心。 第二步，计算其余所有点到聚类中心的距离，并把每个点划分到离它最近的聚类中心所在的聚类中去。在这里，衡量距离一般有多个函数可以选择，最常用的是欧几里得距离 (Euclidean Distance), 也叫欧式距离,公式如下： $$D(C,X) = \sqrt{\sum_{i=1}^n(c_i - x_i)^2}​$$ 其中 C 代表中心点，X 代表任意一个非中心点。 第三步，重新计算每个聚类中所有点的平均值，并将其作为新的聚类中心点。 最后，重复 (二)，(三) 步的过程，直至聚类中心不再发生改变，或者算法达到预定的迭代次数，又或聚类中心的改变小于预先设定的阀值。 光这么看有点不是很直观，其实就是不断换聚类中心以求达到“最合理”的分类的过程，下面我贴一张数据可视化的图，更直观地描述这个过程：如图，k-means算法在每一次迭代中，选择了更靠谱的红蓝中心点，试图让分类更合理，跟均匀。 欧式距离欧几里得公式的含义还是比较简单的，但是一开始看还是有点懵，在这里我稍微提一下自己的理解，如果不对的话欢迎留言指正=。=首先我们先回顾一下那个公式，那个n代表的是什么呢？$$D(C,X) = \sqrt{\sum_{i=1}^n(c_i - x_i)^2}​$$我们先来看看欧式距离在二维和三维中的展开：是不是觉得很熟悉咧，就是求坐标之间的距离嘛，以此类推，在4维空间5维空间或者更高维度的空间中怎么算两点坐标距离呢？那就是原版的欧式距离公式啦！n在我看来，就是代表所求的维度数嘛，对应到实际应用中，这个“维度”对应的就是我们想要聚类的对象的参数（特征）。当n个参数可以确定一个唯一（或近似）的对象，我们通过欧式距离公式计算这个对象坐标的距离，把最后n维空间坐标相近的对象聚到同一个类别里，就完成了K-Means聚类算法要做的工作了。 聚类测试数据在本文中，我们所用到目标数据集是来自UCI Machine Learning Repository的Wholesale customer Data Set。UCI是一个关于机器学习测试数据的下载中心站点，里面包含了适用于做聚类，分群，回归等各种机器学习问题的数据集。 Wholesale customer Data Set是引用某批发经销商的客户在各种类别产品上的年消费数。为了方便处理，本文把原始的CSV格式转化成了两个文本文件，取前面20行作为测试集，其余的数据作为训练集。如图： 代码样例看注释就好了，基本就是处理好数据，然后调用spark-lib包里面的kmeans方法去做训练就ok了，源码分析大概会在做完这个系列之后再考虑要不要做。。。 值得注意的是：val clusters:KMeansModel = KMeans.train(parsedTrainingData, numClusters, numIterations,runTimes)这行代码，我来介绍一下这几个参数： parsedTrainingData： 处理好的训练数据集 numClusters： 聚类的个数。这个值的选取有点学问，K的选择是K-means算法的关键，Spark MLlib在KMeansModel类里提供了computeCost方法，我的demo中也有该方法调用例子。该方法通过计算所有数据点到其最近的中心点的平方和来评估聚类的效果。一般来说，同样的迭代次数和算法跑的次数，这个值越小代表聚类的效果越好。但是在实际情况下，我们还要考虑到聚类结果的可解释性，不能一味的选择使computeCost结果值最小的那个K numIterations： K-means算法的迭代次数 runTimes： K-means算法run的次数 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&#x2F;** * @author Chenyl * @date 2019&#x2F;1&#x2F;23 15:57 *&#x2F;import org.apache.spark.&#123;SparkContext, SparkConf&#125;import org.apache.spark.mllib.clustering.&#123;KMeans, KMeansModel&#125;import org.apache.spark.mllib.linalg.Vectorsobject KMeansClustering &#123; def main (args: Array[String]) &#123; val trainingPath &#x3D; &quot;src&#x2F;main&#x2F;resources&#x2F;Wholesale customers data_training.txt&quot; &#x2F;&#x2F;训练数据集文件路径 val testPath &#x3D; &quot;src&#x2F;main&#x2F;resources&#x2F;Wholesale customers data_test.txt&quot; &#x2F;&#x2F;测试数据集文件路径 val numClusters &#x3D; 8 &#x2F;&#x2F;聚类的个数 val numIterations &#x3D; 30 &#x2F;&#x2F;K-means 算法的迭代次数 val runTimes &#x3D; 3 &#x2F;&#x2F;K-means 算法 run 的次数 val conf &#x3D; new SparkConf().setAppName(&quot;Spark MLlib Exercise:K-Means Clustering&quot;) conf &#x2F;&#x2F;TODO: 生成打包前，需注释掉此行 .setMaster(&quot;local[*]&quot;) .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val sc &#x3D; new SparkContext(conf) sc.setLogLevel(&quot;ERROR&quot;) &#x2F;&#x2F;设置日志级别，info会有很多运行日志出现，也可以打开看看 &#x2F;** *Channel Region Fresh Milk Grocery Frozen Detergents_Paper Delicassen * 2 3 12669 9656 7561 214 2674 1338 * 2 3 7057 9810 9568 1762 3293 1776 * 2 3 6353 8808 7684 2405 3516 7844 *&#x2F; val rawTrainingData &#x3D; sc.textFile(trainingPath) &#x2F;&#x2F;加载测试数据 &#x2F;&#x2F;去表头处理 val parsedTrainingData &#x3D; rawTrainingData.filter(!isColumnNameLine(_)).map(line &#x3D;&gt; &#123; Vectors.dense(line.split(&quot;\t&quot;).map(_.trim).filter(!&quot;&quot;.equals(_)).map(_.toDouble)) &#125;).cache()&#x2F;&#x2F; findK(parsedTrainingData) &#x2F;&#x2F; Cluster the data into two classes using KMeans var clusterIndex:Int &#x3D; 0 &#x2F;&#x2F;使用K-Means训练 val clusters:KMeansModel &#x3D; KMeans.train(parsedTrainingData, numClusters, numIterations,runTimes) println(&quot;Cluster Number:&quot; + clusters.clusterCenters.length) println(&quot;Cluster Centers Information Overview:&quot;) clusters.clusterCenters.foreach(x &#x3D;&gt; &#123; println(&quot;Center Point of Cluster &quot; + clusterIndex + &quot;:&quot;) println(x) clusterIndex +&#x3D; 1 &#125;) &#x2F;&#x2F;begin to check which cluster each test data belongs to based on the clustering result val rawTestData &#x3D; sc.textFile(testPath) &#x2F;&#x2F;加载测试数据 val parsedTestData &#x3D; rawTestData.map(line &#x3D;&gt;&#123; Vectors.dense(line.split(&quot;\t&quot;).map(_.trim).filter(!&quot;&quot;.equals(_)).map(_.toDouble)) &#125;) parsedTestData.collect().foreach(testDataLine &#x3D;&gt; &#123; val predictedClusterIndex: Int &#x3D; clusters.predict(testDataLine) &#x2F;&#x2F;使用训练好的模型进行分类 println(&quot;The data &quot; + testDataLine.toString + &quot; belongs to cluster &quot; +predictedClusterIndex) &#125;) println(&quot;Spark MLlib K-means clustering test finished.&quot;) &#125; private def isColumnNameLine(line:String):Boolean &#x3D; &#123; if (line !&#x3D; null &amp;&amp; line.contains(&quot;Channel&quot;)) true else false &#125; &#x2F;** * 查看最佳的K值 * @param parsedTrainingData *&#x2F; private def findK(parsedTrainingData:org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]): Unit &#x3D;&#123; val ks:Array[Int] &#x3D; Array(3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20) ks.foreach(cluster &#x3D;&gt; &#123; val model:KMeansModel &#x3D; KMeans.train(parsedTrainingData, cluster,30,1) val ssd &#x3D; model.computeCost(parsedTrainingData) println(&quot;sum of squared distances of points to their nearest center when k&#x3D;&quot; + cluster + &quot; -&gt; &quot;+ ssd) &#125;) &#125;&#125; K-Means应用场景适用场景常用的场景是在不清楚用户有几类时，尝试性的将用户进行分类，并根据每类用户的不同特征，决定下步动作。一个典型的应用场景是CRM管理中的数据库营销。举例，对于一个超市/电商网站/综合零售商，可以根据用户的购买行为，将其分为“年轻白领”、“一家三口”、“家有一老”、”初得子女“等等类型，然后通过邮件、短信、推送通知等，向其发起不同的优惠活动。 明尼苏达州一家塔吉特门店被客户投诉，一位中年男子指控塔吉特将婴儿产品优惠券寄给他的女儿 —— 一个高中生。但没多久他却来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。塔吉特百货就是靠着分析用户所有的购物数据，然后通过相关关系分析得出事情的真实状况。 在这个案例中，那个高中生少女明显是被聚到了孕妇那一类，因为她的行为模式与孕妇是很相近的。 总的来说，聚类算法大多用在用户画像、客户分群之类的，也可用于图像分析（色块相近聚集，后续会有一篇文章介绍kmeans在图像分析上的应用）另外附上一篇《K-Means算法的10个有趣用例》,有兴趣的可以详细看看 不适用场景每一种算法都不是放之四海而皆准的，下面是 K-Means 算法不适用的两个场景：这种情况如果由人来聚类，分分钟搞定，但是 K-Means 算法是这样做的：下面这种情况更微妙一些，各个子集的密集程度相差很大：看看 K-Means 算法是怎样做的：好吧，的确是差成渣了，但这真的不是算法的问题，而是我们人类对算法的选择问题。 结束语k-means算法无论是原理还是代码MLlib的使用都是较为简单，但是在某些领域其效果却是出奇的好，作为入门机器学习的第一步，K-means也向我们掀开了机器学习世界神秘面纱的一角，给我们这些初学者极大的信心。 k-means只是机器学习中的一个算法工具，更多算法，将会在后续的日子中一一学习掌握，我也会即时把学习的一些心得写成系列文章，与君共勉。 参考链接 《Spark机器学习2：K-Means聚类算法》 《K-Means算法的10个有趣用例》 《Wholesale customer Data Set》 《使用Spark MLlib做K-mean聚类分析》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kerberos探索遇到的问题及心得]]></title>
    <url>%2FKerberos%E6%8E%A2%E7%B4%A2%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[Kerberos在集群开启时做了什么kerberos在浪潮集群中启动时会自动安装clien端，此时它会针对每个服务以及机器创建对应得Principal，这些principal也将创建成Linux内的用户。 注意，这些用户将只存在于创建这些用户的机器上 关于Hive执行HQL时count(*)会出错 Hive跟HDFS一样，在集群开启Kerberos后不需要特别的设置即可使用，然而在试图执行 1select count( * ) from test 时，会出现以下报错: 原因便是上面提到的，创建的用户只在创建的机器上有，而Hive这个count其实是启动了一个MapReduce在集群上跑，集群上其他的机器没有nassoft_m这个账号，所以报错说找不到用户名。 解决办法： &ensp;&ensp;&ensp;&ensp;在每台机都加上nassoft_m这个账号即可 关于Hive遇到的权限问题 hive很多操作都是对HDFS的，而hdfs默认文件权限为drwxr-xr-x ，即文件创建者拥有全权限，同组及其他用户只拥有读取权限，然而我们登陆的principal很多情况下都不是文件创建者，所以在kerberos启动的情况下使用hive，又时会出现权限不足的报错，就是因为hive可能对hdfs进行了写入操作。 解决办法： &ensp;&ensp;&ensp;&ensp;把nassoft_m账号添加到hdfs用户组里，然后使用chmod 777(或者774)命令修改权限 查看zookeeper节点认证失败 解决办法：在zkCli.sh 中加入 1JVMFLAGS&#x3D;&quot;-Djava.security.auth.login.config&#x3D;&#x2F;usr&#x2F;hdp&#x2F;2.6.4.0-91&#x2F;zookeeper&#x2F;conf&#x2F;zookeeper_jaas.conf&quot; 关于Hbase出现的权限不足报错认证后在hbase中使用命令行出现如下错误： 解决办法：给相应的用户分配权限 1grant &#39;nassoft_r&#39;,&#39;RWXCA&#39; 关于kafka在Kerberos中的使用命令行稍有变化 生产者端命令为：bin/kafka-console-producer.sh –broker-list hd2.bigdata:6667 –topic test_wifi2 –producer.config conf/producer.properties –security-protocol SASL_PLAINTEXT 消费者端命令为：bin/kafka-console-consumer.sh –bootstrap-server hd2.bigdata:6667 –from-beginning –topic test_wifi2 conf/consumer.properties –security-protocol SASL_PLAINTEXT 另外就是java代码中需要添加以下配置： 12345678System.setProperty(&quot;java.security.krb5.conf&quot;, &quot;E:\\krb5.conf&quot;);System.setProperty(&quot;java.security.auth.login.config&quot;, &quot;E:\\kafka_client_jaas.conf&quot;);Properties props &#x3D; new Properties();props.put(&quot;security.protocol&quot;, &quot;SASL_PLAINTEXT&quot;);props.put(&quot;sasl.mechanism&quot;, &quot;GSSAPI&quot;);props.put(&quot;sasl.kerberos.service.name&quot;, &quot;kafka&quot;);...... 需要从服务器下载以下3个文件： krb5.conf kafka_client_jaas.conf kafka.service.keytab 其中kafka_client_jaas.conf稍作修改： 1234567891011121314151617181920Client&#123; com.sun.security.auth.module.Krb5LoginModule required doNotPrompt&#x3D;true useTicketCache&#x3D;true principal&#x3D;&quot;kafka&#x2F;hd2.bigdata@HADOOP.COM&quot; useKeyTab&#x3D;true serviceName&#x3D;&quot;zookeeper&quot; keyTab&#x3D;&quot;E:\\kafka.service.keytab&quot; client&#x3D;true;&#125;;KafkaClient &#123; com.sun.security.auth.module.Krb5LoginModule required doNotPrompt&#x3D;true useTicketCache&#x3D;true principal&#x3D;&quot;kafka&#x2F;hd2.bigdata@HADOOP.COM&quot; useKeyTab&#x3D;true serviceName&#x3D;&quot;kafka&quot; keyTab&#x3D;&quot;E:\\kafka.service.keytab&quot; client&#x3D;true;&#125;; solr的相关问题12Kinit nassoft_mcurl --negotiate -u : &quot;http:&#x2F;&#x2F;hd1.bigdata:8983&#x2F;solr&#x2F;&quot; 解决方法： 1234server&#x2F;scripts&#x2F;cloud-scripts&#x2F;zkcli.sh -zkhost hd2.bigdata:2181,master2.bigdata:2181,master1.bigdata:2181 -cmd put &#x2F;security.json &#39;&#123;&quot;authentication&quot;:&#123;&quot;class&quot;: &quot;org.apache.solr.security.KerberosPlugin&quot;&#125;&#125;&#39;刷新solr keytab修改 keytab 权限重启solr集群 curl --negotiate -u : &quot;http://hd1.bigdata:8983/solr/&quot;成功]]></content>
      <categories>
        <category>集群</category>
      </categories>
      <tags>
        <tag>Keeberos</tag>
        <tag>集群安全</tag>
      </tags>
  </entry>
</search>
