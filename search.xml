<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记8：K-Means的扩展学习]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B08%EF%BC%9AK-Means%E7%9A%84%E6%89%A9%E5%B1%95%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前言关于聚类的经典算法K-Means 算法在第一期就已经讲过了，需要回顾的同学点这里，现在顺着spark的api继续看下去，又看到聚类这一块，就着重讲一下基于流数据的K-Menas算法使用，因为最近比较忙的，所以感觉又一些东西都还没很完整的看完，比如回归的算法，还有聚类的一些统计学方法，这些都会在今后慢慢补上，给自己一个小目标，今年6月底把spark的api全部都过一遍，然后开始又针对地去做人脸识别相关的学习，还有神经网络和深度学习，在此之前希望我能打下较好的基础。 有点扯远了，看回今天的主题，Streaming k-means——基于流的k-means算法，其实就是在原有的k-means算法里面引入流的概念，并放在分布式的集群上去做，下面我们就来简单了解一下。 流式k-means算法spark官网讲的挺清楚的，我就翻译下搬过来了，官网地址：点这里 当数据是以流的方式到达的时候，我们可能想动态地去估算(estimate)聚类的簇，通过新的到达的数据来更新它们。spark.mllib支持流式k-means聚类，并且可以通过参数控制估计衰减(decay)（ 或“忽略”(forgetfulness) ）。 这个算法使用一般地小批量更新规则来更新簇。 对每批新到的数据，我们首先将点分配给距离它们最近的簇，然后计算新的数据中心，最后更新每一个簇。使用的公式如下所示：$$\begin{equation} c_{t+1} = \frac{c_tn_t\alpha + x_tm_t}{n_t\alpha+m_t} \end{equation}$$$$\begin{equation} n_{t+1} = n_t + m_t \end{equation}$$在上面的公式中，$c_{t}$表示前一个簇中心，$n_{t}$表示至今位置分配给这个簇的点的数量， $x_{t}$表示从当前批数据中计算出来的簇中心，$m_{t}$表示当前添加的这批数据的点数量。衰减因子 $\alpha$ 可以被用来忽略过去的聚类中心：当 $\alpha$ 等于1时，所有的批数据赋予相同的权重，当 $\alpha​$ 等于0时，数据中心点完全通过当前数据确定。 这里我想稍微讲一下这个衰减因子 $\alpha$ ，看他在公式中，当 $\alpha = 0$ 的时候，其实就相当于完全忽略掉前面历史累积下来的聚类中心的结果了，所以说他是“衰减因子”挺贴切的，这个系数可以在每一次迭代中消减历史数据带来的影响，这个系数越大，则代表着越看重最新的数据。要知道源源不断的流数据，其实还是挺看重实时数据的，比如说你冬天喜欢买羽绒，到了夏天不可能还这么喜欢买羽绒吧。。。 衰减因子$\alpha$ 也可以通过halfLife参数联合 时间单元（time unit）来确定，时间单元可以是一批数据也可以是一个数据点。假如数据从t时刻到来并定义了halfLife为h， 在t+h时刻，应用到t时刻的数据的折扣（discount）为0.5。 halfLife翻译过来就是半衰期，挺有意思的，这个参数可以让衰减因子随着时间来动态调整，越来越依赖新的数据，在过完一个时间单元（或者叫周期吧）重置。应该说这个衰减因子的变化在每个时间单元中独立生效。 算法步骤流式k-means算法的步骤如下所示： （1）分配新的数据点到离其最近的簇； （2）根据时间单元（time unit）计算折扣（discount）值，并更新簇权重； （3）应用更新规则； （4）应用更新规则后，有些簇可能消失了，那么切分最大的簇为两个簇。 应用场景流式K-means的应用场景还是挺广阔的，比如在实时推荐预测系统方面，比如广告推荐、商业预测之类的，由于推荐预测系统对数据时效性的敏感度较高，而且其数据处于连续实时且快速的变化，所以必须建立起流式的机器学习应用，从而对流式的数据进行实时的预测分析与处理，这对于商业分析与运营而言将十分关键。 另外还有一篇论文，将该算法应用在数据安全领域，用来实时验证分析大数据环境下的DDoS攻击检测，参考论文：《基于Spark Streaming的实时数据分析系统及其应用》 代码demo这里训练数据我们还是用回之前k-means用过的数据集，值得一提的是，这里的测试集需要改成(1.0), [1.7, 0.4, 0.9]这种标签向量的格式。我们就把上一个k-means聚类出来的结果作为这里的测试集试试。 每个训练点应格式化为[x1, x2, x3]，并且每个测试数据点应格式化为(y, [x1, x2, x3])，其中y是一些有用的标签或标识符（例如，真正的类别分配） 除此以外还要注意的是，spark streaming读取文件的时候，旧文件是不识别的，他只会读取新创建的文件，也就是说我们要单独写一个文件IO，把训练集和测试集以流形式给写进去。。。不过生产环境的时候，就可以上kafka来做信息读取了。 下面贴出训练的主函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package SparkMLlib.Clusteringimport org.apache.spark.SparkConfimport org.apache.spark.mllib.clustering.StreamingKMeansimport org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * @Author: voidChen * @Date: 2019/3/19 17:24 * @Version 1.0 */object StreamingKMeansExample &#123; def main(args: Array[String]) &#123; val trainingPath = "D:\\data\\train\\" //训练数据集文件路径// val testPath = userPath + "/src/main/resources/data/StreamingKmeans-test.txt" //测试数据集文件路径 val testPath = "D:\\data\\test\\" //测试数据集文件路径 val conf = new SparkConf().setAppName("StreamingKMeansExample") .setMaster("local[*]") //TODO: 生成打包前，需注释掉此行 val ssc = new StreamingContext(conf, Seconds(10)) //数据量太少这里设置了1秒给它。。 val trainingData = ssc.textFileStream(trainingPath) .filter(!isColumnNameLine(_)) .map(line =&gt; &#123;//处理数据成标准向量 println(line) Vectors.dense(line.split("\t").map(_.trim).filter(!"".equals(_)).map(_.toDouble)) &#125;) val testData = ssc.textFileStream(testPath).map(LabeledPoint.parse) val model = new StreamingKMeans() .setK(8) //聚类中心 .setDecayFactor(1.0) //损失因子 .setRandomCenters(8, 0.0) //初始化随机中心，只需要维数。 第一个参数是维数，第二个参数是簇权重 model.trainOn(trainingData) model.predictOnValues(testData.map(lp =&gt; (lp.label, lp.features))).print() ssc.start() ssc.awaitTermination() &#125; /** * 只是用来去掉表头 * @param line * @return */ def isColumnNameLine(line:String):Boolean = &#123; if (line != null &amp;&amp; line.contains("Channel")) true else false &#125;&#125; 再给出一个文件读写的简易demo： 12345678910111213141516171819202122def FileIODemo(): Unit =&#123; val f= new File("D:\\data\\test.txt")// val f= new File("D:\\data\\Wholesale customers data_training.txt") val in = new InputStreamReader(new FileInputStream(f)) val out = new OutputStreamWriter(new FileOutputStream("D:\\data\\test\\test4.txt"))// val out = new OutputStreamWriter(new FileOutputStream("D:\\data\\train\\train4.txt")) val r = new BufferedReader(in) val w = new BufferedWriter(out) var lineTxt = Option(r.readLine) while ( !lineTxt.isEmpty)&#123; println(lineTxt.getOrElse("")) w.write(lineTxt.getOrElse("")) w.flush() lineTxt = Option(r.readLine) if(!lineTxt.isEmpty) w.newLine() &#125; in.close w.close() &#125; 完整代码请前往github查看，文件路径请灵性修改。。。另外数据集分别就是Wholesale customers data_training.txt 和 StreamingKmeans-test.txt，我会放在github上：点我前往github]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>MachineLeaning</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记7：初探梯度提升决策树]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B07%EF%BC%9A%E5%88%9D%E6%8E%A2%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[前言嗯。。怎么说呢，本来见识到随机森林的强大之后，觉得分类算法里面，另一个集成算法——梯度提升决策树(GBDT)看不看都那样了，看各类书和spark官网对他的介绍也是一笔带过，所以抱着轻视的态度，顺手训练了下模型，结果识别率居然高达99.763593%！！！用的还是官方给的默认参数。。。而且训练速度贼快。我随机森林要跑15分钟，这个几分钟就好了。所以决定单独写一篇日记，来深入地去学习一下这个算法。 比较可惜地是，该算法目前还不支持多分类，不过找了下资料貌似挺多大神在一点点改进这个算法了，比如陈天奇的xgboost。总而言之，不能以一份数据来衡量机器学习算法的好坏，应该根据数据特点，选择最适合这个数据的算法，这也是我这个系列学习希望达成的目标之一。 什么是梯度提升决策树梯度提升决策树简称 GBDT ，是 Gradient Boosting Decision Tree 的缩写，也有文章称之为 MART（Multiple Additive Regression Tree），是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。注意这里的决策树算法用的并不是随机森林提到的分类树，而是回归树，而且梯度提升树目前不支持多分类问题。 梯度提升树依次迭代训练一系列的决策树。在一次迭代中，算法使用现有的集成来对每个训练实例的类别进行预测，然后将预测结果与真实的标签值进行比较。通过重新标记，来赋予预测结果不好的实例更高的权重。所以，在下次迭代中，决策树会对先前的错误进行修正。 对实例标签进行重新标记的机制由损失函数来指定。每次迭代过程中，梯度迭代树在训练数据上进一步减少损失函数的值。spark.ml为分类问题提供一种损失函数（Log Loss），为回归问题提供两种损失函数（平方误差与绝对误差）。 算法原理GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。 在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是 $f_{t-1}(x)​$，损失函数是 $L(y, f_{t-1}(x))​$， 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)​$，让本轮的损失函数$L(y, f_{t}(x) =L(y, f_{t-1}(x)+ h_t(x))​$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 构造过程这里只给出网上找到的算法步骤，里面很多推导以及公式我还没弄懂，实在有些惭愧，后续闲下来会针对这个再进一步学习。 算法如下步骤，截图来自《The Elements of Statistical Learning》： 算法步骤解释： 1、初始化，估计使损失函数极小化的常数值，它是只有一个根节点的树，即ganma是一个常数值。 2、（a）计算损失函数的负梯度在当前模型的值，将它作为残差的估计（b）估计回归树叶节点区域，以拟合残差的近似值（c）利用线性搜索估计叶节点区域的值，使损失函数极小化（d）更新回归树 3、得到输出的最终模型 f(x) GBDT的优缺点及应用场景优点： 相对少的调参时间情况下可以得到较高的准确率。 可灵活处理各种类型数据，包括连续值和离散值，使用范围广。 可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。 缺点： 弱学习器之间存在依赖关系，难以并行训练数据。 应用场景：GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。 而在实际应用中呢，多是作为一种预测分析算法，用在各行各业的领域，如： 故障预测分析 欺诈预测分析 搜索引擎用户体验提升 视频流媒体服务体验提升 个人征信 GBDT重要参数介绍 lose：损失函数。 ​ spark mllib一共提供3种，注意每一种都只适合一类问题，要么是回归，要么是分类。分类只可选择Log Loss，回归问题可选择平方误差(Squared Error)和绝对值误差(Absolute Error)。分别又称为L2损失和L1损失。绝对值误差（L1损失）在处理带有离群值的数据时比L2损失更加具有鲁棒性。分别如下表格： | Loss | Task | Formula | Description | | —- | —- | ——- | ———– | | Log Loss| Classification| 2∑Ni=1log(1+exp(−2yiF(xi)))| Twice binomial negative log likelihood.| | Squared Error| Regression| ∑Ni=1(yi−F(xi))2| 也称为L2损失。回归任务的默认损失。 | | Absolute Error| Regression| ∑Ni=1|yi−F(xi)|| 也称为L1损失。对于异常值比Squared Error更强大。 | numIterations：迭代次数。 ​ GBDT迭代次数，每一次迭代将产生一棵树，因此这个numIterations也是算法中所包含的树的数目。增加numIterations会提高训练集数据预测准确率（注意是训练集数据上的准确率哦）。但是相应的会增加训练的时间。 learningRate：学习率。 ​ 官方建议是不需要调整此参数。如果算法行为看起来不稳定，则降低此值可以提高稳定性。小的学习率（步长）肯定会增加训练的时间。 algo：使用的算法。 ​ 初始化的时候直接给出这个树的算法类型,像这样：BoostingStrategy.defaultParams(&quot;Classification&quot;) 。一共就两种,分类(Classification) 或者是 回归(Regression）。 treeStrategy.numClasses：分类数量。 ​ 由于分类只能做二分类，所以这个参数填2就可以了，多了会报错。 treeStrategy.maxDepth：树深度。 ​ 和随机森林的一样，树越深，训练时间越长。 这里面主要用到的就是这几个参数，较为有用的就是迭代次数了，为了防止过拟合，这个迭代次数也得调整下，随着迭代次数的增加，一开始在验证集上预测误差会减小，迭代次数增大到一定程度后误差反而会增加，那么通过准确度vs.迭代次数曲线可以选择最合适的numIterations。 在MNIST手写数字集上使用GBDT做分类由于这一系列暂时用的都是同一份数据集，我就不重复讲怎么处理数据了，有需要的同学可以看下前面的数据集介绍，对比上一篇随机森林的代码呢，其实差别只是换了一个训练函数，以及因为GBDT只能做二分类所以把数据过滤了下，只取了0和1的数据来做判断，下面贴出主函数，完整代码请点击这里：前往GitHub 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package SparkMLlib.Classificationimport SparkMLlib.Base.MNIST_Utilimport org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.mllib.tree.&#123;GradientBoostedTrees&#125;import org.apache.spark.mllib.tree.configuration.BoostingStrategyimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object GBDT_Example &#123; def main(args: Array[String]): Unit = &#123; // 获取当前运行路径 val userPath = System.getProperty("user.dir") val trainLabelFilePath = userPath + "/src/main/resources/data/train-labels.idx1-ubyte" val trainImageFilePath = userPath + "/src/main/resources/data/train-images.idx3-ubyte" val testLabelFilePath = userPath + "/src/main/resources/data/t10k-labels.idx1-ubyte" val testImageFilePath = userPath + "/src/main/resources/data/t10k-images.idx3-ubyte"// val testLabelFilePath = "C:\\Users\\42532\\Desktop\\test-labels.idx1-ubyte"// val testImageFilePath = "C:\\Users\\42532\\Desktop\\test-images.idx3-ubyte" val conf = new SparkConf().setMaster("local[*]").setAppName("NaiveBayesExample") val sc = new SparkContext(conf) val trainLabel = MNIST_Util.loadLabel(trainLabelFilePath) val trainImages = MNIST_Util.loadImages(trainImageFilePath) val testLabel = MNIST_Util.loadLabel(testLabelFilePath) val testImages = MNIST_Util.loadImages(testImageFilePath) // Train a GradientBoostedTrees model. // The defaultParams for Classification use LogLoss by default. val boostingStrategy = BoostingStrategy.defaultParams("Classification") boostingStrategy.numIterations = 3 // Note: Use more iterations in practice. boostingStrategy.treeStrategy.numClasses = 2 boostingStrategy.treeStrategy.maxDepth = 5 // Empty categoricalFeaturesInfo indicates all features are continuous. boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]() //处理成mlLib能用的基本类型 LabeledPoint if(trainLabel.length == trainImages.length) &#123; //标签数量和图像数量能对上则合并数组 Array[(labe,images)] val data = trainLabel.zip(trainImages) .filter(d =&gt; &#123;d._1.toInt == 0 || d._1.toInt == 1&#125;) //梯度树不能处理多分类问题，这里处理成判断0和1 .map( d =&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble))) ) val trainRdd = sc.makeRDD(data) println("开始计算") val model = GradientBoostedTrees.train(trainRdd, boostingStrategy) model.save(sc,userPath + "/src/main/resources/model/GBDT") println("检验结果") val testData = testLabel.zip(testImages) .filter(d =&gt; &#123;d._1.toInt == 0 || d._1.toInt == 1&#125;) //梯度树不能处理多分类问题，这里处理成判断0和1 .map(d =&gt;( d._1.toInt,Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble )) )) val testRDD = sc.makeRDD(testData.map(_._2)) val res = model.predict(testRDD).map(l =&gt; l.toInt).collect() //res.foreach(println(_)) val tr = res.zip(testData.map(_._1)) val sum = tr.map( f =&gt;&#123; if(f._1 == f._2.toInt) 1 else 0 &#125;).sum println("准确率为："+ sum.toDouble /tr.length.toDouble) &#125; &#125;&#125; 我们运行一下看看，用他默认的参数居然就有99.76% 的识别率！因为只是0和1，所以训练样本也少了五分之一，光看准确率可能并不能说明什么，不过也可以看出，对于二分类问题它是非常厉害的，而且训练时间也比随机森林快很多（可能随机森林调的参数树比较多，而且样本数据比较少）。 相关术语泛化能力（generalization ability）：​ 是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据对背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。 鲁棒性（Robustness）：​ 从英文翻译而来-健壮是体质强壮健康的特性。当它被转换到系统中时，它指的是容忍可能影响系统功能体的扰动的能力。在同一行中，鲁棒性可以定义为“系统抵抗变化而不适应其初始稳定配置的能力”。 后记其实梯度提升决策树最适合的用处并不是在分类上，而是处理一些回归问题，在今后的学习中我肯定也会遇到类似的问题，到时候在拿相关的数据集来进一步学习和尝试吧，今天这篇只能算是初探梯度提升决策树，一些推导和理论我也还不是太清楚明白，这些也都是有待以后补充了。 参考链接 《Spark2.0机器学习系列之5：GBDT（梯度提升决策树）、GBDT与随机森林差异、参数调试及Scikit代码分析》 《[梯度提升树(GBDT)原理小结]》 《GBDT（梯度提升决策树）》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记6：使用随机森林算法识别手写数字]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B06%EF%BC%9A%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[前言上一篇文章中我们讲到了一种常用的分类算法——决策树，今天我们要用到的随机森林算法，正是基于决策树的变种算法。随机森林算法(Random Forest) 和 梯度提升决策树(GradientBoosted Trees) 都是一种 集成学习（Ensemble Learning) 算法，核心的思想就是 “三个臭皮匠顶得过一个诸葛亮” 哈哈哈，就是假如一棵决策树他的分类预测可能是错的话，那么多颗树组成的森林，各自预测后通过投票得到一致的分类预测，那应该就不会错了吧，毕竟一棵错不可能棵棵都是错的嘛，这背后其实体现了一种群体的智慧。 讲到的知识点稍多，所以篇幅有点长，将就下。。。 集成学习（Ensemble Learning)在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。 集成学习在各个规模的数据集上都有很好的策略。 数据集大：划分成多个小数据集，学习多个模型进行组合 数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合 而集成学习算法里面，用得最多的就是 套袋法(Bagging) 和 提升法(Boosting) ,当决策树和套袋法结合到一起的时候，就是我们今天要讲的随机森林(Random Forest),而当决策树和提升法结合到一起就是梯度提升决策树(GradientBoosted Trees)了。下面简单地说下Bagging 和 Boosting两种算法的算法过程和区别。 套袋法(Bagging)Bagging的算法过程如下： 从原始样本集中使用 Bootstrapping 方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复） 对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等） 对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同） 这里的抽样算法 —— 自助法(Bootstrapping)，在统计学中，是一种从给定训练集中有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。这里给出一张抽样后生成决策树的图，应该比较容易地去理解了，值得注意的是，样本放回是在每一次抽取样本的时候，所以看下图会发现同一个样本被多次抽了回去。 提升法(Boosting)Boosting的算法过程如下： 对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。 进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大） Bagging，Boosting的主要区别 样本选择上：Bagging采用的是Bootstrap随机有放回抽样；而Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重。 样本权重：Bagging使用的是均匀取样，每个样本权重相等；Boosting根据错误率调整样本权重，错误率越大的样本权重越大。 预测函数：Bagging所有的预测函数的权重相等；Boosting中误差越小的预测函数其权重越大。 并行计算：Bagging各个预测函数可以并行生成；Boosting各个预测函数必须按顺序迭代生成。 随机森林算法(Random Forest)随机森林算法的基本思想随机森林是决策树的集合，可以说随机森林是用于分类和回归的最成功的机器学习模型之一。它们组合了许多相互独立没有关联的决策树，以降低过度拟合的风险。随机森林的出现也正是为了解单一决策树可能出现的很大误差和过拟合(over-fitting)的问题。 随机森林的“随机“选取数据的随机选取关于数据的随机选取，用的就是上面提到的抽样算法——自助法，首先，从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。 待选特征的随机选取与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征（通过上一篇文章提到的信息熵来选取 ）。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。以下图为例来说明随机选取待选特征的方法。 随机森林算法的优缺点随机森林的优点： 具有极高的准确率 随机性的引入，使得随机森林不容易过拟合 随机性的引入，使得随机森林有很好的抗噪声能力 能处理很高维度的数据，并且不用做特征选择 既能处理离散型数据，也能处理连续型数据，数据集无需规范化 训练速度快，可以得到变量重要性排序 容易实现并行化 随机森林的缺点： 当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大 随机森林模型还有许多不好解释的地方，算是个黑盒模型，由于几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。 随机森林算法可以解决回归问题，但是由于不能输出一个连续型值和作出超越训练集数据范围的预测，导致在对某些噪声的数据进行建模时出现过度拟合 基于随机森林的手写数字识别Spark-Mllib 中带有了随机森林的算法，在org.apache.spark.mllib.tree.RandomForest这个类里面，我们根据它提供的格式输入数据以及参数即可得到训练模型。因为前几期已经见过如何处理MNIST的手写数字数据，所以这里就不重复说了，代码其实基本和上次的朴素贝叶斯 用到的是一样的，只不过换了一个训练的算法以及多了随机森林算法的可调参数，所以这里我就着重讲解参数，需要看完整的代码的同学可前往github查看：完整代码 MLlib中的参数及部分代码这里先贴出部分代码，然后介绍参数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package SparkMLlib.Classificationimport SparkMLlib.Base.MNIST_Utilimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.mllib.tree.RandomForestobject RondomForestExample &#123; def main(args: Array[String]): Unit = &#123; // 获取当前运行路径 val userPath = System.getProperty("user.dir") val trainLabelFilePath = userPath + "/src/main/resources/data/train-labels.idx1-ubyte" val trainImageFilePath = userPath + "/src/main/resources/data/train-images.idx3-ubyte" val testLabelFilePath = userPath + "/src/main/resources/data/t10k-labels.idx1-ubyte" val testImageFilePath = userPath + "/src/main/resources/data/t10k-images.idx3-ubyte" val conf = new SparkConf().setMaster("local[*]").setAppName("NaiveBayesExample") val sc = new SparkContext(conf) val trainLabel = MNIST_Util.loadLabel(trainLabelFilePath) val trainImages = MNIST_Util.loadImages(trainImageFilePath) val testLabel = MNIST_Util.loadLabel(testLabelFilePath) val testImages = MNIST_Util.loadImages(testImageFilePath) // Train a RandomForest model. // Empty categoricalFeaturesInfo indicates all features are continuous. val numClasses = 10 val categoricalFeaturesInfo = Map[Int, Int]() val numTrees = 3 // Use more in practice. val featureSubsetStrategy = "auto" // Let the algorithm choose. val impurity = "gini" val maxDepth = 5 val maxBins = 32 //处理成mlLib能用的基本类型 LabeledPoint if(trainLabel.length == trainImages.length) &#123; //标签数量和图像数量能对上则合并数组 Array[(labe,images)] val data = trainLabel.zip(trainImages).map( d =&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble))) ) val trainRdd = sc.makeRDD(data) println("开始计算") val model = RandomForest.trainClassifier(trainRdd, numClasses, categoricalFeaturesInfo,numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins) println("检验结果") val testData = testImages.map(d =&gt; Vectors.dense(d.map(p =&gt; (p &amp; 0xFF).toDouble ))) val testRDD = sc.makeRDD(testData) val res = model.predict(testRDD).map(l =&gt; l.toInt).collect() //res.foreach(println(_)) val tr = res.zip(testLabel) val sum = tr.map( f =&gt;&#123; if(f._1 == f._2.toInt) 1 else 0 &#125;).sum println("准确率为："+ sum.toDouble /tr.length) &#125; &#125;&#125; 用法还是蛮简单的，数据格式处理成LabeledPoint 即可,主要的参数就是这么几个，现在来一一讲解： numClasses：分类的数目 比如手写数字0 - 9则分类的数目为10。这里定死了也意味着随机森林算法不能增量添加需要训练的分类，也不适合分类数目未知的数据（这种情况用聚类算法=。=） categoricalFeaturesInfo： 指定离散特征 原文：”Map storing arity of categorical features. An entry (n to k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, …, k-1}.“。是一个map，用来表明特征和类别的类型。 numTrees： 森林中的树木数量。 增加树的数量将减少预测的方差，从而提高模型的测试时间准确性。 训练时间在树木数量上大致线性增加。 featureSubsetStrategy：要用作每个树节点处拆分的候选特征的数量。 该数字被指定为特征总数的分数或函数。减少这个数字会加快培训速度，但如果太低，有时会影响性能。一般来说填 auto 让算法自己选择就可以了。 impurity：用于信息增益计算的标准。详细可查看我上一篇博客。 “ gini ”：基尼不纯度 “ entropy ”：信息熵 maxDepth：森林中每棵树的最大深度。 更深的一棵树意味模型预测更有力，需要的训练时间也越长，而且更容易过度拟合。 但是值得注意的是，随机森林算法和单一决策树算法对这个参数的要求是不一样的。随机森林由于是多个的决策树预测结果的投票或平均而降低而预测结果的方差，因此相对于单一决策树而言，不容易出现过拟合的情况。所以随机森林可以选择比决策树模型中更大的maxDepth。甚至有的文献说，随机森林的每棵决策树都最大可能地进行生长而不进行剪枝。但是不管怎样，还是建议对maxDepth参数进行一定的实验，看看是否可以提高预测的效果。 maxBins：决策规则集，可以理解成是决策树的孩子节点的数量。(suggested value: 100) 对MNIST数据集进行训练并查看准确率我们先按照Spark MLlib里面demo 的参数来跑一下看看： 准确率才68.48%。。。。。。甚至比朴素贝叶斯的准确率还低吧(╯‵□′)╯︵┻━┻不过别急，也就3棵树，深度也就5而已，让我们调大点再来试试： 改了森林中树的数量及深度，把信息增益计算的标准换成了上一篇文章着重讲的信息熵，准确率立马飙升到了95.36% 啊！对比朴素贝叶斯训练出来的模型，准确率只有85.65%，看来随机森林不愧是最强的分类算法啊。 时间允许的话，应该多尝试不同的参数看看识别率的，理论上来讲应该会在某个概率区间收敛，但是我笔记本真的不行啊。。跑一次要十几分钟说，温度爆表cpu也占满了，最后在网上找资料看别人的设置，针对这个MNIST数据集的话，树的数量大约是29，深度大约是30，就能得到一个不错的识别率了。下面贴出我跑的结果,识别率为96.47%，有兴趣的小伙伴可以尝试更多的参数组合，并在评论区留言给大家参考下。（PS：有随机因素的影响下，同样参数跑出来的模型也有所偏差，想要得到最完美的模型，只能多试几次随机种子） 更新！！！多试了几次之后（跑一次要20分钟QAQ），得到一个最高的识别率为96.59% 一些有待解决的问题训练随机森林还是挺吃资源的，至少我的笔记本跑起来并不是很快。。而且第一次跑的时候还内存溢出了，只是13棵树而已，在运行参数里改大了内存分配才解决，添加运行参数-Xmn16m -Xms64m -Xmx8000m 就ok了，不过我的电脑是16G内存的，所以才分配8G左右过去，没有多尝试，不过8G至少能跑到29棵树，深度为30。 机器资源好解决，有机会放上spark集群的话应该会好很多，这方面的暂时还没条件去尝试，毕竟随机森林的这种设计，真的很适合放到分布式集群上去啊。 还有一个需要尝试的就是如何利用显卡的GPU去做运算，有机会查查资料出一期window上怎么利用GPU跑机器学习算法，TensorFlow我倒是知道怎么做，Spark应该也可以才对。 另外还有一些疑问就是，随机森林的两个随机中，样本数据的随机抽样，是不是抽出来的每一个样本都和原始数据样本是一样的，这个可以调整吗？ 特征值的随机呢?每次随机抽出n个备选特诊，然后从备选特征中用几个?是否放回或重复? 或许以后的学习中会有新的知识解决我这些疑惑（大概是统计学里抽样的科学）。 参考链接 《知乎——集成学习（Ensemble Learning)》 《随机森林算法学习(RandomForest)》 《随机森林算法（有监督学习）》 《spark官网——随机森林及梯度提升决策树》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记5：决策树与信息熵]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B05%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E4%BF%A1%E6%81%AF%E7%86%B5%2F</url>
    <content type="text"><![CDATA[前言前面提到了朴素贝叶斯，一种通过对比概率来进行分类的分类算法，今天我们来讲讲另外一种基于信息熵的分类算法——决策树。有点意思的是，朴素贝叶斯的基础理论是概率论，帮助我们作出分类判断的是哪个分类（或选项）的概率最高，而决策树恰恰相反，是基于信息熵的。概率是某件事情（宏观态）某个可能情况（微观态）的确定性，而熵是某件事情（宏观态）到底是哪个情况（微观态）的不确定性。两者正好相反，决策树通过不断消减分类（或选项）的不确定性，从而给出正确的分类。 什么是决策树其实按照字面的理解，就是一棵帮助你做决策的树，二叉树又或者是非二叉树，树的节点就是if … else … 的判断，期望每一次的判断都是最好的决定，一直到最后获得预期最好的结果。这种策略也被称为贪心算法。 当然，这只是我的个人看法，处于一个初学者的见解而已，下面给出决策树的定义： 决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。 生硬的定义也不太好理解，我们举个具体的例子。（来自《Spark MLlib机器学习实践》——黄晓华） 小明喜欢出去玩，大多数情况他会选择天气好的条件出去，但是偶尔也会选择天气差的时候出去，而天气的标准又有如下4个属性： 温度 起风 下雨 湿度 为了简便起见，这里的每个属性均二值化为0和1，例如温度高用1表示，温度低用0表示，有如下具体记录表格： 温度 起风 下雨 湿度 出去玩 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 根据这个表格，我们来尝试构造一颗决策树，例如将是否起风作为根节点，能得出以下半成品树： 根据表格的数据，在得知是否会起风的情况下，再根据剩余的属性来判断小明是否会出去玩，比如说起风了之后，得到左子树的数据表格，明显可以看到，高温的时候就会出去玩，而低温这不会出去玩。像这样，确定了一个属性之后不断往下构造决策树，最终得到完整决策树如下： 就这样，我们一步一步就构建了一个决策树出来了，从图上看起来，就是不断地if … else …作出决定嘛，好像也没什么嘛。事实上真的是这样的吗？为什么要吧 起风 放在根节点呢？为什么不能放 温度 呢？ 我们如何去判断把哪个属性作为根节点往下分裂呢？ 举例的这棵决策树的构建过程，其实就是广为应用的ID3算法的算法流程。基本的ID3算法是通过自顶向下构造决策树进行学习的。首先考虑的问题是哪一个属性将在树的根节点测试。为解决这一问题，使用统计测试来确定每一个实例属性单独分类训练样本的能力。将分类能力最好的属性作为树的跟节点，之后根节点属性的每一个可能值会产生一个分支，然后把训练样例排列到适当的分支下，重复整个过程，用每个分支结点关联的训练样本来选择最佳属性。这是对合格决策树的贪婪搜索，也就是说算法从不回溯重新考虑以前的选择。 至于怎么去判断哪个属性的分类能力最好，这就要提到了决策树的基础理论——信息熵 信息熵与决策树的构造信息熵可以说是决策树算法里面的理论基础了，通过计算各个属性的信息熵，求出信息增益才能最好地去确定接下来用哪个属性去做分裂，有效的减小了树的深度，最快地降低不确定性，得出预测分类。可以说理解了信息熵，基本上就能理解决策树了，接下来我们来看看信息熵的定义以及如何计算信息熵。 信息熵的含义定义： 当一件事情有多种可能情况时，这件事情对某人而言具体是哪种情况的不确定性叫做熵，而能够消除该人对这件事情不确定性的事物叫做信息 为了区分开热力学上的熵，所以才会有信息熵这个概念，准确来说应该叫做信息的熵，是一个衡量一个事件的状态的单位，跟千克，米类似的，他是一个实实在在的客观物理量，而不是虚无缥缈的抽象概念。获取信息等于消除熵，这是什么意思呢，就拿上面的例子来说，得知外面的天气是“起风了”，那对于判断小明会不会出去玩就更有“把握”了，这里获知的“起风了”，就是获取了“信息“，更有”把握“则表明进一步确定小明会不会出去玩，就是消除了对”小明会不会出去玩“这件事的”不确定性“，也就是消除了熵。 是不是稍微有一点能理解了呢。一个事件或者属性中，其信息熵越大，所包含的不确定信息越大，对数据分析的计算就越有价值，因为得到了其中包含的信息，就消除了事件的不确定性，因此决策树总是优先选择拥有最高信息熵的属性作为待测属性，并以此分裂子树。 信息熵的计算公式假设事件一共有n种属性，其各自对应的概率为：$P_1,P_2,…,P_n$ ，且各属性之间出现时彼此互相独立无相关性。此时可以将信息熵定义为单个属性的对数的平均值。即：$$E(P) = E(-log P_i) = -\sum_{i=1}^{n} P_i logP_i$$更详细的推导过程可以看下我下面推荐的第二个视频，包括那个负号是哪来的。。。这里我们结合上面表格的数据举一个例子，来套套公式，看下信息熵的计算流程。 以 出去玩 为例，首先根据公式计算出去玩的概率，从表格可知，一共6个样本数据，0出现了2次，1出现了4次，分别求两者的概率：$P_1 = \frac{4}{2 + 4} = \frac{4}{6}$$P_2 = \frac{2}{2 + 4} = \frac{2}{6}$$E(o) = -\sum_{i=1}^{n} P_i logP_i = -(\frac{4}{6} log_2 \frac{4}{6}) -(\frac{2}{6} log_2 \frac{2}{6}) ≈ 0.918$即出去玩（out）的信息熵为0.918。同样用公式套进去，可得：温度的信息熵为：$E(t) ≈ 0.809 $起风的信息熵为：$E(w) ≈ 0.459 $下雨的信息熵为：$E(r) ≈ 0.602 $湿度的信息熵为：$E(h) ≈ 0.874 $ 利用信息熵构造决策树通过上面的公式，我们求出了各个属性的信息熵，那决策树是怎么去利用这些熵的呢？这里我就简单介绍其中的一种经典决策树构造算法——ID3算法。 ID3算法是一种贪心算法，以信息熵的下降速度作为选取测试属性的标准，即在每个节点选取还尚未被用来划分的，具有最高信息增益的属性作为划分标准，然后递归这个过程，直到生成的决策树能完美分类训练样例。可以说，ID3算法的核心其实就是信息增益的计算。 所谓的信息增益(Information Gain)，其实指的就是一个事件中，事件发生前后的信息量之间的差值，即：熵A - 属性熵B，也就是说，一开始是Ａ，属性划分之后变成了Ｂ，则属性引起的信息量变化是A - B，即信息增益（它描述的是变化Delta）。用公式来表示即是：$$Gain(P_1,P_2) = E(P_1) - E(P_2)$$好的属性就是信息增益越大越好，即变化完后熵越小越好（熵代表混乱程度，最大程度地减小了混乱）。因此我们在树分叉的时候，应优先使用信息增益最大的属性，这样降低了复杂度，也简化了后边的逻辑。例如我们在上面的那个例子里面，我们把是否出去玩的信息熵作为最后的数值，而每个不同的划分属性与其相减则可获得各个属性的信息增益：$Gain(o,t) = 0.918 - 0.809 = 0.109​$$Gain(o,w) = 0.918 - 0.459 = 0.459​$$Gain(o,r) = 0.918 - 0.602 = 0.316​$$Gain(o,h) = 0.918 - 0.874 = 0.044​$ 从上面就可以看出，信息增益最大的是”起风“这个属性，所以它首先被选中作为决策树的跟节点，下往下划分子树，以此类推得出最终完整的决策树。 就这样通过信息熵计算出每个属性的信息增益，以此来决定决策树划分属性，不断循环这个过程，最终得到一棵”最优“的决策树。 关于信息论需要进一步了解的东西那些不能够消除某人对某件事情不确定性的事物被称为数据或噪音。 噪音是干扰某人获得信息的事物。 而数据是噪音与信息的混合，需要用知识将其分离。 这里给出两个视频，up主讲得非常清楚，对于像我这种从未接触过信息论的人来说，简直是醍醐灌顶，所以在这里也分享给大家。什么是信息，什么是信息熵？： 如何计算信息熵 相关术语与名词节点杂质（node impurity）节点杂质是节点处标签的均匀度的量度。一开始不太理解，国内基本都是直接翻译过来并没有很好的解释什么叫节点杂质。然后看到一个国外前辈给出的一个例子，直接搬来： In simple terms, let’s say you are trying to predict whether you will go out or not based on weather parameters. If it is raining you will definitely not go. So all observations at this point are ‘No’ i.e. pure node While if is is not raining you will check weather temperature is below 20 C then “yes” else “no”. This node is impure node. You can measure this impurity based on metric of your choice. Thus deciding how you split the variables (concept from decision trees https://www.youtube.com/watch?v=Zze7SKuz9QQ) You can choose your impurity measure based on requirement as Peter has suggested. 就是说这个节点不纯度，其实是衡量这个节点能够直接给出结论，能给出多少程度的结论，这个多少程度就是节点不纯度，它应该是一个可以衡量的单位，例如基尼不纯度，信息熵，方差之类的。 再直观点去看，看我上面给出的那颗树，温度高低这个节点能直接决定出不出去玩，那么这个节点就是“纯洁的节点”，可以说节点不纯度为0%，因为它给出的“信息”足以帮我们确定一个事实。而是否下雨这个节点不能完全帮我们下定论，如果不下雨的话还要看湿度这个节点，那么可以说下雨这个节点是“有杂质的节点”，节点不纯度为50%（当然具体计算肯定不是这样，下面有公式）。 以上皆属于个人理解，如有偏差欢迎留言指正。 基尼不纯度（Gini Impurity）基尼不纯度：将来自集合的某种结果随机应用于某一数据项的预期误差率。大概意思是 一个随机事件变成它的对立事件的概率。$$I_G(f)=\sum_{i=1}^mf_i(1-f_i)=\sum_{i=1}^mf_i-\sum_{i=1}^mf_i^2=1-\sum_{i=1}^mf_i^2​$$ 显然基尼不纯度越小，纯度越高，集合的有序程度越高，分类的效果越好； 基尼不纯度为 0 时，表示集合类别一致； 基尼不纯度最高（纯度最低）时,$f_1=f_2=\ldots =f_m=\frac1m$,$I_G(f)=1-(\frac1m)^2\times m=1-\frac1m$ 例，如果集合中的每个数据项都属于同一分类，此时误差率为 0。如果有四种可能的结果均匀地分布在集合中，此时的误差率为 1−0.25=0.75； 过拟合（Over-fitting）在统计学中，过拟合现象是指在拟合一个统计模型时，使用过多参数。 对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。一个出名的谈及过拟合的案例，就是某个科学家说，只要给我4个点，我就能画出一头大象，要是再加1个点，鼻子还能甩。。。。。。 在决策树算法中，可能会过于针对训练数据，其熵值与真实情况相比可能会有所降低。剪枝的过程就是对具有相同父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否会小于某个指定的阈值。如果确实如此，则这些叶节点会被合并成一个单一的节点，合并后的新节点包含了所有可能的结果值。这种做法有助于过度避免过度拟合的情况，使得决策树做出的预测结果，不至于比从数据集中得到的实际结论还要特殊 后记信息论不止作用在决策树中，还有很多分类算法都能用到这个思想，在机器学习领域中举足轻重。在这次的学习过程中，不断的查询资料和学习，让我对数学有了重新的认识，感觉还是学到了不少东西的。 另外信息熵并不是决策树唯一的划分标准，Spark MLlib 中提供的API还可以选”基尼不纯度“来作为选择属性的依据，只不过思想都是大同小异的。构造算法也不是只有ID3一种，只是扩展讲下去怕是越讲越多。。。 该篇算法介绍暂时不配代码来做了，代码留到下一期随机森林里面做，因为随机森林也就是多颗决策树组合起来的而已嘛。]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记4：将自己手写的图片处理成MNIST格式]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B04%EF%BC%9A%E5%B0%86%E8%87%AA%E5%B7%B1%E6%89%8B%E5%86%99%E7%9A%84%E5%9B%BE%E7%89%87%E5%A4%84%E7%90%86%E6%88%90MNIST%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言今天要讲的呢，是自己制作手写图片，并处理成MNIST的标准格式，输入到我们训练好的模型来做识别，看看效果怎样，一直用别人提供的东西，调调api什么的总感觉参与感少了点哈哈。 本来这一部分我是打算给加到MNIST数据集介绍和朴素贝叶斯那两期后面作为扩展阅读的，因为觉得应该是蛮简单的一件事，没想到实际做下来又花了我2天的时间。遇到了不少难点和细节处理，所以最后决定独立一篇文章算了。 今天这篇呢，会涉及到图像处理的一些相关概念，比如RGB转灰度，还有图像在计算机里面的存储方式；也会涉及到一些计算机二进制的处理，蛮多知识盲点的，网上资料也不齐全，所以接下来会整理一下，结合自己的理解把这个学习过程记录下来。特别注意的是，网上相关代码基本都是python的，都是调用封装好的工具类，我这会用scala来全部手动实现一遍，希望下次跟我一样的小菜鸟遇到这些问题的时候不会像我那样束手无策了哈哈哈哈哈哈哈哈 制作手写图片这里为了省事，我就直接用ps新建一张28 x 28像素的图片来写数字了，不过一开始做出来的图片我拿去识别效果并不好，所以拿了训练数据输出灰度矩阵，再对比自己手写的数字输出的灰度矩阵，发现识别不准的原因主要出现在以下两个地方： 手写数字要尽可能居中，且不要占满整个图片，边缘要留出部分空白 画笔像素过大，边缘像素有所偏差。 对于上述问题，经过我多次尝试后，画笔参数调成柔边，2像素效果较好，创建过程如图： 画笔的选择： 图片的灰度处理手写出来的图片看着是黑白的，其实是RGB真彩图片，要变成我们MNIST的标准格式，还要先把24位深的图片处理成8位深的灰度图。 几种灰度化方法 分量法：使用RGB三个分量中的一个作为灰度图的灰度值。 最值法：使用RGB三个分量中最大值或最小值作为灰度图的灰度值。 均值法：使用RGB三个分量的平均值作为灰度图的灰度值。 加权法：由于人眼颜色敏感度不同，按下一定的权值对RGB三分量进行加权平均能得到较合理的灰度图像。一般情况按照：Y = 0.30R + 0.59G + 0.11B。 从网上看到的资料来说哈，据说是加权法出来的图片效果最好。实际上java有自己的灰度化办法： 123456789101112131415161718public void grayImage() throws IOException&#123; File file = new File(System.getProperty("user.dir")+"/test.jpg"); BufferedImage image = ImageIO.read(file); int width = image.getWidth(); int height = image.getHeight(); BufferedImage grayImage = new BufferedImage(width, height, BufferedImage.TYPE_BYTE_GRAY); for(int i= 0 ; i &lt; width ; i++)&#123; for(int j = 0 ; j &lt; height; j++)&#123; int rgb = image.getRGB(i, j); grayImage.setRGB(i, j, rgb); &#125; &#125; File newFile = new File(System.getProperty("user.dir")+"/method1.jpg"); ImageIO.write(grayImage, "jpg", newFile); &#125; 看代码就是创建一个灰度图对象，然后把原图的RGB像素点强塞进去，输出成灰度图。出来的效果相当简陋，对于人来说看起来颜色比加权法出来的要糟糕不少，下面给出对比图片： 原图: Java自带的处理出来的灰度图： 自己实现的加权法出来的灰度图： 很明显的，加权法出来的图片肉眼看起来更清晰，因此最后我选择用scala来实现图片的灰度化 scala中实现图片灰度化说是scala实现吧，其实还是调了java的包哈哈哈，完整代码我放到了github上，顺手做了个批量读取文件夹里图片拼装成MNIST数据格式，所以分了3个class，于下列代码有点不一样，所以想要看如何实现还是去看完整代码吧：完整代码这里一步步来，首先是读取图片文件： 12val file = new File(path)val img = mageIO.read(file) 把图片灰度化：1234567891011121314151617181920212223/* 把图片转成灰度值图片 * 原理其实还蛮简单的，就是按比例来取RGB三色，例如按比例3:6:1，就是R*0.3 + G*0.59 + B*0.11得出灰度值 */def getGrayImg(img:BufferedImage): BufferedImage = &#123; //宽度（行） val width = img.getWidth //高度（列） val height = img.getHeight //创建一个灰度图对象 val grayImg = new BufferedImage(width,height,img.getType) //按比例计算灰度值 for(i &lt;- 0 until width ; j &lt;- 0 until height)&#123; val color = img.getRGB(i,j) val r = (color &gt;&gt; 16) &amp; 0xff val g = (color &gt;&gt; 8) &amp; 0xff val b = color &amp; 0xff val gray =255 - (0.3 * r + 0.59 * g + 0.11 * b).toInt val newPixel = colorToRGB(256, gray, gray, gray) grayImg.setRGB(i,j,newPixel) &#125; grayImg&#125; 到这一步就已经得到了一个灰度化图片了，接下来我们只需要按照MNIST格式 把灰度图拼装成MNIST格式按照MNIST官网的格式，把一些数据信息转成byte数组拼接起来，再把灰度图的像素值以8位每个点（1byte）拼接起来就可以了，这里我直接给出代码，为了方便我还写了个批量读取文件夹内的图片，把这些图片全部转成byte数组再一一进行拼接。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182package SparkMLlib.Baseimport java.io.&#123;File, FileInputStream, FileOutputStream&#125;import java.nio.ByteBufferimport SparkMLlib.Base.FileUtil.usingimport SparkMLlib.Base.ImageUtil.&#123;getGrayArray, getGrayImg&#125;import javax.imageio.ImageIOimport scala.collection.mutable.ArrayBuffer/** * @author voidChen * @date 2019/2/27 14:47 */object MNIST_Util &#123; /** * 生成灰度图和MNIST格式文件 * @param args */ def main(args: Array[String]): Unit = &#123; val path = "C:\\Users\\42532\\Desktop\\own\\" val files = FileUtil.getFileList(path,".jpg","灰") //处理成（标签，灰度数组） val datas:Array[(Int, Array[Byte])] = files.map(f =&gt; (f.getName.substring(0,1).toInt,getGrayArray(ImageIO.read(f)))) //生成MNIST文件 imgToMNIST(datas) //输出灰度图像 files.foreach(f =&gt; &#123; val grayImg = getGrayImg(ImageIO.read(f)) val newFile = new File(path+"灰"+f.getName); ImageIO.write(grayImg, "jpg", newFile); &#125;) println("图像输出完成") &#125; /** * 读取标签 * @param labelPath * @return */ def loadLabel(labelPath: String): Array[Byte] =&#123; val file = new File(labelPath) val in = new FileInputStream(file) var labelDS = new Array[Byte](file.length.toInt) using(new FileInputStream(file)) &#123; source =&gt; &#123; in.read(labelDS) &#125; &#125; //32 bit integer 0x00000801(2049) magic number (MSB first--high endian) val magicLabelNum = ByteBuffer.wrap(labelDS.take(4)).getInt println(s"magicLabelNum=$magicLabelNum") //32 bit integer 60000 number of items val numOfLabelItems = ByteBuffer.wrap(labelDS.slice(4, 8)).getInt println(s"numOfLabelItems=$numOfLabelItems") //删掉前面的文件描述 labelDS = labelDS.drop(8) //打印测试数据 for ((e, index) &lt;- labelDS.take(3).zipWithIndex) &#123; println(s"image$index is $e") &#125; labelDS &#125; /** * 读取图像文件 * @param imagesPath * @return */ def loadImages(imagesPath: String): Array[Array[Byte]] =&#123; val file = new File(imagesPath) val in = new FileInputStream(file) var trainingDS = new Array[Byte](file.length.toInt) using(new FileInputStream(file)) &#123; source =&gt; &#123; in.read(trainingDS) &#125; &#125; //32 bit integer 0x00000803(2051) magic number val magicNum = ByteBuffer.wrap(trainingDS.take(4)).getInt println(s"magicNum=$magicNum") //32 bit integer 60000 number of items val numOfItems = ByteBuffer.wrap(trainingDS.slice(4, 8)).getInt println(s"numOfItems=$numOfItems") //32 bit integer 28 number of rows val numOfRows = ByteBuffer.wrap(trainingDS.slice(8, 12)).getInt println(s"numOfRows=$numOfRows") //32 bit integer 28 number of columns val numOfCols = ByteBuffer.wrap(trainingDS.slice(12, 16)).getInt println(s"numOfCols=$numOfCols") trainingDS = trainingDS.drop(16) val itemsBuffer = new ArrayBuffer[Array[Byte]] for(i &lt;- 0 until numOfItems)&#123; itemsBuffer += trainingDS.slice( i * numOfCols * numOfRows , (i+1) * numOfCols * numOfRows) &#125; itemsBuffer.toArray &#125; /** * 转换成MNIST数据 * @param grayArray * @return */ def imgToMNIST(data: Array[(Int, Array[Byte])])&#123; //组装MNIST格式的图像文件头 var images = ArrayBuffer[Byte]() images ++= intToByteArray(2051) //magicNum images ++= intToByteArray(data.length) //number of items images ++= intToByteArray(28) //number of rows images ++= intToByteArray(28) //number of columns //组装MNIST格式的标签文件头 var labels = ArrayBuffer[Byte]() labels ++= intToByteArray(2049) //magicNum labels ++= intToByteArray(data.length) //number of items //组装数据 data.foreach(d =&gt;&#123; val label = (d._1 &amp; 0xFF).toByte val image = d._2 labels += label images ++= image &#125;) // var i = 0 // images.drop(16).toArray.foreach( // b =&gt;&#123; // print(b+" ") // i = i+1 // if(i == 28)&#123; // i = 0 // println() // &#125;&#125; // ) //输出二进制文件 val imagesFile = new File("C:\\Users\\42532\\Desktop\\test-images.idx3-ubyte"); val labelsFile = new File("C:\\Users\\42532\\Desktop\\test-labels.idx1-ubyte"); val imagesWriter = new FileOutputStream(imagesFile) imagesWriter.write(images.toArray) imagesWriter.close() val labelsWriter = new FileOutputStream(labelsFile) labelsWriter.write(labels.toArray) labelsWriter.close() println("文件写入完成") &#125; /** * int到byte[] * * @param i * @return */ def intToByteArray(i: Int): Array[Byte] = &#123; val result = new Array[Byte](4) //由高位到低位 result(0) = ((i &gt;&gt; 24) &amp; 0xFF).toByte result(1) = ((i &gt;&gt; 16) &amp; 0xFF).toByte result(2) = ((i &gt;&gt; 8) &amp; 0xFF).toByte result(3) = (i &amp; 0xFF).toByte result &#125;&#125; 生成的图片和文件如下：标签起名直接做成图片文件名，如果有多个“1”的图片数据呢，把文件名弄成1-1.jpg,1-2.jpg,······这样就好了 关于图像处理的一些知识扩展图像位深一个像素用多少位表示，这里的位，只是针对每一个像素点而言的。考虑到位深度平均分给R, G, B和Alpha，而只有RGB可以相互组合成颜色。所以4位颜色的图，它的位深度是4，只有2的4次幂种颜色，即16种颜色或16种灰度等级 ) 。8位颜色的图，位深度就是8，用2的8次幂表示，它含有256种颜色 ( 或256种灰度等级 )。24位颜色可称之为真彩色，位深度是24，它能组合成2的24次幂种颜色，即：16777216种颜色 ( 或称千万种颜色 )，超过了人眼能够分辨的颜色数量。当我们用24位来记录颜色时，实际上是以2^（8×3），即红、绿、蓝 ( RGB ) 三基色各以2的8次幂，256种颜色而存在的，三色组合就形成一千六百万种颜色。 图像在计算机的存储格式一般来说，我们自己做的图片都是24位的RGB真彩图，在上述代码中，img.getRGB(i,j) 这里get出来的只有一个int值，其实他已经把RGB三原色的值全放在里面了，由高位到低位排列下来，分别是R 8位 + G 8位 + B 8位，所以我们通过右移操作符 &gt;&gt; 每次移动8位，在&amp; 0xFF 一下，就能取出RGB分别的Int 颜色值来了。而最高的32位深的图片，只是在RGB的基础三多加了一个alpha透明度通道。对于32位的像素值，里面的排列顺序由高位到低位排列下来，分别是alpha 8位 + R 8位 + G 8位 + B 8位，同样可以按位把真实值取出来。 关于 &amp; 0xFF 操作回想一下大学学的计算机导论，有一条很重要但是不被当时我所重视的知识：计算机中整数以补码的形式存储。正数的补码与原码一致，这没什么问题，当时负数的补码却是原码的反码+1，这也导致了为什么我们没做&amp; 0xFF 操作的时候直接byte转int会由出现负数的原因之一（另一个原因是java里面int都是带了符号位，表示的数字范围在 -128 ~ 127，之间，而实际像素值是 0 ~ 255的正整数）。 为什么byte类型的数字要 &amp;0xff 再赋值给int类型呢，其本质原因就是想保持二进制补码的一致性。例如二进制的的129为10000001 ，8位byte就能存下这个数字，当时若是直接转int，出来的结果却是-127，其原因就在于，因为当系统检测到byte可能会转化成int或者说byte与int类型进行运算的时候，就会将byte的内存空间高位补1（也就是按符号位补位）扩充到32位,变成1111111111111111111111111 10000001，再参与运算，这样其二进制补码其实已经不一致了。而进行与操作 &amp;0xff 则是这样的： 1111111111111111111111111 10000001 &amp; 11111111 = 000000000000000000000000 10000001​显然，这就是为什么我们在byte转成int的时候，要做一下 &amp;0xff 操作了 参考链接 《Java实现图像灰度化》 《byte为什么要与上0xff？》 《计算机中整数为什么以【补码】的形式存储》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>MachineLeaning</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记3：使用朴素贝叶斯识别手写数字图片]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B03%EF%BC%9A%E4%BD%BF%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[前言上一篇讲完了MNIST手写数字数据集的读取使用，今天呢就来讲讲使用分类算法——朴素贝叶斯算法(Naive Bayesian classification)来对这个数据集进行训练，看看使用这个算法训练出来的模型，对手写数字的识别率有多高。本人也是初学者，所以对于算法的讲述只是基于自身粗浅的理解，如有不同意见欢迎指正。话不多说，我们来进入正题。 朴素贝叶斯算法原理不得不说，这个名字起得太贴切了，在我了解完这个算法之后，才知道他真的很“朴素”。。。 关于分类问题先来感性认识一下这个算法，朴素贝叶斯算法是一个分类算法，对于分类，其实我们并不陌生，在生活中我们也常常会下意思地对事物进行“分类”： 例如走在街上，看到一个路人，我们会下意思地去判断是男是女；别人请你吃水果，看一眼就分辨出是苹果还是橘子；在奶茶店买一杯茶，看茶汤颜色会判断是红茶还是绿茶或者是乌龙茶。这里例子都说明了一件事，我们生活中无时无刻都在对事物进行着分类。 但是分类的前提，是我们得对这个事物有基本的认知，比如，舍友给了我一个牛油果，但是我没吃过牛油果，所以我不能准确地把它分类到“牛油果”这个类别去，虽然我不能明确他的分类，但是我能确定它不是“衣服”，因为它的特征跟衣服完全不一致，根据已知的事物中，它的特征跟“水果”最像，所以它是水果的概率最高，所以我们会把他分类到“水果”去。 看到是不是觉得有点“朴素”。。。对的，朴素贝叶斯算法，听起来好像很厉害，实际上就是这么一个“朴素”的思想，在已知的条件下，选出概率最高的一个类别作为该项的分类，“朴素”到我这种数学渣渣都露出了会心的微笑2333333 朴素贝叶斯分类的数学原理与流程朴素贝叶斯分类的正式定义如下： 设$x=\{a_1,a_2,\ldots,a_m\}$为一个待分类项，而每个$a$为$x$的一个特征属性。 有类别集合$C=\{y_1,y_2,\ldots,y_n\}$。 计算$P(y_1|x),P(y_2|x),\ldots,P(y_n|x)$。 如果$P(y_k|x)=max\{P(y_1|x),P(y_2|x),\ldots,P(y_n|x)\}$，则$x \in y_k$。 那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做： 找到一个已知分类的待分类项集合，这个集合叫做训练样本集。 统计得到在各类别下各个特征属性的条件概率估计,即：$P(a_1|y_1),P(a_2|y_1),\ldots,P(a_m|y_1);$$P(a_1|y_2),P(a_2|y_2),\ldots,P(a_m|y_2);$ $\vdots$$P(a_1|y_n),P(a_2|y_n),\ldots,P(a_m|y_n);$ 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：$$P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)} $$ 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：$$P(x|y_i)P(y_i)=P(a_1|y_i)P(a_2|y_i)\ldots P(a_m|y_i)=P(y_i)\prod_{j=1}^{n} P(a_j|y_i)$$ 对于朴素贝叶斯在机器学习上解决分类问题，主要是分为三个阶段：第一阶段——准备工作阶段，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。 第二阶段——分类器训练阶段，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。 第三阶段——应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。 以上引用自： 《算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)》 朴素贝叶斯的应用场景和局限优点： 算法简单容易理解，对分类结果的可解释性较强 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练。 缺点： 需要明确有哪些分类，如果给定没有出现过的类和特征，则该类别的条件概率估计将出现0，该问题被称为“零条件概率问题”。 特征之间独立的假设非常强。 在现实生活中几乎很难找到这样的数据集。在属性个数比较多或者属性之间相关性较大时，朴素贝叶斯分类模型的分类效率低。而在属性相关性较小时，朴素贝叶斯分类模型的性能最为良好。 应用场景： 欺诈检测中使用较多 垃圾邮件过滤 文本分类 人脸识别 朴素贝叶斯在Spark MLlib中的使用Spark MLlib中提供了朴素贝叶斯算法工具，我们调用的时候只需要把数据处理成spark的标准输入格式即可轻松得到训练模型。下面我将使用MNIST手写数字数据集来做训练，并用其提供的测试集计算准确率。（ps:后续会加入对自己手写数字图片进行识别，请等下一次更新_(:з」∠)_） 1、首先把标签数据和图片数据处理成 LabeledPoint 标签向量数据格式：1234val data = trainLabe.zip(trainImages).map( d =&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble))))val trainRdd = sc.makeRDD(data) 2、调用MLlib的api，输入数据进行训练，得到分类模型：1val model = NaiveBayes.train(trainRdd) 3、利用训练好的模型，对测试数据进行检验，看看识别准确率有多少：1234567891011val testData = testImages.map(d =&gt; Vectors.dense(d.map(p =&gt; (p &amp; 0xFF).toDouble )))val testRDD = sc.makeRDD(testData)val res = model.predict(testRDD).map(l =&gt; l.toInt).collect()val tr = res.zip(testLabe)val sum = tr.map( f =&gt;&#123; if(f._1 == f._2.toInt) 1 else 0&#125;).sumprintln(&quot;准确率为：&quot;+ sum.toDouble /tr.length) 如图，利用它提供的测试集呢，我这里得到的识别率约为0.8365，看样子识别效率并不怎么样啊，因为据我所知这份数据最高的识别率能高达99%甚至100%的，下一期我将会尝试使用随机森林算法来对这个数据集进行训练，稍微期待一下吧~ 此处只贴了部分代码，完整代码可前往github查看： 完整代码点我 参考链接 《算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)》 《机器学习|朴素贝叶斯（Naive Bayes）算法总结》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记2：MNIST手写数字的读取使用]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B02%EF%BC%9AMNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%9A%84%E8%AF%BB%E5%8F%96%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言近来广州阴雨连绵，实在是让人提不起精神来，文档什么的也看不进去，脑袋昏昏沉沉的，所以拖拖拉拉到今天才出更新=。= 重申一遍，我是沿着spark官网中MLlib的api往下去学习的，提到的相关算法都会去学习下。文档上有的东西呢，我就不赘述了，只是把自己学习过程中一些重要的，别的博客没有提到的，记录一下，以后自己复习也方便。在这篇东西之前，我已经自己看完了spark MLlib的数据类型和基本统计，没了解的可以先去看一下文档，后续会有用到。 好了，言归正传，今天要讲的是被称为机器学习界的“Hello world”，几乎每个入门系列都会用到的一个知名数据集——MNIST手写数字图像数据集，该系列的后续文章中也经常会用到，所以今天独立出来讲一下好了。 MNIST手写数字数据集首先给出官网：THE MNIST DATABASE of handwritten digits MNIST是一个经典的手写数字数据集，来自美国国家标准与技术研究所，由不同人手写的0至9的数字构成，由60000个训练样本集和10000个测试样本集构成，每个样本的尺寸为28x28，以二进制格式存储，如下图所示：这里有个比较坑的细节，就是数据采集都是外国人，他们的手写数字风格跟我们亚洲人的略有不同，这也导致识别我们自己手写的数字时候准确率下降了不少=。= 下载下来之后是4个gz压缩包： train-images-idx3-ubyte.gz: training set images (9912422 bytes) train-labels-idx1-ubyte.gz: training set labels (28881 bytes) t10k-images-idx3-ubyte.gz: test set images (1648877 bytes) t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes) 解压之后得到的是一个二进制文件，文件的格式官网有给出，这里我顺便贴一下： TEST SET LABEL FILE (train-labels-idx1-ubyte): [offset] [type] [value] [description] 0000 32 bit integer 0x00000801(2049) magic number (MSB first) 0004 32 bit integer 60000 number of items 0008 unsigned byte ?? label 0009 unsigned byte ?? label …….. unsigned byte ?? label The labels values are 0 to 9.TEST SET IMAGE FILE (train-images-idx3-ubyte): [offset] [type] [value] [description] 0000 32 bit integer 0x00000803(2051) magic number 0004 32 bit integer 60000 number of images 0008 32 bit integer 28 number of rows 0012 32 bit integer 28 number of columns 0016 unsigned byte ?? pixel 0017 unsigned byte ?? pixel …….. unsigned byte ?? pixel Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black). 训练集和测试集的结构都是一样的，这里我就只贴出训练集的结构，标签是0到9的数字，图像则是0到255的像素值。 其实一开始看到二进制文件我也是有点懵，因为以前工作也没接触过。不过看多两眼就明白了。以images数据集来说吧，这个train-images-idx3-ubyte文件： 第0000字节到第0003字节，是一个32位int类型数字，值为2051的魔数。 第0004字节到第0007字节也是一个32位int类型数字，转换过来值为60000，表示一共有60000个训练样本。 第0008字节到第00011字节和第0012字节到第0015字节同理，代表着有28行和28列。 这里的行和列，其实就是一张图片的像素矩阵，也就是每个图片都有28×28=784个像素。所以说，从第16个字节开始，就是图片的每一个像素点的值了。 说到这里可能会有些朋友担心，会不会每一张图片都是这么排下来呢？为此我去算了下这个二进制文件的长度，刚好就是16+28*28*60000,所以文件描述信息呢就之在文件开头出现了，后面的就是每张图片的像素连在一起了，图片的规格统一被处理成28*28的图片了。 因此，读取图片信息有了如下代码：123456789101112131415161718192021222324252627282930313233343536373839/** * 读取图像文件 * @param imagesPath * @return */def loadImages(imagesPath: String): Array[Array[Byte]] =&#123; val file = new File(imagesPath) val in = new FileInputStream(file) var trainingDS = new Array[Byte](file.length.toInt) using(new FileInputStream(file)) &#123; source =&gt; &#123; in.read(trainingDS) &#125; &#125; //32 bit integer 0x00000803(2051) magic number val magicNum = ByteBuffer.wrap(trainingDS.take(4)).getInt println(s&quot;magicNum=$magicNum&quot;) //32 bit integer 60000 number of items val numOfItems = ByteBuffer.wrap(trainingDS.slice(4, 8)).getInt println(s&quot;numOfItems=$numOfItems&quot;) //32 bit integer 28 number of rows val numOfRows = ByteBuffer.wrap(trainingDS.slice(8, 12)).getInt println(s&quot;numOfRows=$numOfRows&quot;) //32 bit integer 28 number of columns val numOfCols = ByteBuffer.wrap(trainingDS.slice(12, 16)).getInt println(s&quot;numOfCols=$numOfCols&quot;) trainingDS = trainingDS.drop(16) val itemsBuffer = new ArrayBuffer[Array[Byte]] for(i &lt;- 0 until numOfItems)&#123; itemsBuffer += trainingDS.slice( i * numOfCols * numOfRows , (i+1) * numOfCols * numOfRows) &#125; itemsBuffer.toArray&#125; 代码里面提取了文件描述信息，并返回有每一张图片像素数据组成的数组。完整代码我放在github上了： 完整代码 读取标签数据也是同理，在下一篇文章中，我将会详细讲述如何用标签和图片数据去做训练，敬请期待=。= 其实数据集到这里已经是完成读取了，后续只要转换成spark MLlib的数据格式就能用了，但是！我在把像素值由二进制byte类型转换成double类型的时候，居然出现了负数。这是怎么回事呢，看官网说的，值应该是在0到255之间才对啊。再仔细看了一遍文件架构，发现标明的像素类型为unsigned byte，即无符号byte类型，在我学习java的时候，隐约记得有学过说java类型都是有符号的，第一位即是正负号。于是去搜索了一下，找到了这篇文章：《Java中对于unsigned byte类型的转换处理》 把二进制像素值转换为Double类型数字由于Java中没有unsigned byte类型，所以一个字节只能表示（-128，127），而想要表示（0，255），也很简单，只要跟0xFF取&amp;操作即可。 原理其实就是正数的反码和补码都是其本身，负数的反码是对原码除了符号位之外作取反运算，补码则为反码+1。 更多细节可以参考：《byte为什么要与上0xff？》、《Java中对于unsigned byte类型的转换处理》下面贴出我处理逻辑的部分代码：123val data = trainLabe.zip(trainImages).map( d =&gt; LabeledPoint(d._1.toInt, Vectors.dense(d._2.map(p =&gt; (p &amp; 0xFF).toDouble)))) 下期预告下一期我将会记录我用spark MLlib中提供的朴树贝叶斯算法，对该数据集进行训练，并用测试集测试。关于该数据集呢，其实还有一点要补充的，比如把自己的手写图片转换成MNIST标准格式，还有替代MNIST手写数字集的图像数据集Fashion MNIST，这些后续都会补充进该文章。持续更新ing… 参考链接 《THE MNIST DATABASE of handwritten digits》 《（超详细）读取mnist数据集并保存成图片》 《Java中对于unsigned byte类型的转换处理》 《byte为什么要与上0xff？》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>MachineLeaning</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小技巧！从网易云音乐缓存中获取版权失效的音乐]]></title>
    <url>%2F%E5%B0%8F%E6%8A%80%E5%B7%A7%EF%BC%81%E4%BB%8E%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E7%BC%93%E5%AD%98%E4%B8%AD%E8%8E%B7%E5%8F%96%E7%89%88%E6%9D%83%E5%A4%B1%E6%95%88%E7%9A%84%E9%9F%B3%E4%B9%90%2F</url>
    <content type="text"><![CDATA[起因前两天听了赵方婧小姐姐翻唱的《知否》，惊为天人，就点了颗心收藏了。结果今天在点开一看。居然没有版权了！！！这也不是一次两次了，充了两个会员跟没充一样，说下架就下架，喜欢的歌听不到，这怎么能忍！ 把爱曲抢救回来！想到这两天听过，电脑一定有缓存的，所以去设置里面看了下缓存目录：嗯。。。果然改了名字，看文件大小就知道这个.uc后缀的就是缓存的MP3文件了，文件名应该就是它id 那这个id是什么呢？客户端看不到，我们把这首歌复制到浏览器去看看： 还是挺显眼的，参数名就叫id了哈哈哈哈 拿到这个id，去文件夹搜索一下，就能找到我们想要的歌的缓存啦 然后按照原本的想法，后缀改成MP3之后，应该直接就能听了，然而。。。右键网易云音乐打开，提示解码失败！！！看来网易云也防了一手啊，大概是做了什么加密处理。 对缓存文件反向解密考虑到文件加密后大小跟原来的基本没有变化，大概是进行了与或处理。 上google搜索了一下，很多人都遇到了，有大佬已经试出来了，就是逐字节跟0xa3与或一下就好了。 至于大佬是怎么试出来的，我个人猜测应该是拿到缓存文件，再拿到下载文件，各取第一个字节来试一下，与或虽然不能逆运算，不过应该能拿来猜一下。下面贴出转换代码123456789101112131415161718192021222324252627282930import java.io.*;/** * @author Chenyl * @date 2019/2/14 10:41 */public class music &#123; public static void main(String[] args)&#123; try&#123; File inFile = new File(&quot;C:\\Users\\Void\\Desktop\\知否-赵方婧.mp3&quot;); File outFile = new File(&quot;C:\\Users\\Void\\Desktop\\知否-赵方婧(解).mp3&quot;); DataInputStream dis = new DataInputStream( new FileInputStream(inFile)); DataOutputStream dos = new DataOutputStream( new FileOutputStream(outFile)); byte[] by = new byte[1000]; int len; while((len=dis.read(by))!=-1)&#123; for(int i=0;i&lt;len;i++)&#123; by[i]^=0xa3; &#125; dos.write(by,0,len); &#125; dis.close(); dos.close(); &#125;catch(IOException ioe)&#123; System.err.println(ioe); &#125; &#125;&#125; 解出来的文件直接就可以听啦！不过不知道是不是心理作用，总觉得音质受损了= = 一些感想此方法仅限于自己学习使用，请勿无版权传播音频。其实我也是很不愿意这样做的，各大运营商为了利益把版权弄得四分五裂，听歌都要各个软件来回切换，开四五个会员就算了，还经常突然下架歌曲，甚至有偷偷删除歌单的行为出现。 希望国家早日立法，统一版权，给我们一个稳定付费的环境吧。真的，支持歌手，不差那点钱，现在是想给钱都听不到，我也是醉了。 参考链接 《网易云音乐.uc格式的缓存文件转.mp3》]]></content>
      <categories>
        <category>好玩的东西</category>
      </categories>
      <tags>
        <tag>网易云音乐</tag>
        <tag>小技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-MLlib学习日记1：K-means聚类分析]]></title>
    <url>%2FSpark-MLlib%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B01%EF%BC%9AK-means%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言作为该系列的第一篇文章，也是我正式步入机器学习的第一篇学习日记，想说几句话，机器学习跟普通编程不一样，门槛对于大多数人来说都是算比较高的。曾经我也在惊叹和好奇中望而却步，现在在大佬的推动下也是鼓起勇气正式开始学习，完成这个系列也是我2019的目标。 由于公司用的是Spark，所以从Spark MLlib开始学起，不积硅步无以至千里，在这个系列我也会从简单一点地基础开始，一点点切实地去学习，希望看到这篇文章地小伙伴也能下定决心，一起攻克机器学习。 K-Means 聚类算法原理K-Means算法流程关于原理呢，我直接引用《使用Spark MLlib做K-mean聚类分析》里面的资料好了： 聚类分析是一个无监督学习 (Unsupervised Learning) 过程, 一般是用来对数据对象按照其特征属性进行分组，经常被应用在客户分群，欺诈检测，图像分析等领域。K-means 应该是最有名并且最经常使用的聚类算法了，其原理比较容易理解，并且聚类效果良好，有着广泛的使用。和诸多机器学习算法一样，K-means 算法也是一个迭代式的算法，其主要步骤如下: 第一步，选择 K 个点作为初始聚类中心。 第二步，计算其余所有点到聚类中心的距离，并把每个点划分到离它最近的聚类中心所在的聚类中去。在这里，衡量距离一般有多个函数可以选择，最常用的是欧几里得距离 (Euclidean Distance), 也叫欧式距离,公式如下： $$D(C,X) = \sqrt{\sum_{i=1}^n(c_i - x_i)^2}​$$ 其中 C 代表中心点，X 代表任意一个非中心点。 第三步，重新计算每个聚类中所有点的平均值，并将其作为新的聚类中心点。 最后，重复 (二)，(三) 步的过程，直至聚类中心不再发生改变，或者算法达到预定的迭代次数，又或聚类中心的改变小于预先设定的阀值。 光这么看有点不是很直观，其实就是不断换聚类中心以求达到“最合理”的分类的过程，下面我贴一张数据可视化的图，更直观地描述这个过程：如图，k-means算法在每一次迭代中，选择了更靠谱的红蓝中心点，试图让分类更合理，跟均匀。 欧式距离欧几里得公式的含义还是比较简单的，但是一开始看还是有点懵，在这里我稍微提一下自己的理解，如果不对的话欢迎留言指正=。=首先我们先回顾一下那个公式，那个n代表的是什么呢？$$D(C,X) = \sqrt{\sum_{i=1}^n(c_i - x_i)^2}​$$我们先来看看欧式距离在二维和三维中的展开：是不是觉得很熟悉咧，就是求坐标之间的距离嘛，以此类推，在4维空间5维空间或者更高维度的空间中怎么算两点坐标距离呢？那就是原版的欧式距离公式啦！n在我看来，就是代表所求的维度数嘛，对应到实际应用中，这个“维度”对应的就是我们想要聚类的对象的参数（特征）。当n个参数可以确定一个唯一（或近似）的对象，我们通过欧式距离公式计算这个对象坐标的距离，把最后n维空间坐标相近的对象聚到同一个类别里，就完成了K-Means聚类算法要做的工作了。 聚类测试数据在本文中，我们所用到目标数据集是来自UCI Machine Learning Repository的Wholesale customer Data Set。UCI是一个关于机器学习测试数据的下载中心站点，里面包含了适用于做聚类，分群，回归等各种机器学习问题的数据集。 Wholesale customer Data Set是引用某批发经销商的客户在各种类别产品上的年消费数。为了方便处理，本文把原始的CSV格式转化成了两个文本文件，取前面20行作为测试集，其余的数据作为训练集。如图： 代码样例看注释就好了，基本就是处理好数据，然后调用spark-lib包里面的kmeans方法去做训练就ok了，源码分析大概会在做完这个系列之后再考虑要不要做。。。 值得注意的是：val clusters:KMeansModel = KMeans.train(parsedTrainingData, numClusters, numIterations,runTimes)这行代码，我来介绍一下这几个参数： parsedTrainingData： 处理好的训练数据集 numClusters： 聚类的个数。这个值的选取有点学问，K的选择是K-means算法的关键，Spark MLlib在KMeansModel类里提供了computeCost方法，我的demo中也有该方法调用例子。该方法通过计算所有数据点到其最近的中心点的平方和来评估聚类的效果。一般来说，同样的迭代次数和算法跑的次数，这个值越小代表聚类的效果越好。但是在实际情况下，我们还要考虑到聚类结果的可解释性，不能一味的选择使computeCost结果值最小的那个K numIterations： K-means算法的迭代次数 runTimes： K-means算法run的次数 完整代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * @author Chenyl * @date 2019/1/23 15:57 */import org.apache.spark.&#123;SparkContext, SparkConf&#125;import org.apache.spark.mllib.clustering.&#123;KMeans, KMeansModel&#125;import org.apache.spark.mllib.linalg.Vectorsobject KMeansClustering &#123; def main (args: Array[String]) &#123; val trainingPath = &quot;src/main/resources/Wholesale customers data_training.txt&quot; //训练数据集文件路径 val testPath = &quot;src/main/resources/Wholesale customers data_test.txt&quot; //测试数据集文件路径 val numClusters = 8 //聚类的个数 val numIterations = 30 //K-means 算法的迭代次数 val runTimes = 3 //K-means 算法 run 的次数 val conf = new SparkConf().setAppName(&quot;Spark MLlib Exercise:K-Means Clustering&quot;) conf //TODO: 生成打包前，需注释掉此行 .setMaster(&quot;local[*]&quot;) .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;ERROR&quot;) //设置日志级别，info会有很多运行日志出现，也可以打开看看 /** *Channel Region Fresh Milk Grocery Frozen Detergents_Paper Delicassen * 2 3 12669 9656 7561 214 2674 1338 * 2 3 7057 9810 9568 1762 3293 1776 * 2 3 6353 8808 7684 2405 3516 7844 */ val rawTrainingData = sc.textFile(trainingPath) //加载测试数据 //去表头处理 val parsedTrainingData = rawTrainingData.filter(!isColumnNameLine(_)).map(line =&gt; &#123; Vectors.dense(line.split(&quot;\t&quot;).map(_.trim).filter(!&quot;&quot;.equals(_)).map(_.toDouble)) &#125;).cache()// findK(parsedTrainingData) // Cluster the data into two classes using KMeans var clusterIndex:Int = 0 //使用K-Means训练 val clusters:KMeansModel = KMeans.train(parsedTrainingData, numClusters, numIterations,runTimes) println(&quot;Cluster Number:&quot; + clusters.clusterCenters.length) println(&quot;Cluster Centers Information Overview:&quot;) clusters.clusterCenters.foreach(x =&gt; &#123; println(&quot;Center Point of Cluster &quot; + clusterIndex + &quot;:&quot;) println(x) clusterIndex += 1 &#125;) //begin to check which cluster each test data belongs to based on the clustering result val rawTestData = sc.textFile(testPath) //加载测试数据 val parsedTestData = rawTestData.map(line =&gt;&#123; Vectors.dense(line.split(&quot;\t&quot;).map(_.trim).filter(!&quot;&quot;.equals(_)).map(_.toDouble)) &#125;) parsedTestData.collect().foreach(testDataLine =&gt; &#123; val predictedClusterIndex: Int = clusters.predict(testDataLine) //使用训练好的模型进行分类 println(&quot;The data &quot; + testDataLine.toString + &quot; belongs to cluster &quot; +predictedClusterIndex) &#125;) println(&quot;Spark MLlib K-means clustering test finished.&quot;) &#125; private def isColumnNameLine(line:String):Boolean = &#123; if (line != null &amp;&amp; line.contains(&quot;Channel&quot;)) true else false &#125; /** * 查看最佳的K值 * @param parsedTrainingData */ private def findK(parsedTrainingData:org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]): Unit =&#123; val ks:Array[Int] = Array(3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20) ks.foreach(cluster =&gt; &#123; val model:KMeansModel = KMeans.train(parsedTrainingData, cluster,30,1) val ssd = model.computeCost(parsedTrainingData) println(&quot;sum of squared distances of points to their nearest center when k=&quot; + cluster + &quot; -&gt; &quot;+ ssd) &#125;) &#125;&#125; K-Means应用场景适用场景常用的场景是在不清楚用户有几类时，尝试性的将用户进行分类，并根据每类用户的不同特征，决定下步动作。一个典型的应用场景是CRM管理中的数据库营销。举例，对于一个超市/电商网站/综合零售商，可以根据用户的购买行为，将其分为“年轻白领”、“一家三口”、“家有一老”、”初得子女“等等类型，然后通过邮件、短信、推送通知等，向其发起不同的优惠活动。 明尼苏达州一家塔吉特门店被客户投诉，一位中年男子指控塔吉特将婴儿产品优惠券寄给他的女儿 —— 一个高中生。但没多久他却来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。塔吉特百货就是靠着分析用户所有的购物数据，然后通过相关关系分析得出事情的真实状况。 在这个案例中，那个高中生少女明显是被聚到了孕妇那一类，因为她的行为模式与孕妇是很相近的。 总的来说，聚类算法大多用在用户画像、客户分群之类的，也可用于图像分析（色块相近聚集，后续会有一篇文章介绍kmeans在图像分析上的应用）另外附上一篇《K-Means算法的10个有趣用例》,有兴趣的可以详细看看 不适用场景每一种算法都不是放之四海而皆准的，下面是 K-Means 算法不适用的两个场景：这种情况如果由人来聚类，分分钟搞定，但是 K-Means 算法是这样做的：下面这种情况更微妙一些，各个子集的密集程度相差很大：看看 K-Means 算法是怎样做的：好吧，的确是差成渣了，但这真的不是算法的问题，而是我们人类对算法的选择问题。 结束语k-means算法无论是原理还是代码MLlib的使用都是较为简单，但是在某些领域其效果却是出奇的好，作为入门机器学习的第一步，K-means也向我们掀开了机器学习世界神秘面纱的一角，给我们这些初学者极大的信心。 k-means只是机器学习中的一个算法工具，更多算法，将会在后续的日子中一一学习掌握，我也会即时把学习的一些心得写成系列文章，与君共勉。 参考链接 《Spark机器学习2：K-Means聚类算法》 《K-Means算法的10个有趣用例》 《Wholesale customer Data Set》 《使用Spark MLlib做K-mean聚类分析》]]></content>
      <categories>
        <category>Spark-MLlib学习日记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MachineLeaning</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kerberos探索遇到的问题及心得]]></title>
    <url>%2FKerberos%E6%8E%A2%E7%B4%A2%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[Kerberos在集群开启时做了什么kerberos在浪潮集群中启动时会自动安装clien端，此时它会针对每个服务以及机器创建对应得Principal，这些principal也将创建成Linux内的用户。 注意，这些用户将只存在于创建这些用户的机器上 关于Hive执行HQL时count(*)会出错 Hive跟HDFS一样，在集群开启Kerberos后不需要特别的设置即可使用，然而在试图执行 1select count( * ) from test 时，会出现以下报错: 原因便是上面提到的，创建的用户只在创建的机器上有，而Hive这个count其实是启动了一个MapReduce在集群上跑，集群上其他的机器没有nassoft_m这个账号，所以报错说找不到用户名。 解决办法： &ensp;&ensp;&ensp;&ensp;在每台机都加上nassoft_m这个账号即可 关于Hive遇到的权限问题 hive很多操作都是对HDFS的，而hdfs默认文件权限为drwxr-xr-x ，即文件创建者拥有全权限，同组及其他用户只拥有读取权限，然而我们登陆的principal很多情况下都不是文件创建者，所以在kerberos启动的情况下使用hive，又时会出现权限不足的报错，就是因为hive可能对hdfs进行了写入操作。 解决办法： &ensp;&ensp;&ensp;&ensp;把nassoft_m账号添加到hdfs用户组里，然后使用chmod 777(或者774)命令修改权限 查看zookeeper节点认证失败 解决办法：在zkCli.sh 中加入 1JVMFLAGS=&quot;-Djava.security.auth.login.config=/usr/hdp/2.6.4.0-91/zookeeper/conf/zookeeper_jaas.conf&quot; 关于Hbase出现的权限不足报错认证后在hbase中使用命令行出现如下错误： 解决办法：给相应的用户分配权限1grant &apos;nassoft_r&apos;,&apos;RWXCA&apos; 关于kafka在Kerberos中的使用命令行稍有变化 生产者端命令为：bin/kafka-console-producer.sh –broker-list hd2.bigdata:6667 –topic test_wifi2 –producer.config conf/producer.properties –security-protocol SASL_PLAINTEXT 消费者端命令为：bin/kafka-console-consumer.sh –bootstrap-server hd2.bigdata:6667 –from-beginning –topic test_wifi2 conf/consumer.properties –security-protocol SASL_PLAINTEXT 另外就是java代码中需要添加以下配置：12345678System.setProperty(&quot;java.security.krb5.conf&quot;, &quot;E:\\krb5.conf&quot;);System.setProperty(&quot;java.security.auth.login.config&quot;, &quot;E:\\kafka_client_jaas.conf&quot;);Properties props = new Properties();props.put(&quot;security.protocol&quot;, &quot;SASL_PLAINTEXT&quot;);props.put(&quot;sasl.mechanism&quot;, &quot;GSSAPI&quot;);props.put(&quot;sasl.kerberos.service.name&quot;, &quot;kafka&quot;);...... 需要从服务器下载以下3个文件： krb5.conf kafka_client_jaas.conf kafka.service.keytab 其中kafka_client_jaas.conf稍作修改：1234567891011121314151617181920Client&#123; com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useTicketCache=true principal=&quot;kafka/hd2.bigdata@HADOOP.COM&quot; useKeyTab=true serviceName=&quot;zookeeper&quot; keyTab=&quot;E:\\kafka.service.keytab&quot; client=true;&#125;;KafkaClient &#123; com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useTicketCache=true principal=&quot;kafka/hd2.bigdata@HADOOP.COM&quot; useKeyTab=true serviceName=&quot;kafka&quot; keyTab=&quot;E:\\kafka.service.keytab&quot; client=true;&#125;; solr的相关问题12Kinit nassoft_mcurl --negotiate -u : &quot;http://hd1.bigdata:8983/solr/&quot; 解决方法：1234server/scripts/cloud-scripts/zkcli.sh -zkhost hd2.bigdata:2181,master2.bigdata:2181,master1.bigdata:2181 -cmd put /security.json &apos;&#123;&quot;authentication&quot;:&#123;&quot;class&quot;: &quot;org.apache.solr.security.KerberosPlugin&quot;&#125;&#125;&apos;刷新solr keytab修改 keytab 权限重启solr集群 curl --negotiate -u : &quot;http://hd1.bigdata:8983/solr/&quot;成功]]></content>
      <categories>
        <category>集群</category>
      </categories>
      <tags>
        <tag>Keeberos</tag>
        <tag>集群安全</tag>
      </tags>
  </entry>
</search>
